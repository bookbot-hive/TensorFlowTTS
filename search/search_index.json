{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home This documentation web page aims to fully cover the end-to-end pipeline of using TensorFlowTTS to train and deploy text-to-speech models. Since the original repository seems to be inactive, these set of tutorials will work on this fork developed by Wilson Wongso which contains new features. Newly Added Features IPA-based Acoustic models for English and Indonesian LightSpeech Guides Installing TensorFlowTTS Train LightSpeech & Multi-band MelGAN Convert and Infer Models on TensorFlowLite Implementing Custom Processor Adding British English Support to gruut","title":"Home"},{"location":"#home","text":"This documentation web page aims to fully cover the end-to-end pipeline of using TensorFlowTTS to train and deploy text-to-speech models. Since the original repository seems to be inactive, these set of tutorials will work on this fork developed by Wilson Wongso which contains new features.","title":"Home"},{"location":"#newly-added-features","text":"IPA-based Acoustic models for English and Indonesian LightSpeech","title":"Newly Added Features"},{"location":"#guides","text":"Installing TensorFlowTTS Train LightSpeech & Multi-band MelGAN Convert and Infer Models on TensorFlowLite Implementing Custom Processor Adding British English Support to gruut","title":"Guides"},{"location":"guides/british_english_gruut/","text":"Adding British English Support to gruut Everything here can be followed along in Google Colab! gruut is a very awesome grapheme-to-phoneme converter for English, and it also includes additional goodies such as a text processor that further handles numbers, abbreviations, and other intricate details. This is why gruut was an excellent choice when we wanted to implement a custom IPA processor for English. One missing feature, however, is the ability to phonemize British English. As of the time of writing, gruut only supports American English. Several users , myself included, would love to see it implemented with a similar API that we know and love. But seeing how the project has not been updated since June 2022, it looks like our only choice is to implement it ourselves -- which is what we're going to do today. Follow along as we integrate espeak-ng 's British English g2p into gruut. A fork that implements these changes can be found here . Info A lot of the steps here are based on the official guide which also seems to be outdated. To start, we need to install espeak-ng , gruut , phonetisaurus , and clone the gruut repository as well. We can do these via the command line. Installation ! apt - get - q install espeak - ng Reading package lists... Building dependency tree... Reading state information... The following package was automatically installed and is no longer required: libnvidia-common-460 Use 'apt autoremove' to remove it. The following additional packages will be installed: espeak-ng-data libespeak-ng1 libpcaudio0 libsonic0 The following NEW packages will be installed: espeak-ng espeak-ng-data libespeak-ng1 libpcaudio0 libsonic0 0 upgraded, 5 newly installed, 0 to remove and 20 not upgraded. Need to get 3,957 kB of archives. After this operation, 10.9 MB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpcaudio0 amd64 1.0-1 [6,536 B] Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsonic0 amd64 0.2.0-6 [13.4 kB] Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 espeak-ng-data amd64 1.49.2+dfsg-1 [3,469 kB] Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libespeak-ng1 amd64 1.49.2+dfsg-1 [187 kB] Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 espeak-ng amd64 1.49.2+dfsg-1 [282 kB] Fetched 3,957 kB in 1s (3,165 kB/s) Selecting previously unselected package libpcaudio0. (Reading database ... 124013 files and directories currently installed.) Preparing to unpack .../libpcaudio0_1.0-1_amd64.deb ... Unpacking libpcaudio0 (1.0-1) ... Selecting previously unselected package libsonic0:amd64. Preparing to unpack .../libsonic0_0.2.0-6_amd64.deb ... Unpacking libsonic0:amd64 (0.2.0-6) ... Selecting previously unselected package espeak-ng-data:amd64. Preparing to unpack .../espeak-ng-data_1.49.2+dfsg-1_amd64.deb ... Unpacking espeak-ng-data:amd64 (1.49.2+dfsg-1) ... Selecting previously unselected package libespeak-ng1:amd64. Preparing to unpack .../libespeak-ng1_1.49.2+dfsg-1_amd64.deb ... Unpacking libespeak-ng1:amd64 (1.49.2+dfsg-1) ... Selecting previously unselected package espeak-ng. Preparing to unpack .../espeak-ng_1.49.2+dfsg-1_amd64.deb ... Unpacking espeak-ng (1.49.2+dfsg-1) ... Setting up libsonic0:amd64 (0.2.0-6) ... Setting up libpcaudio0 (1.0-1) ... Setting up espeak-ng-data:amd64 (1.49.2+dfsg-1) ... Setting up libespeak-ng1:amd64 (1.49.2+dfsg-1) ... Setting up espeak-ng (1.49.2+dfsg-1) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... Processing triggers for libc-bin (2.27-3ubuntu1.6) ... ! pip install - q gruut phonetisaurus |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 74 kB 1.8 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.1 MB 11.2 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 292 kB 63.3 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 101 kB 9.6 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15.2 MB 15.4 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125 kB 54.2 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.0 MB 60.4 MB/s Building wheel for gruut (setup.py) ... done Building wheel for gruut-ipa (setup.py) ... done Building wheel for gruut-lang-en (setup.py) ... done Building wheel for docopt (setup.py) ... done ! git clone - q https : // github . com / rhasspy / gruut . git Corpus The first thing we have to decide is the corpus. What words do we want to include in the lexicon? For simplicity's sake, I chose to grab all words in the current American English lexicon, and hence make a British \"counterpart\" of it. Since all words are stored in a SQLite database, we just need to connect into the database via the sqlite3 library provided in Python. import sqlite3 con = sqlite3 . connect ( \"gruut/gruut-lang-en/gruut_lang_en/lexicon.db\" ) cur = con . cursor () After making a connection into the database and creating a cursor, we can then query all words in the database. words = sorted ( list ( set ([ row [ 0 ] for row in cur . execute ( \"SELECT word FROM word_phonemes\" )]))) words [: 5 ] ['a', \"a's\", 'a.m.', 'aaberg', 'aachen'] len ( words ) 124392 Then, we just need to write each word into a plain textfile, each separated by a newline break. with open ( \"words.txt\" , \"w\" ) as f : for word in words : f . write ( f \" { word } \\n \" ) A quick check of the results: ! head words . txt a a's a.m. aaberg aachen aachener aah aaker aaliyah aalseth ! tail words . txt zynda zysk zyskowski zyuganov zyuganov's zywicki \u00e9abha \u00f3rla \u00f3rlagh \u0161erbed\u017eija Building a Lexicon Now that we have a list of words (corpus), we can simply use the pre-provided espeak_word.sh script that conveniently reads the list of words in a plain textfile and writes out a lexicon. The lexicon will contain the word, and its corresponding IPA phonemes. In our case, these phonemes are generated by espeak-ng , specifically the en-gb voice. Notice that we also have to pass the argument pos , such that the lexicon will also contain the word's part-of-speech (POS) tag. This will help identify if a homograph is a noun, or a verb, or is of other POS tag. We will see later how this would be integrated into the resultant lexicon database. ! bash gruut / bin / espeak_word . sh en - gb pos < words . txt > lexicon . txt After about 3-4 hours of running, the lexicon file should be successfully generated. ! head lexicon . txt a _ \u02c8e\u026a a's _ \u02c8e\u026a z a.m. _ \u02cce\u026a \u02c8\u025b m aaberg _ \u02c8\u0251\u02d0 b \u025c\u02d0 \u0261 aachen _ \u02c8\u0251\u02d0 t\u0283 \u0259 n aachener _ \u02c8\u0251\u02d0 t\u0283 \u0259 n \u02cc\u0259 aah _ \u02c8\u0251\u02d0 aaker _ \u02c8\u0251\u02d0 k \u0259 aaliyah _ \u0259 l \u02c8i\u02d0 \u0259 aalseth _ \u02c8\u0251\u02d0 l s \u0259 \u03b8 ! tail lexicon . txt zynda _ z \u02c8\u026a n d \u0259 zysk _ z \u02c8\u026a s k zyskowski _ z \u026a s k \u02c8a\u028a s k \u026a zyuganov _ z j \u02c8u\u02d0 \u0261 \u0250 n \u02cc\u0252 v zyuganov's _ z j \u02c8u\u02d0 \u0261 \u0250 n \u02cc\u0252 v z zywicki _ z a\u026a w \u02c8\u026a k i \u00e9abha _ \u026a \u02c8a b h \u0259 \u00f3rla _ \u02c8\u0254\u02d0 l \u0259 \u00f3rlagh _ \u02c8\u0254\u02d0 l \u0251\u02d0 \u0261 \u0161erbed\u017eija _ \u0283 \u02c8\u025c\u02d0 b \u026a d \u0292 \u02cc\u026a d\u0292 \u0259 ! wc - l lexicon . txt 125059 lexicon.txt As expected, there are more entries in the lexicon than the original word list (125,059 from 124,392) due to homographs. A word could appear twice or more, if it's a homograph. Lexicon Database We then convert the resultant lexicon text file to a SQLite database that gruut will read from later. Again, the script was conveniently provided by the authors of gruut. We simply have to run the Python utility script and pass the additional --role argument as we're also working with POS tags. ! python3 - m gruut . lexicon2db -- role -- casing lower -- lexicon lexicon . txt -- database lexicon . db G2P Model The last component that we have to train is a grapheme-to-phoneme (g2p) model. Here, we'll be using Phonetisaurus to generate a grapheme-phoneme alignment corpus. ! phonetisaurus train -- corpus g2p . corpus -- model g2p . fst lexicon . txt INFO:phonetisaurus-train:2022-12-12 06:57:23: Checking command configuration... DEBUG:phonetisaurus-train:2022-12-12 06:57:23: Directory does not exist. Trying to create. INFO:phonetisaurus-train:2022-12-12 06:57:23: Checking lexicon for reserved characters: '}', '|', '_'... DEBUG:phonetisaurus-train:2022-12-12 06:57:23: arpa_path: train/model.o8.arpa DEBUG:phonetisaurus-train:2022-12-12 06:57:23: corpus_path: train/model.corpus DEBUG:phonetisaurus-train:2022-12-12 06:57:23: dir_prefix: train DEBUG:phonetisaurus-train:2022-12-12 06:57:23: grow: False DEBUG:phonetisaurus-train:2022-12-12 06:57:23: lexicon_file: /tmp/tmp9apulg06.txt DEBUG:phonetisaurus-train:2022-12-12 06:57:23: logger: <Logger phonetisaurus-train (DEBUG)> DEBUG:phonetisaurus-train:2022-12-12 06:57:23: makeJointNgramCommand: <bound method G2PModelTrainer._mitlm of <__main__.G2PModelTrainer object at 0x7ff2e63a4610>> DEBUG:phonetisaurus-train:2022-12-12 06:57:23: model_path: train/model.fst DEBUG:phonetisaurus-train:2022-12-12 06:57:23: model_prefix: model DEBUG:phonetisaurus-train:2022-12-12 06:57:23: ngram_order: 8 DEBUG:phonetisaurus-train:2022-12-12 06:57:23: seq1_del: False DEBUG:phonetisaurus-train:2022-12-12 06:57:23: seq1_max: 2 DEBUG:phonetisaurus-train:2022-12-12 06:57:23: seq2_del: True DEBUG:phonetisaurus-train:2022-12-12 06:57:23: seq2_max: 2 DEBUG:phonetisaurus-train:2022-12-12 06:57:23: verbose: True DEBUG:phonetisaurus-train:2022-12-12 06:57:23: phonetisaurus-align --input=/tmp/tmp9apulg06.txt --ofile=train/model.corpus --seq1_del=false --seq2_del=true --seq1_max=2 --seq2_max=2 --grow=false INFO:phonetisaurus-train:2022-12-12 06:57:23: Aligning lexicon... GitRevision: package Loading input file: /tmp/tmp9apulg06.txt Alignment failed: i t Alignment failed: m o h a m e d Alignment failed: o n e Alignment failed: s t Alignment failed: s t Alignment failed: t o Alignment failed: v i Alignment failed: v i Alignment failed: v i Alignment failed: w h e r e v e r Alignment failed: x i Alignment failed: x i Alignment failed: x i Starting EM... Finished first iter... Iteration: 1 Change: 2.5657 Iteration: 2 Change: 0.00875473 Iteration: 3 Change: 0.012403 Iteration: 4 Change: 0.00788784 Iteration: 5 Change: 0.00399113 Iteration: 6 Change: 0.00240803 Iteration: 7 Change: 0.00218391 Iteration: 8 Change: 0.00197029 Iteration: 9 Change: 0.00191975 Iteration: 10 Change: 0.00169945 Iteration: 11 Change: 0.00149059 Last iteration: DEBUG:phonetisaurus-train:2022-12-12 06:57:24: estimate-ngram -o 8 -t train/model.corpus -wl train/model.o8.arpa INFO:phonetisaurus-train:2022-12-12 06:57:24: Training joint ngram model... 0.001 Loading corpus train/model.corpus... 0.020 Smoothing[1] = ModKN 0.020 Smoothing[2] = ModKN 0.020 Smoothing[3] = ModKN 0.020 Smoothing[4] = ModKN 0.020 Smoothing[5] = ModKN 0.020 Smoothing[6] = ModKN 0.020 Smoothing[7] = ModKN 0.020 Smoothing[8] = ModKN 0.020 Set smoothing algorithms... 0.020 Y 6.304348e-01 0.020 Y 6.363636e-01 0.020 Y 7.024504e-01 0.020 Y 7.710983e-01 0.020 Y 8.060942e-01 0.020 Y 8.090737e-01 0.020 Y 8.037634e-01 0.021 Y 6.779026e-01 0.021 Estimating full n-gram model... 0.021 Saving LM to train/model.o8.arpa... DEBUG:phonetisaurus-train:2022-12-12 06:57:24: phonetisaurus-arpa2wfst --lm=train/model.o8.arpa --ofile=train/model.fst INFO:phonetisaurus-train:2022-12-12 06:57:24: Converting ARPA format joint n-gram model to WFST format... GitRevision: package Initializing... Converting... INFO:phonetisaurus-train:2022-12-12 06:57:24: G2P training succeeded: train/model.fst Using the alignment corpus, we can then train a g2p conditional random field (CRF) model using the script provided by gruut. ! python3 - m gruut . g2p train -- corpus g2p . corpus -- output g2p / model . crf INFO:gruut.g2p:Training INFO:gruut.g2p:Training completed in 25.27762404100031 second(s) INFO:gruut.g2p:{'num': 49, 'scores': {}, 'loss': 5350.58727, 'feature_norm': 49.770636, 'error_norm': 0.506685, 'active_features': 46707, 'linesearch_trials': 1, 'linesearch_step': 1.0, 'time': 0.471} Finally, we have to integrate the alignment corpus back into the lexicon database. ! python3 - m gruut . corpus2db -- corpus g2p . corpus -- database lexicon . db Added 429 alignments to lexicon.db And we're all set to integrate our new British English g2p model into gruut! Note We did not go through the necessary steps to build a POS tagging model, simply because we can just use the existing American English POS tagging model. Integrating into gruut The official guide is outdated on how we could add a new language into gruut. But after reading the source code, it seems that it's simply going to look up files in the gruut/gruut-lang-{lang} directory, where {lang} is the language that we want to support. Now, the issue is that there's already an English directory, which contains the necessary files for American English. What we decided to do is to replace the lexicon database and the g2p model inside gruut/gruut-lang-en/gruut-lang-en/espeak folder, which can later be accessed by flagging espeak=True in the Python front-end. This is a rather hacky solution since we're assuming that anytime the end user asks for the espeak English model, we're going to return them the British English model -- regardless if they ask for en-us . But nonetheless, I think this would still work anyway. A simple replacement of gruut/gruut-lang-en/gruut_lang_en/espeak/lexicon.db and gruut/gruut-lang-en/gruut_lang_en/espeak/g2p/model.crf with our newly generated files from above should do the job. !cp g2p/model.crf gruut/gruut-lang-en/gruut_lang_en/espeak/g2p/ !cp lexicon.db gruut/gruut-lang-en/gruut_lang_en/espeak/ gruut \u251c\u2500\u2500 ... ... \u251c\u2500\u2500 gruut-lang-en \u2502 \u251c\u2500\u2500 LANGUAGE \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 gruut_lang_en \u2502 \u2502 \u251c\u2500\u2500 VERSION \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 espeak \u2502 \u2502 \u2502 \u251c\u2500\u2500 g2p + \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 model.crf + \u2502 \u2502 \u2502 \u2514\u2500\u2500 lexicon.db \u2502 \u2502 \u251c\u2500\u2500 g2p \u2502 \u2502 \u2502 \u2514\u2500\u2500 model.crf \u2502 \u2502 \u251c\u2500\u2500 lexicon.db \u2502 \u2502 \u2514\u2500\u2500 pos \u2502 \u2502 \u2514\u2500\u2500 model.crf \u2502 \u2514\u2500\u2500 setup.py ... \u2514\u2500\u2500 ... An example demonstrating this can be found in this commit . Test To test, we need to re-install our local copy of gruut. ! cd gruut && pip install . && pip install ./ gruut - lang - en At this point you might need to restart your Google Colab runtime to re-import the new gruut. And finally, we can perform a conversion through the Python frontend like so: import gruut text = \"an accelerating runner\" for words in gruut . sentences ( text , lang = \"en-gb\" , espeak = True ): for word in words : print ( word . phonemes ) ['\u02c8a', 'n'] ['\u0250', 'k', 's', '\u02c8\u025b', 'l', '\u0259', '\u0279', '\u02cce\u026a', 't', '\u026a', '\u014b'] ['\u0279', '\u02c8\u028c', 'n', '\u0259'] Comparing that to American English for words in gruut . sentences ( text , lang = \"en-us\" ): # don't set espeak=True! for word in words : print ( word . phonemes ) ['\u0259', 'n'] ['\u00e6', 'k', 's', '\u02c8\u025b', 'l', '\u025a', '\u02cce\u026a', 't', '\u026a', '\u014b'] ['\u0279', '\u02c8\u028c', 'n', '\u025a'] All done!","title":"Adding British English Support to gruut"},{"location":"guides/british_english_gruut/#adding-british-english-support-to-gruut","text":"Everything here can be followed along in Google Colab! gruut is a very awesome grapheme-to-phoneme converter for English, and it also includes additional goodies such as a text processor that further handles numbers, abbreviations, and other intricate details. This is why gruut was an excellent choice when we wanted to implement a custom IPA processor for English. One missing feature, however, is the ability to phonemize British English. As of the time of writing, gruut only supports American English. Several users , myself included, would love to see it implemented with a similar API that we know and love. But seeing how the project has not been updated since June 2022, it looks like our only choice is to implement it ourselves -- which is what we're going to do today. Follow along as we integrate espeak-ng 's British English g2p into gruut. A fork that implements these changes can be found here . Info A lot of the steps here are based on the official guide which also seems to be outdated. To start, we need to install espeak-ng , gruut , phonetisaurus , and clone the gruut repository as well. We can do these via the command line.","title":"Adding British English Support to gruut"},{"location":"guides/british_english_gruut/#installation","text":"! apt - get - q install espeak - ng Reading package lists... Building dependency tree... Reading state information... The following package was automatically installed and is no longer required: libnvidia-common-460 Use 'apt autoremove' to remove it. The following additional packages will be installed: espeak-ng-data libespeak-ng1 libpcaudio0 libsonic0 The following NEW packages will be installed: espeak-ng espeak-ng-data libespeak-ng1 libpcaudio0 libsonic0 0 upgraded, 5 newly installed, 0 to remove and 20 not upgraded. Need to get 3,957 kB of archives. After this operation, 10.9 MB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpcaudio0 amd64 1.0-1 [6,536 B] Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsonic0 amd64 0.2.0-6 [13.4 kB] Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 espeak-ng-data amd64 1.49.2+dfsg-1 [3,469 kB] Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libespeak-ng1 amd64 1.49.2+dfsg-1 [187 kB] Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 espeak-ng amd64 1.49.2+dfsg-1 [282 kB] Fetched 3,957 kB in 1s (3,165 kB/s) Selecting previously unselected package libpcaudio0. (Reading database ... 124013 files and directories currently installed.) Preparing to unpack .../libpcaudio0_1.0-1_amd64.deb ... Unpacking libpcaudio0 (1.0-1) ... Selecting previously unselected package libsonic0:amd64. Preparing to unpack .../libsonic0_0.2.0-6_amd64.deb ... Unpacking libsonic0:amd64 (0.2.0-6) ... Selecting previously unselected package espeak-ng-data:amd64. Preparing to unpack .../espeak-ng-data_1.49.2+dfsg-1_amd64.deb ... Unpacking espeak-ng-data:amd64 (1.49.2+dfsg-1) ... Selecting previously unselected package libespeak-ng1:amd64. Preparing to unpack .../libespeak-ng1_1.49.2+dfsg-1_amd64.deb ... Unpacking libespeak-ng1:amd64 (1.49.2+dfsg-1) ... Selecting previously unselected package espeak-ng. Preparing to unpack .../espeak-ng_1.49.2+dfsg-1_amd64.deb ... Unpacking espeak-ng (1.49.2+dfsg-1) ... Setting up libsonic0:amd64 (0.2.0-6) ... Setting up libpcaudio0 (1.0-1) ... Setting up espeak-ng-data:amd64 (1.49.2+dfsg-1) ... Setting up libespeak-ng1:amd64 (1.49.2+dfsg-1) ... Setting up espeak-ng (1.49.2+dfsg-1) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... Processing triggers for libc-bin (2.27-3ubuntu1.6) ... ! pip install - q gruut phonetisaurus |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 74 kB 1.8 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12.1 MB 11.2 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 292 kB 63.3 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 101 kB 9.6 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15.2 MB 15.4 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125 kB 54.2 MB/s |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.0 MB 60.4 MB/s Building wheel for gruut (setup.py) ... done Building wheel for gruut-ipa (setup.py) ... done Building wheel for gruut-lang-en (setup.py) ... done Building wheel for docopt (setup.py) ... done ! git clone - q https : // github . com / rhasspy / gruut . git","title":"Installation"},{"location":"guides/british_english_gruut/#corpus","text":"The first thing we have to decide is the corpus. What words do we want to include in the lexicon? For simplicity's sake, I chose to grab all words in the current American English lexicon, and hence make a British \"counterpart\" of it. Since all words are stored in a SQLite database, we just need to connect into the database via the sqlite3 library provided in Python. import sqlite3 con = sqlite3 . connect ( \"gruut/gruut-lang-en/gruut_lang_en/lexicon.db\" ) cur = con . cursor () After making a connection into the database and creating a cursor, we can then query all words in the database. words = sorted ( list ( set ([ row [ 0 ] for row in cur . execute ( \"SELECT word FROM word_phonemes\" )]))) words [: 5 ] ['a', \"a's\", 'a.m.', 'aaberg', 'aachen'] len ( words ) 124392 Then, we just need to write each word into a plain textfile, each separated by a newline break. with open ( \"words.txt\" , \"w\" ) as f : for word in words : f . write ( f \" { word } \\n \" ) A quick check of the results: ! head words . txt a a's a.m. aaberg aachen aachener aah aaker aaliyah aalseth ! tail words . txt zynda zysk zyskowski zyuganov zyuganov's zywicki \u00e9abha \u00f3rla \u00f3rlagh \u0161erbed\u017eija","title":"Corpus"},{"location":"guides/british_english_gruut/#building-a-lexicon","text":"Now that we have a list of words (corpus), we can simply use the pre-provided espeak_word.sh script that conveniently reads the list of words in a plain textfile and writes out a lexicon. The lexicon will contain the word, and its corresponding IPA phonemes. In our case, these phonemes are generated by espeak-ng , specifically the en-gb voice. Notice that we also have to pass the argument pos , such that the lexicon will also contain the word's part-of-speech (POS) tag. This will help identify if a homograph is a noun, or a verb, or is of other POS tag. We will see later how this would be integrated into the resultant lexicon database. ! bash gruut / bin / espeak_word . sh en - gb pos < words . txt > lexicon . txt After about 3-4 hours of running, the lexicon file should be successfully generated. ! head lexicon . txt a _ \u02c8e\u026a a's _ \u02c8e\u026a z a.m. _ \u02cce\u026a \u02c8\u025b m aaberg _ \u02c8\u0251\u02d0 b \u025c\u02d0 \u0261 aachen _ \u02c8\u0251\u02d0 t\u0283 \u0259 n aachener _ \u02c8\u0251\u02d0 t\u0283 \u0259 n \u02cc\u0259 aah _ \u02c8\u0251\u02d0 aaker _ \u02c8\u0251\u02d0 k \u0259 aaliyah _ \u0259 l \u02c8i\u02d0 \u0259 aalseth _ \u02c8\u0251\u02d0 l s \u0259 \u03b8 ! tail lexicon . txt zynda _ z \u02c8\u026a n d \u0259 zysk _ z \u02c8\u026a s k zyskowski _ z \u026a s k \u02c8a\u028a s k \u026a zyuganov _ z j \u02c8u\u02d0 \u0261 \u0250 n \u02cc\u0252 v zyuganov's _ z j \u02c8u\u02d0 \u0261 \u0250 n \u02cc\u0252 v z zywicki _ z a\u026a w \u02c8\u026a k i \u00e9abha _ \u026a \u02c8a b h \u0259 \u00f3rla _ \u02c8\u0254\u02d0 l \u0259 \u00f3rlagh _ \u02c8\u0254\u02d0 l \u0251\u02d0 \u0261 \u0161erbed\u017eija _ \u0283 \u02c8\u025c\u02d0 b \u026a d \u0292 \u02cc\u026a d\u0292 \u0259 ! wc - l lexicon . txt 125059 lexicon.txt As expected, there are more entries in the lexicon than the original word list (125,059 from 124,392) due to homographs. A word could appear twice or more, if it's a homograph.","title":"Building a Lexicon"},{"location":"guides/british_english_gruut/#lexicon-database","text":"We then convert the resultant lexicon text file to a SQLite database that gruut will read from later. Again, the script was conveniently provided by the authors of gruut. We simply have to run the Python utility script and pass the additional --role argument as we're also working with POS tags. ! python3 - m gruut . lexicon2db -- role -- casing lower -- lexicon lexicon . txt -- database lexicon . db","title":"Lexicon Database"},{"location":"guides/british_english_gruut/#g2p-model","text":"The last component that we have to train is a grapheme-to-phoneme (g2p) model. Here, we'll be using Phonetisaurus to generate a grapheme-phoneme alignment corpus. ! phonetisaurus train -- corpus g2p . corpus -- model g2p . fst lexicon . txt INFO:phonetisaurus-train:2022-12-12 06:57:23: Checking command configuration... DEBUG:phonetisaurus-train:2022-12-12 06:57:23: Directory does not exist. Trying to create. INFO:phonetisaurus-train:2022-12-12 06:57:23: Checking lexicon for reserved characters: '}', '|', '_'... DEBUG:phonetisaurus-train:2022-12-12 06:57:23: arpa_path: train/model.o8.arpa DEBUG:phonetisaurus-train:2022-12-12 06:57:23: corpus_path: train/model.corpus DEBUG:phonetisaurus-train:2022-12-12 06:57:23: dir_prefix: train DEBUG:phonetisaurus-train:2022-12-12 06:57:23: grow: False DEBUG:phonetisaurus-train:2022-12-12 06:57:23: lexicon_file: /tmp/tmp9apulg06.txt DEBUG:phonetisaurus-train:2022-12-12 06:57:23: logger: <Logger phonetisaurus-train (DEBUG)> DEBUG:phonetisaurus-train:2022-12-12 06:57:23: makeJointNgramCommand: <bound method G2PModelTrainer._mitlm of <__main__.G2PModelTrainer object at 0x7ff2e63a4610>> DEBUG:phonetisaurus-train:2022-12-12 06:57:23: model_path: train/model.fst DEBUG:phonetisaurus-train:2022-12-12 06:57:23: model_prefix: model DEBUG:phonetisaurus-train:2022-12-12 06:57:23: ngram_order: 8 DEBUG:phonetisaurus-train:2022-12-12 06:57:23: seq1_del: False DEBUG:phonetisaurus-train:2022-12-12 06:57:23: seq1_max: 2 DEBUG:phonetisaurus-train:2022-12-12 06:57:23: seq2_del: True DEBUG:phonetisaurus-train:2022-12-12 06:57:23: seq2_max: 2 DEBUG:phonetisaurus-train:2022-12-12 06:57:23: verbose: True DEBUG:phonetisaurus-train:2022-12-12 06:57:23: phonetisaurus-align --input=/tmp/tmp9apulg06.txt --ofile=train/model.corpus --seq1_del=false --seq2_del=true --seq1_max=2 --seq2_max=2 --grow=false INFO:phonetisaurus-train:2022-12-12 06:57:23: Aligning lexicon... GitRevision: package Loading input file: /tmp/tmp9apulg06.txt Alignment failed: i t Alignment failed: m o h a m e d Alignment failed: o n e Alignment failed: s t Alignment failed: s t Alignment failed: t o Alignment failed: v i Alignment failed: v i Alignment failed: v i Alignment failed: w h e r e v e r Alignment failed: x i Alignment failed: x i Alignment failed: x i Starting EM... Finished first iter... Iteration: 1 Change: 2.5657 Iteration: 2 Change: 0.00875473 Iteration: 3 Change: 0.012403 Iteration: 4 Change: 0.00788784 Iteration: 5 Change: 0.00399113 Iteration: 6 Change: 0.00240803 Iteration: 7 Change: 0.00218391 Iteration: 8 Change: 0.00197029 Iteration: 9 Change: 0.00191975 Iteration: 10 Change: 0.00169945 Iteration: 11 Change: 0.00149059 Last iteration: DEBUG:phonetisaurus-train:2022-12-12 06:57:24: estimate-ngram -o 8 -t train/model.corpus -wl train/model.o8.arpa INFO:phonetisaurus-train:2022-12-12 06:57:24: Training joint ngram model... 0.001 Loading corpus train/model.corpus... 0.020 Smoothing[1] = ModKN 0.020 Smoothing[2] = ModKN 0.020 Smoothing[3] = ModKN 0.020 Smoothing[4] = ModKN 0.020 Smoothing[5] = ModKN 0.020 Smoothing[6] = ModKN 0.020 Smoothing[7] = ModKN 0.020 Smoothing[8] = ModKN 0.020 Set smoothing algorithms... 0.020 Y 6.304348e-01 0.020 Y 6.363636e-01 0.020 Y 7.024504e-01 0.020 Y 7.710983e-01 0.020 Y 8.060942e-01 0.020 Y 8.090737e-01 0.020 Y 8.037634e-01 0.021 Y 6.779026e-01 0.021 Estimating full n-gram model... 0.021 Saving LM to train/model.o8.arpa... DEBUG:phonetisaurus-train:2022-12-12 06:57:24: phonetisaurus-arpa2wfst --lm=train/model.o8.arpa --ofile=train/model.fst INFO:phonetisaurus-train:2022-12-12 06:57:24: Converting ARPA format joint n-gram model to WFST format... GitRevision: package Initializing... Converting... INFO:phonetisaurus-train:2022-12-12 06:57:24: G2P training succeeded: train/model.fst Using the alignment corpus, we can then train a g2p conditional random field (CRF) model using the script provided by gruut. ! python3 - m gruut . g2p train -- corpus g2p . corpus -- output g2p / model . crf INFO:gruut.g2p:Training INFO:gruut.g2p:Training completed in 25.27762404100031 second(s) INFO:gruut.g2p:{'num': 49, 'scores': {}, 'loss': 5350.58727, 'feature_norm': 49.770636, 'error_norm': 0.506685, 'active_features': 46707, 'linesearch_trials': 1, 'linesearch_step': 1.0, 'time': 0.471} Finally, we have to integrate the alignment corpus back into the lexicon database. ! python3 - m gruut . corpus2db -- corpus g2p . corpus -- database lexicon . db Added 429 alignments to lexicon.db And we're all set to integrate our new British English g2p model into gruut! Note We did not go through the necessary steps to build a POS tagging model, simply because we can just use the existing American English POS tagging model.","title":"G2P Model"},{"location":"guides/british_english_gruut/#integrating-into-gruut","text":"The official guide is outdated on how we could add a new language into gruut. But after reading the source code, it seems that it's simply going to look up files in the gruut/gruut-lang-{lang} directory, where {lang} is the language that we want to support. Now, the issue is that there's already an English directory, which contains the necessary files for American English. What we decided to do is to replace the lexicon database and the g2p model inside gruut/gruut-lang-en/gruut-lang-en/espeak folder, which can later be accessed by flagging espeak=True in the Python front-end. This is a rather hacky solution since we're assuming that anytime the end user asks for the espeak English model, we're going to return them the British English model -- regardless if they ask for en-us . But nonetheless, I think this would still work anyway. A simple replacement of gruut/gruut-lang-en/gruut_lang_en/espeak/lexicon.db and gruut/gruut-lang-en/gruut_lang_en/espeak/g2p/model.crf with our newly generated files from above should do the job. !cp g2p/model.crf gruut/gruut-lang-en/gruut_lang_en/espeak/g2p/ !cp lexicon.db gruut/gruut-lang-en/gruut_lang_en/espeak/ gruut \u251c\u2500\u2500 ... ... \u251c\u2500\u2500 gruut-lang-en \u2502 \u251c\u2500\u2500 LANGUAGE \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 gruut_lang_en \u2502 \u2502 \u251c\u2500\u2500 VERSION \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 espeak \u2502 \u2502 \u2502 \u251c\u2500\u2500 g2p + \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 model.crf + \u2502 \u2502 \u2502 \u2514\u2500\u2500 lexicon.db \u2502 \u2502 \u251c\u2500\u2500 g2p \u2502 \u2502 \u2502 \u2514\u2500\u2500 model.crf \u2502 \u2502 \u251c\u2500\u2500 lexicon.db \u2502 \u2502 \u2514\u2500\u2500 pos \u2502 \u2502 \u2514\u2500\u2500 model.crf \u2502 \u2514\u2500\u2500 setup.py ... \u2514\u2500\u2500 ... An example demonstrating this can be found in this commit .","title":"Integrating into gruut"},{"location":"guides/british_english_gruut/#test","text":"To test, we need to re-install our local copy of gruut. ! cd gruut && pip install . && pip install ./ gruut - lang - en At this point you might need to restart your Google Colab runtime to re-import the new gruut. And finally, we can perform a conversion through the Python frontend like so: import gruut text = \"an accelerating runner\" for words in gruut . sentences ( text , lang = \"en-gb\" , espeak = True ): for word in words : print ( word . phonemes ) ['\u02c8a', 'n'] ['\u0250', 'k', 's', '\u02c8\u025b', 'l', '\u0259', '\u0279', '\u02cce\u026a', 't', '\u026a', '\u014b'] ['\u0279', '\u02c8\u028c', 'n', '\u0259'] Comparing that to American English for words in gruut . sentences ( text , lang = \"en-us\" ): # don't set espeak=True! for word in words : print ( word . phonemes ) ['\u0259', 'n'] ['\u00e6', 'k', 's', '\u02c8\u025b', 'l', '\u025a', '\u02cce\u026a', 't', '\u026a', '\u014b'] ['\u0279', '\u02c8\u028c', 'n', '\u025a'] All done!","title":"Test"},{"location":"guides/custom_processor/","text":"Implementing Custom Processor There are only a handful of languages supported by the original TensorFlowTTS repository. Supporting a new language requires the implementation of a custom processor, which is crucial when designing text-to-speech models. Processor and Phonemization A processor serves as a tokenizer that helps encode texts into IDs, i.e. numerical representation of texts that you're going to synthesize. Naively, we can represent each character of say, the English language, as one unique ID and have characters as our 'tokens'. This is indeed the default behavior of the LJSpeech Processor . Other processors, however, choose to utilize grapheme-to-phoneme converters, and have phonemes as their tokens instead of individual raw characters. This intuitively makes sense, since phonemization in languages like English isn't straightforward, and mapping phonemes to their corresponding mel-spectrogram makes much more sense. However, this is only possible if a grapheme-to-phoneme (g2p) converter for a particular language exists. High-resource languages like English has tools such as g2pE and gruut , Korean has g2pK , etc. Alternatively, you might be able to build your own g2p converter if you at least have a lexicon (see ipa-dict ), which you can then take and use to build g2p models via Montreal Forced Aligner , for example. What if you don't, though? High-to-mid-resource might be able to get away with existing curated tools and/or lexicons, but low-resource languages might be a bit more problematic. In that case, probably the best bet is a character-based tokenizer, where your graphemes (characters) serve as proxy-phonemes. Meyer et al. (2022) trained text-to-speech models on low-resource African languages, and for certain languages where a lexicon is unavailable, they used proxy-phonemes: Two languages (ewe and yor) were aligned via forced alignment from scratch. Using only the found audio and transcripts (i.e., without a pre-trained acoustic model), an acoustic model was trained and the data aligned with the Montreal Forced Aligner. Graphemes were used as a proxy for phonemes in place of G2P data. As a practical example, I will be attempting to implement a character-based tokenizer for Javanese. Let's get started! Processor Vocabulary You first have to create a new processor class under tensorflow_tts/processor , which I'll call JavaneseCharacterProcessor . I really like how the LibriTTS Processor works -- it is well integrated with Montreal Forced Aligner for duration extraction and multi-speaker models. Because of that, I'll be basing my new processor on that file. Note This was also how I based both EnglishIPAProcessor and IndonesianIPAProcessor . To begin, we have to define a hard-coded list of symbols (vocabulary) that the processor will support. This is highly dependent on the dataset that you're using, as we would want to cover all possible graphemes (so none of them become unknowns). In my case, there's the 26 latin alphabet characters A-Z, plus two additional E's with diacritics: \u00e9 and \u00e8. tensorflow_tts/processor/javanese_char.py valid_symbols = [ \"a\" , \"b\" , \"c\" , \"d\" , \"e\" , \"f\" , \"g\" , \"h\" , \"i\" , \"j\" , \"k\" , \"l\" , \"m\" , \"n\" , \"o\" , \"p\" , \"q\" , \"r\" , \"s\" , \"t\" , \"u\" , \"v\" , \"w\" , \"x\" , \"y\" , \"z\" , \"\u00e8\" , \"\u00e9\" , ] Be sure to inspect your dataset and cover all possible tokens. Next would be punctuations. Again, this is dependent on how you would like to design the processor and which punctuations you would like to keep. I normally only go for ! , . ? ; : and nothing else. Anything other than those will simply be ignored. Note that this will be important during inference -- anything outside of the vocabulary will be ignored! tensorflow_tts/processor/javanese_char.py _punctuation = \"!,.?;:\" There are also additional special tokens such as @SIL , @EOS , @PAD for silence, end-of-sentence, and padding tokens, respectively. tensorflow_tts/processor/javanese_char.py _sil = \"@SIL\" _eos = \"@EOS\" _pad = \"@PAD\" _char = [ \"@\" + s for s in valid_symbols ] JAVANESE_CHARACTER_SYMBOLS = [ _pad ] + _char + list ( _punctuation ) + [ _sil ] + [ _eos ] Metadata Format An important method in LibriTTSProcessor -based processors is how metadata is formatted and later read. Sticking to the original implementation, I will keep the default format of having a metadata called train.txt , which is populated with | -delimited lines. Each line has 3 columns (in this particular order): Path to file Text read Speaker name Moreover, audios are expected to be of .wav format. These are then implemented as class variables inside the processor and influences how the create_items and get_one_sample methods behave. create_items \"creates\" each training sample, and get_one_sample loads each training sample in their right formats (e.g. read audio via SoundFile, convert text to sequence of IDs, etc.) tensorflow_tts/processor/javanese_char.py @dataclass class JavaneseCharacterProcessor ( BaseProcessor ): mode : str = \"train\" train_f_name : str = \"train.txt\" positions = { \"file\" : 0 , \"text\" : 1 , \"speaker_name\" : 2 , } # positions of file,text,speaker_name after split line f_extension : str = \".wav\" cleaner_names : str = None def create_items ( self ): with open ( os . path . join ( self . data_dir , self . train_f_name ), mode = \"r\" , encoding = \"utf-8\" ) as f : for line in f : parts = line . strip () . split ( self . delimiter ) wav_path = os . path . join ( self . data_dir , parts [ self . positions [ \"file\" ]]) wav_path = ( wav_path + self . f_extension if wav_path [ - len ( self . f_extension ) :] != self . f_extension else wav_path ) text = parts [ self . positions [ \"text\" ]] speaker_name = parts [ self . positions [ \"speaker_name\" ]] self . items . append ([ text , wav_path , speaker_name ]) def get_one_sample ( self , item ): text , wav_path , speaker_name = item audio , rate = sf . read ( wav_path , dtype = \"float32\" ) text_ids = np . asarray ( self . text_to_sequence ( text ), np . int32 ) sample = { \"raw_text\" : text , \"text_ids\" : text_ids , \"audio\" : audio , \"utt_id\" : wav_path . split ( \"/\" )[ - 1 ] . split ( \".\" )[ 0 ], \"speaker_name\" : speaker_name , \"rate\" : rate , } return sample ... Text-to-Sequence Then, the only remaining crucial feature to implement is how we will be converting texts into sequence of IDs. Again, following the original implementation of LibriTTSProcessor , our processor will have two modes: train and eval. For training, we expect our text is training-ready. What I meant by that is, in the metadata file, all texts have been pre-converted into tokens and are separated by whitespaces, for example: k a p i n g l i m a n i n d a a k e k a j i SIL Because of that, we would not need to re-separate them and could simply encode/map them to their corresponding IDs via the symbols_to_id method. On the other hand, we would probably want to have a separate behavior for inference purposes. tensorflow_tts/processor/javanese_char.py @dataclass class JavaneseCharacterProcessor ( BaseProcessor ): ... def text_to_sequence ( self , text ): if ( self . mode == \"train\" ): # in train mode text should be already transformed to characters return self . symbols_to_ids ( self . clean_char ( text . split ())) else : return self . inference_text_to_seq ( text ) def inference_text_to_seq ( self , text : str ): return self . symbols_to_ids ( self . text_to_char ( text )) def symbols_to_ids ( self , symbols_list : list ): return [ self . symbol_to_id [ s ] for s in symbols_list ] ... The code above should be relatively self-explanatory. On training mode, we simply split tokens by whitespace, clean them, and encode them as IDs. On inference, we first need to convert texts to characters, and only then encode them as IDs. Converting texts to characters is quite straightforward for the case of character-based processors. We would just need to iterate through each characters and that's it! However, for more complex processors that involve g2p conversion, this is probably where you'd want to integrate it. For instance, in EnglishIPAProcessor , this is where we pass the job to gruut: tensorflow_tts/processor/english_ipa.py @dataclass class EnglishIPAProcessor ( BaseProcessor ): ... def text_to_ph ( self , text : str ): phn_arr = [] for words in sentences ( text ): for word in words : if word . is_major_break or word . is_minor_break : phn_arr += [ word . text ] elif word . phonemes : phn_arr += word . phonemes return self . clean_g2p ( phn_arr ) ... And finally, we need to implement how we're going to \"clean\" tokens, namely, separating punctuations from actual character tokens and special tokens (the latter two beginning with an @ ): tensorflow_tts/processor/javanese_char.py @dataclass class JavaneseCharacterProcessor ( BaseProcessor ): ... def clean_char ( self , characters : list ): data = [] for char in characters : if char in _punctuation : data . append ( char ) elif char != \" \" : data . append ( \"@\" + char . lower ()) return data One thing not to miss is adding our new processor class to tensorflow_tts/processor/__init__.py : tensorflow_tts/processor/__init__.py from tensorflow_tts.processor.javanese_char import JavaneseCharacterProcessor Preprocess Once you're done with implementing the new processor class, you'll need to also register it to the pre-processor. It's fairly simple to do, with only a few additional lines to add in certain lines: tensorflow_tts/bin/preprocess.py from tensorflow_tts.processor import JavaneseCharacterProcessor from tensorflow_tts.processor.javanese_char import JAVANESE_CHARACTER_SYMBOLS tensorflow_tts/bin/preprocess.py ... parser.add_argument( \"--dataset\", type=str, default=\"ljspeech\", choices=[ \"ljspeech\", \"ljspeech_multi\", \"kss\", \"libritts\", \"baker\", \"thorsten\", \"ljspeechu\", \"synpaflex\", \"jsut\", \"indonesianipa\", \"englishipa\", + \"javanesechar\", # what we're going to call our processor ], help=\"Dataset to preprocess.\", ) ... def preprocess(): \"\"\"Run preprocessing process and compute statistics for normalizing.\"\"\" config = parse_and_config() dataset_processor = { \"ljspeech\": LJSpeechProcessor, \"ljspeech_multi\": LJSpeechMultiProcessor, \"kss\": KSSProcessor, \"libritts\": LibriTTSProcessor, \"baker\": BakerProcessor, \"thorsten\": ThorstenProcessor, \"ljspeechu\": LJSpeechUltimateProcessor, \"synpaflex\": SynpaflexProcessor, \"jsut\": JSUTProcessor, \"indonesianipa\": IndonesianIPAProcessor, \"englishipa\": EnglishIPAProcessor, + \"javanesechar\": JavaneseCharacterProcessor, } dataset_symbol = { \"ljspeech\": LJSPEECH_SYMBOLS, \"ljspeech_multi\": LJSPEECH_SYMBOLS, \"kss\": KSS_SYMBOLS, \"libritts\": LIBRITTS_SYMBOLS, \"baker\": BAKER_SYMBOLS, \"thorsten\": THORSTEN_SYMBOLS, \"ljspeechu\": LJSPEECH_U_SYMBOLS, \"synpaflex\": SYNPAFLEX_SYMBOLS, \"jsut\": JSUT_SYMBOLS, \"indonesianipa\": INDONESIAN_IPA_SYMBOLS, \"englishipa\": ENGLISH_IPA_SYMBOLS, + \"javanesechar\": JAVANESE_CHARACTER_SYMBOLS, } dataset_cleaner = { \"ljspeech\": \"english_cleaners\", \"ljspeech_multi\": \"english_cleaners\", \"kss\": \"korean_cleaners\", \"libritts\": None, \"baker\": None, \"thorsten\": \"german_cleaners\", \"ljspeechu\": \"english_cleaners\", \"synpaflex\": \"basic_cleaners\", \"jsut\": None, \"indonesianipa\": None, \"englishipa\": None, + \"javanesechar\": None, } Integrating with text2mel Models Lastly, we need to integrate our processor with existing text2mel models, such as FastSpeech2 . The number of tokens in the processor will be used as the vocabulary size of the text2mel model. tensorflow_tts/configs/fastspeech.py from tensorflow_tts.processor.javanese_char import ( JAVANESE_CHARACTER_SYMBOLS as javanese_char_symbols , ) tensorflow_tts/configs/fastspeech.py ... if dataset == \"ljspeech\": self.vocab_size = vocab_size elif dataset == \"kss\": self.vocab_size = len(kss_symbols) elif dataset == \"baker\": self.vocab_size = len(bk_symbols) elif dataset == \"libritts\": self.vocab_size = len(lbri_symbols) elif dataset == \"jsut\": self.vocab_size = len(jsut_symbols) elif dataset == \"ljspeechu\": self.vocab_size = len(lju_symbols) elif dataset == \"indonesianipa\": self.vocab_size = len(indonesian_ipa_symbols) elif dataset == \"englishipa\": self.vocab_size = len(english_ipa_symbols) + elif dataset == \"javanesechar\": + self.vocab_size = len(javanese_char_symbols) else: raise ValueError(\"No such dataset: {}\".format(dataset)) ... AutoProcessor A handy feature found in TensorFlowTTS is the ability to load processors from HuggingFace Hub. This allows you to do something like: from tensorflow_tts.inference import AutoProcessor processor = AutoProcessor . from_pretrained ( \"bookbot/lightspeech-mfa-id\" ) To allow such support for our custom processor, we simply have to add it to tensorflow_tts/inference/auto_processor.py : tensorflow_tts/inference/auto_processor.py from tensorflow_tts.processor import ( LJSpeechProcessor, KSSProcessor, BakerProcessor, LibriTTSProcessor, ThorstenProcessor, LJSpeechUltimateProcessor, SynpaflexProcessor, JSUTProcessor, LJSpeechMultiProcessor, IndonesianIPAProcessor, EnglishIPAProcessor, + JavaneseCharacterProcessor, ) from tensorflow_tts.utils import CACHE_DIRECTORY, PROCESSOR_FILE_NAME, LIBRARY_NAME from tensorflow_tts import __version__ as VERSION from huggingface_hub import hf_hub_url, cached_download CONFIG_MAPPING = OrderedDict( [ (\"LJSpeechProcessor\", LJSpeechProcessor), (\"LJSpeechMultiProcessor\", LJSpeechMultiProcessor), (\"KSSProcessor\", KSSProcessor), (\"BakerProcessor\", BakerProcessor), (\"LibriTTSProcessor\", LibriTTSProcessor), (\"ThorstenProcessor\", ThorstenProcessor), (\"LJSpeechUltimateProcessor\", LJSpeechUltimateProcessor), (\"SynpaflexProcessor\", SynpaflexProcessor), (\"JSUTProcessor\", JSUTProcessor), (\"IndonesianIPAProcessor\", IndonesianIPAProcessor), (\"EnglishIPAProcessor\", EnglishIPAProcessor), + (\"JavaneseCharacterProcessor\", JavaneseCharacterProcessor), ] )","title":"Implementing Custom Processor"},{"location":"guides/custom_processor/#implementing-custom-processor","text":"There are only a handful of languages supported by the original TensorFlowTTS repository. Supporting a new language requires the implementation of a custom processor, which is crucial when designing text-to-speech models.","title":"Implementing Custom Processor"},{"location":"guides/custom_processor/#processor-and-phonemization","text":"A processor serves as a tokenizer that helps encode texts into IDs, i.e. numerical representation of texts that you're going to synthesize. Naively, we can represent each character of say, the English language, as one unique ID and have characters as our 'tokens'. This is indeed the default behavior of the LJSpeech Processor . Other processors, however, choose to utilize grapheme-to-phoneme converters, and have phonemes as their tokens instead of individual raw characters. This intuitively makes sense, since phonemization in languages like English isn't straightforward, and mapping phonemes to their corresponding mel-spectrogram makes much more sense. However, this is only possible if a grapheme-to-phoneme (g2p) converter for a particular language exists. High-resource languages like English has tools such as g2pE and gruut , Korean has g2pK , etc. Alternatively, you might be able to build your own g2p converter if you at least have a lexicon (see ipa-dict ), which you can then take and use to build g2p models via Montreal Forced Aligner , for example. What if you don't, though? High-to-mid-resource might be able to get away with existing curated tools and/or lexicons, but low-resource languages might be a bit more problematic. In that case, probably the best bet is a character-based tokenizer, where your graphemes (characters) serve as proxy-phonemes. Meyer et al. (2022) trained text-to-speech models on low-resource African languages, and for certain languages where a lexicon is unavailable, they used proxy-phonemes: Two languages (ewe and yor) were aligned via forced alignment from scratch. Using only the found audio and transcripts (i.e., without a pre-trained acoustic model), an acoustic model was trained and the data aligned with the Montreal Forced Aligner. Graphemes were used as a proxy for phonemes in place of G2P data. As a practical example, I will be attempting to implement a character-based tokenizer for Javanese. Let's get started!","title":"Processor and Phonemization"},{"location":"guides/custom_processor/#processor","text":"","title":"Processor"},{"location":"guides/custom_processor/#vocabulary","text":"You first have to create a new processor class under tensorflow_tts/processor , which I'll call JavaneseCharacterProcessor . I really like how the LibriTTS Processor works -- it is well integrated with Montreal Forced Aligner for duration extraction and multi-speaker models. Because of that, I'll be basing my new processor on that file. Note This was also how I based both EnglishIPAProcessor and IndonesianIPAProcessor . To begin, we have to define a hard-coded list of symbols (vocabulary) that the processor will support. This is highly dependent on the dataset that you're using, as we would want to cover all possible graphemes (so none of them become unknowns). In my case, there's the 26 latin alphabet characters A-Z, plus two additional E's with diacritics: \u00e9 and \u00e8. tensorflow_tts/processor/javanese_char.py valid_symbols = [ \"a\" , \"b\" , \"c\" , \"d\" , \"e\" , \"f\" , \"g\" , \"h\" , \"i\" , \"j\" , \"k\" , \"l\" , \"m\" , \"n\" , \"o\" , \"p\" , \"q\" , \"r\" , \"s\" , \"t\" , \"u\" , \"v\" , \"w\" , \"x\" , \"y\" , \"z\" , \"\u00e8\" , \"\u00e9\" , ] Be sure to inspect your dataset and cover all possible tokens. Next would be punctuations. Again, this is dependent on how you would like to design the processor and which punctuations you would like to keep. I normally only go for ! , . ? ; : and nothing else. Anything other than those will simply be ignored. Note that this will be important during inference -- anything outside of the vocabulary will be ignored! tensorflow_tts/processor/javanese_char.py _punctuation = \"!,.?;:\" There are also additional special tokens such as @SIL , @EOS , @PAD for silence, end-of-sentence, and padding tokens, respectively. tensorflow_tts/processor/javanese_char.py _sil = \"@SIL\" _eos = \"@EOS\" _pad = \"@PAD\" _char = [ \"@\" + s for s in valid_symbols ] JAVANESE_CHARACTER_SYMBOLS = [ _pad ] + _char + list ( _punctuation ) + [ _sil ] + [ _eos ]","title":"Vocabulary"},{"location":"guides/custom_processor/#metadata-format","text":"An important method in LibriTTSProcessor -based processors is how metadata is formatted and later read. Sticking to the original implementation, I will keep the default format of having a metadata called train.txt , which is populated with | -delimited lines. Each line has 3 columns (in this particular order): Path to file Text read Speaker name Moreover, audios are expected to be of .wav format. These are then implemented as class variables inside the processor and influences how the create_items and get_one_sample methods behave. create_items \"creates\" each training sample, and get_one_sample loads each training sample in their right formats (e.g. read audio via SoundFile, convert text to sequence of IDs, etc.) tensorflow_tts/processor/javanese_char.py @dataclass class JavaneseCharacterProcessor ( BaseProcessor ): mode : str = \"train\" train_f_name : str = \"train.txt\" positions = { \"file\" : 0 , \"text\" : 1 , \"speaker_name\" : 2 , } # positions of file,text,speaker_name after split line f_extension : str = \".wav\" cleaner_names : str = None def create_items ( self ): with open ( os . path . join ( self . data_dir , self . train_f_name ), mode = \"r\" , encoding = \"utf-8\" ) as f : for line in f : parts = line . strip () . split ( self . delimiter ) wav_path = os . path . join ( self . data_dir , parts [ self . positions [ \"file\" ]]) wav_path = ( wav_path + self . f_extension if wav_path [ - len ( self . f_extension ) :] != self . f_extension else wav_path ) text = parts [ self . positions [ \"text\" ]] speaker_name = parts [ self . positions [ \"speaker_name\" ]] self . items . append ([ text , wav_path , speaker_name ]) def get_one_sample ( self , item ): text , wav_path , speaker_name = item audio , rate = sf . read ( wav_path , dtype = \"float32\" ) text_ids = np . asarray ( self . text_to_sequence ( text ), np . int32 ) sample = { \"raw_text\" : text , \"text_ids\" : text_ids , \"audio\" : audio , \"utt_id\" : wav_path . split ( \"/\" )[ - 1 ] . split ( \".\" )[ 0 ], \"speaker_name\" : speaker_name , \"rate\" : rate , } return sample ...","title":"Metadata Format"},{"location":"guides/custom_processor/#text-to-sequence","text":"Then, the only remaining crucial feature to implement is how we will be converting texts into sequence of IDs. Again, following the original implementation of LibriTTSProcessor , our processor will have two modes: train and eval. For training, we expect our text is training-ready. What I meant by that is, in the metadata file, all texts have been pre-converted into tokens and are separated by whitespaces, for example: k a p i n g l i m a n i n d a a k e k a j i SIL Because of that, we would not need to re-separate them and could simply encode/map them to their corresponding IDs via the symbols_to_id method. On the other hand, we would probably want to have a separate behavior for inference purposes. tensorflow_tts/processor/javanese_char.py @dataclass class JavaneseCharacterProcessor ( BaseProcessor ): ... def text_to_sequence ( self , text ): if ( self . mode == \"train\" ): # in train mode text should be already transformed to characters return self . symbols_to_ids ( self . clean_char ( text . split ())) else : return self . inference_text_to_seq ( text ) def inference_text_to_seq ( self , text : str ): return self . symbols_to_ids ( self . text_to_char ( text )) def symbols_to_ids ( self , symbols_list : list ): return [ self . symbol_to_id [ s ] for s in symbols_list ] ... The code above should be relatively self-explanatory. On training mode, we simply split tokens by whitespace, clean them, and encode them as IDs. On inference, we first need to convert texts to characters, and only then encode them as IDs. Converting texts to characters is quite straightforward for the case of character-based processors. We would just need to iterate through each characters and that's it! However, for more complex processors that involve g2p conversion, this is probably where you'd want to integrate it. For instance, in EnglishIPAProcessor , this is where we pass the job to gruut: tensorflow_tts/processor/english_ipa.py @dataclass class EnglishIPAProcessor ( BaseProcessor ): ... def text_to_ph ( self , text : str ): phn_arr = [] for words in sentences ( text ): for word in words : if word . is_major_break or word . is_minor_break : phn_arr += [ word . text ] elif word . phonemes : phn_arr += word . phonemes return self . clean_g2p ( phn_arr ) ... And finally, we need to implement how we're going to \"clean\" tokens, namely, separating punctuations from actual character tokens and special tokens (the latter two beginning with an @ ): tensorflow_tts/processor/javanese_char.py @dataclass class JavaneseCharacterProcessor ( BaseProcessor ): ... def clean_char ( self , characters : list ): data = [] for char in characters : if char in _punctuation : data . append ( char ) elif char != \" \" : data . append ( \"@\" + char . lower ()) return data One thing not to miss is adding our new processor class to tensorflow_tts/processor/__init__.py : tensorflow_tts/processor/__init__.py from tensorflow_tts.processor.javanese_char import JavaneseCharacterProcessor","title":"Text-to-Sequence"},{"location":"guides/custom_processor/#preprocess","text":"Once you're done with implementing the new processor class, you'll need to also register it to the pre-processor. It's fairly simple to do, with only a few additional lines to add in certain lines: tensorflow_tts/bin/preprocess.py from tensorflow_tts.processor import JavaneseCharacterProcessor from tensorflow_tts.processor.javanese_char import JAVANESE_CHARACTER_SYMBOLS tensorflow_tts/bin/preprocess.py ... parser.add_argument( \"--dataset\", type=str, default=\"ljspeech\", choices=[ \"ljspeech\", \"ljspeech_multi\", \"kss\", \"libritts\", \"baker\", \"thorsten\", \"ljspeechu\", \"synpaflex\", \"jsut\", \"indonesianipa\", \"englishipa\", + \"javanesechar\", # what we're going to call our processor ], help=\"Dataset to preprocess.\", ) ... def preprocess(): \"\"\"Run preprocessing process and compute statistics for normalizing.\"\"\" config = parse_and_config() dataset_processor = { \"ljspeech\": LJSpeechProcessor, \"ljspeech_multi\": LJSpeechMultiProcessor, \"kss\": KSSProcessor, \"libritts\": LibriTTSProcessor, \"baker\": BakerProcessor, \"thorsten\": ThorstenProcessor, \"ljspeechu\": LJSpeechUltimateProcessor, \"synpaflex\": SynpaflexProcessor, \"jsut\": JSUTProcessor, \"indonesianipa\": IndonesianIPAProcessor, \"englishipa\": EnglishIPAProcessor, + \"javanesechar\": JavaneseCharacterProcessor, } dataset_symbol = { \"ljspeech\": LJSPEECH_SYMBOLS, \"ljspeech_multi\": LJSPEECH_SYMBOLS, \"kss\": KSS_SYMBOLS, \"libritts\": LIBRITTS_SYMBOLS, \"baker\": BAKER_SYMBOLS, \"thorsten\": THORSTEN_SYMBOLS, \"ljspeechu\": LJSPEECH_U_SYMBOLS, \"synpaflex\": SYNPAFLEX_SYMBOLS, \"jsut\": JSUT_SYMBOLS, \"indonesianipa\": INDONESIAN_IPA_SYMBOLS, \"englishipa\": ENGLISH_IPA_SYMBOLS, + \"javanesechar\": JAVANESE_CHARACTER_SYMBOLS, } dataset_cleaner = { \"ljspeech\": \"english_cleaners\", \"ljspeech_multi\": \"english_cleaners\", \"kss\": \"korean_cleaners\", \"libritts\": None, \"baker\": None, \"thorsten\": \"german_cleaners\", \"ljspeechu\": \"english_cleaners\", \"synpaflex\": \"basic_cleaners\", \"jsut\": None, \"indonesianipa\": None, \"englishipa\": None, + \"javanesechar\": None, }","title":"Preprocess"},{"location":"guides/custom_processor/#integrating-with-text2mel-models","text":"Lastly, we need to integrate our processor with existing text2mel models, such as FastSpeech2 . The number of tokens in the processor will be used as the vocabulary size of the text2mel model. tensorflow_tts/configs/fastspeech.py from tensorflow_tts.processor.javanese_char import ( JAVANESE_CHARACTER_SYMBOLS as javanese_char_symbols , ) tensorflow_tts/configs/fastspeech.py ... if dataset == \"ljspeech\": self.vocab_size = vocab_size elif dataset == \"kss\": self.vocab_size = len(kss_symbols) elif dataset == \"baker\": self.vocab_size = len(bk_symbols) elif dataset == \"libritts\": self.vocab_size = len(lbri_symbols) elif dataset == \"jsut\": self.vocab_size = len(jsut_symbols) elif dataset == \"ljspeechu\": self.vocab_size = len(lju_symbols) elif dataset == \"indonesianipa\": self.vocab_size = len(indonesian_ipa_symbols) elif dataset == \"englishipa\": self.vocab_size = len(english_ipa_symbols) + elif dataset == \"javanesechar\": + self.vocab_size = len(javanese_char_symbols) else: raise ValueError(\"No such dataset: {}\".format(dataset)) ...","title":"Integrating with text2mel Models"},{"location":"guides/custom_processor/#autoprocessor","text":"A handy feature found in TensorFlowTTS is the ability to load processors from HuggingFace Hub. This allows you to do something like: from tensorflow_tts.inference import AutoProcessor processor = AutoProcessor . from_pretrained ( \"bookbot/lightspeech-mfa-id\" ) To allow such support for our custom processor, we simply have to add it to tensorflow_tts/inference/auto_processor.py : tensorflow_tts/inference/auto_processor.py from tensorflow_tts.processor import ( LJSpeechProcessor, KSSProcessor, BakerProcessor, LibriTTSProcessor, ThorstenProcessor, LJSpeechUltimateProcessor, SynpaflexProcessor, JSUTProcessor, LJSpeechMultiProcessor, IndonesianIPAProcessor, EnglishIPAProcessor, + JavaneseCharacterProcessor, ) from tensorflow_tts.utils import CACHE_DIRECTORY, PROCESSOR_FILE_NAME, LIBRARY_NAME from tensorflow_tts import __version__ as VERSION from huggingface_hub import hf_hub_url, cached_download CONFIG_MAPPING = OrderedDict( [ (\"LJSpeechProcessor\", LJSpeechProcessor), (\"LJSpeechMultiProcessor\", LJSpeechMultiProcessor), (\"KSSProcessor\", KSSProcessor), (\"BakerProcessor\", BakerProcessor), (\"LibriTTSProcessor\", LibriTTSProcessor), (\"ThorstenProcessor\", ThorstenProcessor), (\"LJSpeechUltimateProcessor\", LJSpeechUltimateProcessor), (\"SynpaflexProcessor\", SynpaflexProcessor), (\"JSUTProcessor\", JSUTProcessor), (\"IndonesianIPAProcessor\", IndonesianIPAProcessor), (\"EnglishIPAProcessor\", EnglishIPAProcessor), + (\"JavaneseCharacterProcessor\", JavaneseCharacterProcessor), ] )","title":"AutoProcessor"},{"location":"guides/installation/","text":"Installation Local Machines I highly recommend installing TensorFlowTTS (and TensorFlow) on a designated Conda environment. I personally prefer Miniconda over Anaconda, but either one works. To begin with, follow this guide to install Conda, and then create a new Python 3.9 environment, which I will call tensorflow . conda create -n tensorflow python = 3 .9 conda activate tensorflow In the new environment, I will install TensorFlow v2.3.1 which I have found to work for training and inference later. You can install it via pip . pip install tensorflow == 2 .3.1 Afterwards, clone the forked repository and install the library plus all of its requirements. git clone https://github.com/w11wo/TensorFlowTTS.git cd TensorFlowTTS pip install . Google Cloud Virtual Machines Installing TensorFlowTTS on a Google Cloud VM is similar to installing on a local machine. To make things easier, Google has provided us with a list of pre-built VM images that comes with TensorFlow and support for GPUs. I would go for the image: Debian 10 based Deep Learning VM for TensorFlow Enterprise 2.6 with CUDA 11.0 . Because the image already has TensorFlow installed, we just need to install the main library like the steps above git clone https://github.com/w11wo/TensorFlowTTS.git cd TensorFlowTTS pip install . For some reason, there will be a bug involving Numba, which we can easily solve by upgrading NumPy to the latest version pip install -U numpy And also install libsndfile1 via apt sudo apt-get install libsndfile1 Checking for a Successful Install A way to check if your installation is correct is by importing the library through Python. We can do so through command line. python -c \"import tensorflow_tts\" If no errors are raised, then we should be good to go!","title":"Installation"},{"location":"guides/installation/#installation","text":"","title":"Installation"},{"location":"guides/installation/#local-machines","text":"I highly recommend installing TensorFlowTTS (and TensorFlow) on a designated Conda environment. I personally prefer Miniconda over Anaconda, but either one works. To begin with, follow this guide to install Conda, and then create a new Python 3.9 environment, which I will call tensorflow . conda create -n tensorflow python = 3 .9 conda activate tensorflow In the new environment, I will install TensorFlow v2.3.1 which I have found to work for training and inference later. You can install it via pip . pip install tensorflow == 2 .3.1 Afterwards, clone the forked repository and install the library plus all of its requirements. git clone https://github.com/w11wo/TensorFlowTTS.git cd TensorFlowTTS pip install .","title":"Local Machines"},{"location":"guides/installation/#google-cloud-virtual-machines","text":"Installing TensorFlowTTS on a Google Cloud VM is similar to installing on a local machine. To make things easier, Google has provided us with a list of pre-built VM images that comes with TensorFlow and support for GPUs. I would go for the image: Debian 10 based Deep Learning VM for TensorFlow Enterprise 2.6 with CUDA 11.0 . Because the image already has TensorFlow installed, we just need to install the main library like the steps above git clone https://github.com/w11wo/TensorFlowTTS.git cd TensorFlowTTS pip install . For some reason, there will be a bug involving Numba, which we can easily solve by upgrading NumPy to the latest version pip install -U numpy And also install libsndfile1 via apt sudo apt-get install libsndfile1","title":"Google Cloud Virtual Machines"},{"location":"guides/installation/#checking-for-a-successful-install","text":"A way to check if your installation is correct is by importing the library through Python. We can do so through command line. python -c \"import tensorflow_tts\" If no errors are raised, then we should be good to go!","title":"Checking for a Successful Install"},{"location":"guides/tensorflowlite/","text":"Convert and Infer Models on TensorFlowLite Everything here can be followed along in Google Colab! Installation and Setup We need to first install TensorFlowTTS, which is a fork of the original repository developed by w11wo . ! git clone - q https : // github . com / w11wo / TensorFlowTTS . git ! cd TensorFlowTTS && pip install - q . > / dev / null Then, we'll need to downgrade TensorFlow to version 2.3.1 . This is the version that I found to be working well all the way to mobile deployment. You could probably get away with the newer versions, nonetheless. ! pip install - q tensorflow - gpu == 2.3.1 We also need to downgrade NumPy to the right version for this TensorFlow version. ! pip install - q numpy == 1.20.3 IMPORTANT : after re-installing TensorFlow and NumPy, be sure to restart your Colab Runtime! Log into HuggingFace Hub If you have previously saved your model weights in HuggingFace Hub, it'll be immensely easier to load them back. Private Hub models can also be loaded, so long as you first log in to the Hub, which we'll do via notebook_login . from huggingface_hub import notebook_login notebook_login () Token is valid. Your token has been saved in your configured git credential helpers (store). Your token has been saved to /root/.huggingface/token Login successful To load a private Hub model, you just have to specify use_auth_token=True later. Convert Models Typically, converting to a TensorFlowLite model involves these steps: Loading the model weights Getting the concrete function of the model's forward pass Setting up the converter Specifying optimizations Convert and save TFLite model import tensorflow as tf from tensorflow_tts.inference import TFAutoModel /usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.3.0 (nightly versions are not supported). The versions of TensorFlow you are currently using is 2.3.1 and is not supported. Some things might work, some things might not. If you were to encounter a bug, do not file an issue. If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. You can find the compatibility matrix in TensorFlow Addon's readme: https://github.com/tensorflow/addons UserWarning, [nltk_data] Downloading package averaged_perceptron_tagger to [nltk_data] /root/nltk_data... [nltk_data] Unzipping taggers/averaged_perceptron_tagger.zip. [nltk_data] Downloading package cmudict to /root/nltk_data... [nltk_data] Unzipping corpora/cmudict.zip. [nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data] Unzipping tokenizers/punkt.zip. def convert_text2mel_tflite ( model_path : str , save_name : str , use_auth_token : bool = False ) -> float : \"\"\"Convert text2mel model to TFLite. Args: model_path (str): Pretrained model checkpoint in HuggingFace Hub. save_name (str): TFLite file savename. use_auth_token (bool, optional): Use HF Hub Token. Defaults to False. Returns: float: Model size in Megabytes. \"\"\" # load pretrained model model = TFAutoModel . from_pretrained ( model_path , enable_tflite_convertible = True , use_auth_token = use_auth_token ) # setup model concrete function concrete_function = model . inference_tflite . get_concrete_function () converter = tf . lite . TFLiteConverter . from_concrete_functions ([ concrete_function ]) # specify optimizations converter . optimizations = [ tf . lite . Optimize . DEFAULT ] converter . target_spec . supported_ops = [ tf . lite . OpsSet . TFLITE_BUILTINS , # quantize tf . lite . OpsSet . SELECT_TF_OPS , ] # convert and save model to TensorFlowLite tflite_model = converter . convert () with open ( save_name , \"wb\" ) as f : f . write ( tflite_model ) size = len ( tflite_model ) / 1024 / 1024.0 return size def convert_vocoder_tflite ( model_path : str , save_name : str , use_auth_token : bool = False ) -> float : \"\"\"Convert vocoder model to TFLite. Args: model_path (str): Pretrained model checkpoint in HuggingFace Hub. save_name (str): TFLite file savename. use_auth_token (bool, optional): Use HF Hub Token. Defaults to False. Returns: float: Model size in Megabytes. \"\"\" # load pretrained model model = TFAutoModel . from_pretrained ( model_path , use_auth_token = use_auth_token ) # setup model concrete function concrete_function = model . inference_tflite . get_concrete_function () converter = tf . lite . TFLiteConverter . from_concrete_functions ([ concrete_function ]) # specify optimizations converter . optimizations = [ tf . lite . Optimize . DEFAULT ] converter . target_spec . supported_ops = [ tf . lite . OpsSet . SELECT_TF_OPS ] converter . target_spec . supported_types = [ tf . float16 ] # fp16 ops # convert and save model to TensorFlowLite tflite_model = converter . convert () with open ( save_name , \"wb\" ) as f : f . write ( tflite_model ) size = len ( tflite_model ) / 1024 / 1024.0 return size text2mel = convert_text2mel_tflite ( model_path = \"bookbot/lightspeech-mfa-id-v3\" , save_name = \"lightspeech_quant.tflite\" , use_auth_token = True , ) vocoder = convert_vocoder_tflite ( model_path = \"bookbot/mb-melgan-hifi-postnets-id-v10\" , save_name = \"mbmelgan.tflite\" , use_auth_token = True , ) /usr/local/lib/python3.7/dist-packages/huggingface_hub/file_download.py:595: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download` FutureWarning, Downloading: 0%| | 0.00/19.5M [00:00<?, ?B/s] Downloading: 0%| | 0.00/1.89k [00:00<?, ?B/s] Downloading: 0%| | 0.00/10.2M [00:00<?, ?B/s] Downloading: 0%| | 0.00/2.53k [00:00<?, ?B/s] print ( f \"Text2mel: { text2mel } MBs \\n Vocoder: { vocoder } MBs\" ) Text2mel: 4.323883056640625 MBs Vocoder: 5.0258941650390625 MBs Conversion Script We also provide an script version of the conversion steps above, which can be found here . To use it, you just have to specify the arguments through the command line. An example is as follows: !python TensorFlowTTS/examples/tensorflowlite/convert_tflite.py \\ --text2mel_path = \"bookbot/lightspeech-mfa-id-v3\" \\ --text2mel_savename = \"lightspeech_quant.tflite\" \\ --vocoder_path = \"bookbot/mb-melgan-hifi-postnets-id-v10\" \\ --vocoder_savename = \"mbmelgan.tflite\" \\ --use_auth_token Inference With the converted TFLite models, we can then perform inference on TFLite Runtime. Here, we'll only be presenting a way to perform inference for LightSpeech + Multi-band MelGAN. Other models might differ (e.g. FastSpeech2 has different model outputs compared to LightSpeech). However, adapting the inference code to other models should be fairly doable given that you know the outputs of each model. Tokenization To apply tokenization to our raw text, we can simply load the processor (tokenizer) we used during training. Again, if it's stored to the HuggingFace Hub, you can conveniently load it from it during inference. You could optionally specify if it's private, and load it the same way as you would load a private Hub model. from tensorflow_tts.inference import AutoProcessor processor = AutoProcessor . from_pretrained ( \"bookbot/lightspeech-mfa-id-v3\" , use_auth_token = True ) processor . mode = \"eval\" # change processor from train to eval mode Downloading: 0%| | 0.00/1.04k [00:00<?, ?B/s] With the processor, we can then tokenize any input text and convert them to its correponding input ids (list of token IDs). from typing import List , Tuple def tokenize ( text : str , processor : AutoProcessor ) -> List [ int ]: \"\"\"Tokenize text to input ids. Args: text (str): Input text to tokenize. processor (AutoProcessor): Processor for tokenization. Returns: List[int]: List of input (token) ids. \"\"\" return processor . text_to_sequence ( text ) text = \"Halo, bagaimana kabar mu?\" input_ids = tokenize ( text , processor ) input_ids [8, 1, 12, 15, 32, 2, 1, 7, 1, 9, 13, 1, 14, 1, 11, 1, 2, 1, 17, 13, 20, 34] Prepare LightSpeech Input LightSpeech expects 5 inputs for inference, namely: Input IDs Speaker ID Speed Ratio Pitch Ratio Energy Ratio Speaker ID is only relevant for a multi-speaker model, with each index (starting from 0) corresponding to different speaker embeddings for the text2mel model to use. You can also alter other options such as speed, which serves like a duration multiplier (i.e. speed ratio of 2 is half the normal speed). You could also alter the pitch and energy in a similar way. For simplicity, I'll just be parameterizing the speaker ID. def prepare_input ( input_ids : List [ str ], speaker : int ) -> Tuple [ tf . Tensor , tf . Tensor , tf . Tensor , tf . Tensor , tf . Tensor ]: \"\"\"Prepares input for LightSpeech TFLite inference. Args: input_ids (List[str]): Phoneme input ids according to processor. speaker (int): Speaker ID. Returns: Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]: Tuple of tensors consisting of: 1. Input IDs 2. Speaker ID 3. Speed Ratio 4. Pitch Ratio 5. Energy Ratio \"\"\" input_ids = tf . expand_dims ( tf . convert_to_tensor ( input_ids , dtype = tf . int32 ), 0 ) return ( input_ids , tf . convert_to_tensor ([ speaker ], tf . int32 ), tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), ) Inference To perform inference on a TensorFlowLite Runtime, the general flow is as follows: Load model weights to TFLite Interpreter Resize interpreter input tensors according to actual input Allocate tensors according to Interpreter's inputs Set the actual tensor values as inputs to the Interpreter Invoke interpreter Return output tensors We can follow the steps outlined above and create an inference function for LightSpeech and MB-MelGAN. def ls_infer ( input_ids : List [ str ], speaker : int , lightspeech_path : str ) -> Tuple [ tf . Tensor , tf . Tensor , tf . Tensor ]: \"\"\"Performs LightSpeech inference. Args: input_ids (List[str]): Phoneme input ids according to processor. speaker (int): Speaker ID. lightspeech_path (str): Path to LightSpeech weights. Returns: Tuple[tf.Tensor, tf.Tensor, tf.Tensor]: Tuple of tensors consisting of: 1. Mel-spectrogram output 2. Durations array \"\"\" # load model to Interpreter lightspeech = tf . lite . Interpreter ( model_path = lightspeech_path ) input_details = lightspeech . get_input_details () output_details = lightspeech . get_output_details () # resize input tensors according to actual shape lightspeech . resize_tensor_input ( input_details [ 0 ][ \"index\" ], [ 1 , len ( input_ids )]) lightspeech . resize_tensor_input ( input_details [ 1 ][ \"index\" ], [ 1 ]) lightspeech . resize_tensor_input ( input_details [ 2 ][ \"index\" ], [ 1 ]) lightspeech . resize_tensor_input ( input_details [ 3 ][ \"index\" ], [ 1 ]) lightspeech . resize_tensor_input ( input_details [ 4 ][ \"index\" ], [ 1 ]) # allocate tensors lightspeech . allocate_tensors () input_data = prepare_input ( input_ids , speaker ) # set input tensors for i , detail in enumerate ( input_details ): lightspeech . set_tensor ( detail [ \"index\" ], input_data [ i ]) # invoke interpreter lightspeech . invoke () # return outputs return ( lightspeech . get_tensor ( output_details [ 0 ][ \"index\" ]), lightspeech . get_tensor ( output_details [ 1 ][ \"index\" ]), ) def melgan_infer ( melspectrogram : tf . Tensor , mb_melgan_path : str ) -> tf . Tensor : \"\"\"Performs MB-MelGAN inference. Args: melspectrogram (tf.Tensor): Mel-spectrogram to synthesize. mb_melgan_path (str): Path to MB-MelGAN weights. Returns: tf.Tensor: Synthesized audio output tensor. \"\"\" # load model to Interpreter mb_melgan = tf . lite . Interpreter ( model_path = mb_melgan_path ) input_details = mb_melgan . get_input_details () output_details = mb_melgan . get_output_details () # resize input tensors according to actual shape mb_melgan . resize_tensor_input ( input_details [ 0 ][ \"index\" ], [ 1 , melspectrogram . shape [ 1 ], melspectrogram . shape [ 2 ]], strict = True , ) # allocate tensors mb_melgan . allocate_tensors () # set input tensors mb_melgan . set_tensor ( input_details [ 0 ][ \"index\" ], melspectrogram ) # invoke interpreter mb_melgan . invoke () # return output return mb_melgan . get_tensor ( output_details [ 0 ][ \"index\" ]) Finally, we can perform inference with the model weights which we have converted earlier and run an end-to-end inference. mel_output_tflite , * _ = ls_infer ( input_ids , speaker = 0 , lightspeech_path = \"lightspeech_quant.tflite\" ) audio_tflite = melgan_infer ( mel_output_tflite , mb_melgan_path = \"mbmelgan.tflite\" )[ 0 , :, 0 ] To listen to the synthesized output via Jupyter, we can directly pass the outputs to IPython's Audio widget, while specifying the sample rate of the audio. from IPython.display import Audio Audio ( data = audio_tflite , rate = 32000 ) Your browser does not support the audio element. Alternatively, we can write the output audio tensors to file. import soundfile as sf sf . write ( \"./audio.wav\" , audio_tflite , 32000 , \"PCM_16\" ) from IPython.display import Audio Audio ( \"audio.wav\" ) Your browser does not support the audio element.","title":"Convert and Infer Models on TensorFlowLite"},{"location":"guides/tensorflowlite/#convert-and-infer-models-on-tensorflowlite","text":"Everything here can be followed along in Google Colab!","title":"Convert and Infer Models on TensorFlowLite"},{"location":"guides/tensorflowlite/#installation-and-setup","text":"We need to first install TensorFlowTTS, which is a fork of the original repository developed by w11wo . ! git clone - q https : // github . com / w11wo / TensorFlowTTS . git ! cd TensorFlowTTS && pip install - q . > / dev / null Then, we'll need to downgrade TensorFlow to version 2.3.1 . This is the version that I found to be working well all the way to mobile deployment. You could probably get away with the newer versions, nonetheless. ! pip install - q tensorflow - gpu == 2.3.1 We also need to downgrade NumPy to the right version for this TensorFlow version. ! pip install - q numpy == 1.20.3 IMPORTANT : after re-installing TensorFlow and NumPy, be sure to restart your Colab Runtime!","title":"Installation and Setup"},{"location":"guides/tensorflowlite/#log-into-huggingface-hub","text":"If you have previously saved your model weights in HuggingFace Hub, it'll be immensely easier to load them back. Private Hub models can also be loaded, so long as you first log in to the Hub, which we'll do via notebook_login . from huggingface_hub import notebook_login notebook_login () Token is valid. Your token has been saved in your configured git credential helpers (store). Your token has been saved to /root/.huggingface/token Login successful To load a private Hub model, you just have to specify use_auth_token=True later.","title":"Log into HuggingFace Hub"},{"location":"guides/tensorflowlite/#convert-models","text":"Typically, converting to a TensorFlowLite model involves these steps: Loading the model weights Getting the concrete function of the model's forward pass Setting up the converter Specifying optimizations Convert and save TFLite model import tensorflow as tf from tensorflow_tts.inference import TFAutoModel /usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.3.0 (nightly versions are not supported). The versions of TensorFlow you are currently using is 2.3.1 and is not supported. Some things might work, some things might not. If you were to encounter a bug, do not file an issue. If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. You can find the compatibility matrix in TensorFlow Addon's readme: https://github.com/tensorflow/addons UserWarning, [nltk_data] Downloading package averaged_perceptron_tagger to [nltk_data] /root/nltk_data... [nltk_data] Unzipping taggers/averaged_perceptron_tagger.zip. [nltk_data] Downloading package cmudict to /root/nltk_data... [nltk_data] Unzipping corpora/cmudict.zip. [nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data] Unzipping tokenizers/punkt.zip. def convert_text2mel_tflite ( model_path : str , save_name : str , use_auth_token : bool = False ) -> float : \"\"\"Convert text2mel model to TFLite. Args: model_path (str): Pretrained model checkpoint in HuggingFace Hub. save_name (str): TFLite file savename. use_auth_token (bool, optional): Use HF Hub Token. Defaults to False. Returns: float: Model size in Megabytes. \"\"\" # load pretrained model model = TFAutoModel . from_pretrained ( model_path , enable_tflite_convertible = True , use_auth_token = use_auth_token ) # setup model concrete function concrete_function = model . inference_tflite . get_concrete_function () converter = tf . lite . TFLiteConverter . from_concrete_functions ([ concrete_function ]) # specify optimizations converter . optimizations = [ tf . lite . Optimize . DEFAULT ] converter . target_spec . supported_ops = [ tf . lite . OpsSet . TFLITE_BUILTINS , # quantize tf . lite . OpsSet . SELECT_TF_OPS , ] # convert and save model to TensorFlowLite tflite_model = converter . convert () with open ( save_name , \"wb\" ) as f : f . write ( tflite_model ) size = len ( tflite_model ) / 1024 / 1024.0 return size def convert_vocoder_tflite ( model_path : str , save_name : str , use_auth_token : bool = False ) -> float : \"\"\"Convert vocoder model to TFLite. Args: model_path (str): Pretrained model checkpoint in HuggingFace Hub. save_name (str): TFLite file savename. use_auth_token (bool, optional): Use HF Hub Token. Defaults to False. Returns: float: Model size in Megabytes. \"\"\" # load pretrained model model = TFAutoModel . from_pretrained ( model_path , use_auth_token = use_auth_token ) # setup model concrete function concrete_function = model . inference_tflite . get_concrete_function () converter = tf . lite . TFLiteConverter . from_concrete_functions ([ concrete_function ]) # specify optimizations converter . optimizations = [ tf . lite . Optimize . DEFAULT ] converter . target_spec . supported_ops = [ tf . lite . OpsSet . SELECT_TF_OPS ] converter . target_spec . supported_types = [ tf . float16 ] # fp16 ops # convert and save model to TensorFlowLite tflite_model = converter . convert () with open ( save_name , \"wb\" ) as f : f . write ( tflite_model ) size = len ( tflite_model ) / 1024 / 1024.0 return size text2mel = convert_text2mel_tflite ( model_path = \"bookbot/lightspeech-mfa-id-v3\" , save_name = \"lightspeech_quant.tflite\" , use_auth_token = True , ) vocoder = convert_vocoder_tflite ( model_path = \"bookbot/mb-melgan-hifi-postnets-id-v10\" , save_name = \"mbmelgan.tflite\" , use_auth_token = True , ) /usr/local/lib/python3.7/dist-packages/huggingface_hub/file_download.py:595: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download` FutureWarning, Downloading: 0%| | 0.00/19.5M [00:00<?, ?B/s] Downloading: 0%| | 0.00/1.89k [00:00<?, ?B/s] Downloading: 0%| | 0.00/10.2M [00:00<?, ?B/s] Downloading: 0%| | 0.00/2.53k [00:00<?, ?B/s] print ( f \"Text2mel: { text2mel } MBs \\n Vocoder: { vocoder } MBs\" ) Text2mel: 4.323883056640625 MBs Vocoder: 5.0258941650390625 MBs","title":"Convert Models"},{"location":"guides/tensorflowlite/#conversion-script","text":"We also provide an script version of the conversion steps above, which can be found here . To use it, you just have to specify the arguments through the command line. An example is as follows: !python TensorFlowTTS/examples/tensorflowlite/convert_tflite.py \\ --text2mel_path = \"bookbot/lightspeech-mfa-id-v3\" \\ --text2mel_savename = \"lightspeech_quant.tflite\" \\ --vocoder_path = \"bookbot/mb-melgan-hifi-postnets-id-v10\" \\ --vocoder_savename = \"mbmelgan.tflite\" \\ --use_auth_token","title":"Conversion Script"},{"location":"guides/tensorflowlite/#inference","text":"With the converted TFLite models, we can then perform inference on TFLite Runtime. Here, we'll only be presenting a way to perform inference for LightSpeech + Multi-band MelGAN. Other models might differ (e.g. FastSpeech2 has different model outputs compared to LightSpeech). However, adapting the inference code to other models should be fairly doable given that you know the outputs of each model.","title":"Inference"},{"location":"guides/tensorflowlite/#tokenization","text":"To apply tokenization to our raw text, we can simply load the processor (tokenizer) we used during training. Again, if it's stored to the HuggingFace Hub, you can conveniently load it from it during inference. You could optionally specify if it's private, and load it the same way as you would load a private Hub model. from tensorflow_tts.inference import AutoProcessor processor = AutoProcessor . from_pretrained ( \"bookbot/lightspeech-mfa-id-v3\" , use_auth_token = True ) processor . mode = \"eval\" # change processor from train to eval mode Downloading: 0%| | 0.00/1.04k [00:00<?, ?B/s] With the processor, we can then tokenize any input text and convert them to its correponding input ids (list of token IDs). from typing import List , Tuple def tokenize ( text : str , processor : AutoProcessor ) -> List [ int ]: \"\"\"Tokenize text to input ids. Args: text (str): Input text to tokenize. processor (AutoProcessor): Processor for tokenization. Returns: List[int]: List of input (token) ids. \"\"\" return processor . text_to_sequence ( text ) text = \"Halo, bagaimana kabar mu?\" input_ids = tokenize ( text , processor ) input_ids [8, 1, 12, 15, 32, 2, 1, 7, 1, 9, 13, 1, 14, 1, 11, 1, 2, 1, 17, 13, 20, 34]","title":"Tokenization"},{"location":"guides/tensorflowlite/#prepare-lightspeech-input","text":"LightSpeech expects 5 inputs for inference, namely: Input IDs Speaker ID Speed Ratio Pitch Ratio Energy Ratio Speaker ID is only relevant for a multi-speaker model, with each index (starting from 0) corresponding to different speaker embeddings for the text2mel model to use. You can also alter other options such as speed, which serves like a duration multiplier (i.e. speed ratio of 2 is half the normal speed). You could also alter the pitch and energy in a similar way. For simplicity, I'll just be parameterizing the speaker ID. def prepare_input ( input_ids : List [ str ], speaker : int ) -> Tuple [ tf . Tensor , tf . Tensor , tf . Tensor , tf . Tensor , tf . Tensor ]: \"\"\"Prepares input for LightSpeech TFLite inference. Args: input_ids (List[str]): Phoneme input ids according to processor. speaker (int): Speaker ID. Returns: Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]: Tuple of tensors consisting of: 1. Input IDs 2. Speaker ID 3. Speed Ratio 4. Pitch Ratio 5. Energy Ratio \"\"\" input_ids = tf . expand_dims ( tf . convert_to_tensor ( input_ids , dtype = tf . int32 ), 0 ) return ( input_ids , tf . convert_to_tensor ([ speaker ], tf . int32 ), tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), )","title":"Prepare LightSpeech Input"},{"location":"guides/tensorflowlite/#inference_1","text":"To perform inference on a TensorFlowLite Runtime, the general flow is as follows: Load model weights to TFLite Interpreter Resize interpreter input tensors according to actual input Allocate tensors according to Interpreter's inputs Set the actual tensor values as inputs to the Interpreter Invoke interpreter Return output tensors We can follow the steps outlined above and create an inference function for LightSpeech and MB-MelGAN. def ls_infer ( input_ids : List [ str ], speaker : int , lightspeech_path : str ) -> Tuple [ tf . Tensor , tf . Tensor , tf . Tensor ]: \"\"\"Performs LightSpeech inference. Args: input_ids (List[str]): Phoneme input ids according to processor. speaker (int): Speaker ID. lightspeech_path (str): Path to LightSpeech weights. Returns: Tuple[tf.Tensor, tf.Tensor, tf.Tensor]: Tuple of tensors consisting of: 1. Mel-spectrogram output 2. Durations array \"\"\" # load model to Interpreter lightspeech = tf . lite . Interpreter ( model_path = lightspeech_path ) input_details = lightspeech . get_input_details () output_details = lightspeech . get_output_details () # resize input tensors according to actual shape lightspeech . resize_tensor_input ( input_details [ 0 ][ \"index\" ], [ 1 , len ( input_ids )]) lightspeech . resize_tensor_input ( input_details [ 1 ][ \"index\" ], [ 1 ]) lightspeech . resize_tensor_input ( input_details [ 2 ][ \"index\" ], [ 1 ]) lightspeech . resize_tensor_input ( input_details [ 3 ][ \"index\" ], [ 1 ]) lightspeech . resize_tensor_input ( input_details [ 4 ][ \"index\" ], [ 1 ]) # allocate tensors lightspeech . allocate_tensors () input_data = prepare_input ( input_ids , speaker ) # set input tensors for i , detail in enumerate ( input_details ): lightspeech . set_tensor ( detail [ \"index\" ], input_data [ i ]) # invoke interpreter lightspeech . invoke () # return outputs return ( lightspeech . get_tensor ( output_details [ 0 ][ \"index\" ]), lightspeech . get_tensor ( output_details [ 1 ][ \"index\" ]), ) def melgan_infer ( melspectrogram : tf . Tensor , mb_melgan_path : str ) -> tf . Tensor : \"\"\"Performs MB-MelGAN inference. Args: melspectrogram (tf.Tensor): Mel-spectrogram to synthesize. mb_melgan_path (str): Path to MB-MelGAN weights. Returns: tf.Tensor: Synthesized audio output tensor. \"\"\" # load model to Interpreter mb_melgan = tf . lite . Interpreter ( model_path = mb_melgan_path ) input_details = mb_melgan . get_input_details () output_details = mb_melgan . get_output_details () # resize input tensors according to actual shape mb_melgan . resize_tensor_input ( input_details [ 0 ][ \"index\" ], [ 1 , melspectrogram . shape [ 1 ], melspectrogram . shape [ 2 ]], strict = True , ) # allocate tensors mb_melgan . allocate_tensors () # set input tensors mb_melgan . set_tensor ( input_details [ 0 ][ \"index\" ], melspectrogram ) # invoke interpreter mb_melgan . invoke () # return output return mb_melgan . get_tensor ( output_details [ 0 ][ \"index\" ]) Finally, we can perform inference with the model weights which we have converted earlier and run an end-to-end inference. mel_output_tflite , * _ = ls_infer ( input_ids , speaker = 0 , lightspeech_path = \"lightspeech_quant.tflite\" ) audio_tflite = melgan_infer ( mel_output_tflite , mb_melgan_path = \"mbmelgan.tflite\" )[ 0 , :, 0 ] To listen to the synthesized output via Jupyter, we can directly pass the outputs to IPython's Audio widget, while specifying the sample rate of the audio. from IPython.display import Audio Audio ( data = audio_tflite , rate = 32000 ) Your browser does not support the audio element. Alternatively, we can write the output audio tensors to file. import soundfile as sf sf . write ( \"./audio.wav\" , audio_tflite , 32000 , \"PCM_16\" ) from IPython.display import Audio Audio ( \"audio.wav\" ) Your browser does not support the audio element.","title":"Inference"},{"location":"guides/lightspeech-mbmelgan/dataset/","text":"Dataset Folder Structure The structure of the training files is fairly simple and straightforward: {YOUR_DATASET}/ \u251c\u2500\u2500 {SPEAKER-1}/ \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-0}.lab \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-0}.wav \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-1}.lab \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-1}.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 {SPEAKER-2}/ \u2502 \u251c\u2500\u2500 {SPEAKER-2}_{UTTERANCE-0}.lab \u2502 \u251c\u2500\u2500 {SPEAKER-2}_{UTTERANCE-0}.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 ... A few key things to note here: Each speaker has its own subfolder within the root dataset folder. The filenames in the speaker subfolders follow the convention of {SPEAKER-#}_{UTTERANCE-#} . It is important that they are delimited by an underscore ( _ ), so make sure that there is no _ within the speaker name and within the utterance ID. Use dashes - instead within them instead. Audios are in wav format and transcripts are of lab format (same content as you expect from a txt file; nothing fancy about it). The reason we use lab is simply to facilitate Montreal Forced Aligner training later. Example In the root directory en-bookbot , there are three speakers: en-AU-Zak , en-UK-Thalia , and en-US-Madison . The struture of the files are as follows: en-bookbot/ \u251c\u2500\u2500 en-AU-Zak/ \u2502 \u251c\u2500\u2500 en-AU-Zak_0.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_0.wav \u2502 \u251c\u2500\u2500 en-AU-Zak_1.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_1.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-UK-Thalia/ \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.lab \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.wav \u2502 \u251c\u2500\u2500 ... \u2514\u2500\u2500 en-US-Madison/ \u251c\u2500\u2500 en-US-Madison_0.lab \u251c\u2500\u2500 en-US-Madison_0.wav \u251c\u2500\u2500 ... Lexicon Another required component for training a Montreal Forced Aligner is a lexicon file (usually named lexicon.txt ). A lexicon simply maps words (graphemes) to phonemes, i.e. a pronunciation dictionary. Later, these phonemes will be aligned to segments of audio that they correpond to, and its duration will be learned by the forced aligner. There are many available lexicons out there, such as ones provided by Montreal Forced Aligner and Open Dict Data . You can either find other pre-existing lexicons, or create your own. Otherwise, another option would to treat each grapheme character as proxy phonemes as done by Meyer et al. (2022) where a lexicon is unavailable in certain languages: Two languages (ewe and yor) were aligned via forced alignment from scratch. Using only the found audio and transcripts (i.e., without a pre-trained acoustic model), an acoustic model was trained and the data aligned with the Montreal Forced Aligner. Graphemes were used as a proxy for phonemes in place of G2P data. In any case, the lexicon file should consist of tab -delimited word-phoneme pairs, which looks like the following: what w \u02c8\u028c t is \u02c8\u026a z biology b a\u026a \u02c8\u0251 l \u0259 d\u0361\u0292 i ? ? the \u00f0 \u0259 study s t \u02c8\u028c d i of \u0259 v living l \u02c8\u026a v \u026a \u014b things \u03b8 \u02c8\u026a \u014b z . . from f \u0279 \u02c8\u028c m ... Another example would be: : : , , . . ! ! ? ? ; ; a a b b e c t\u0283 e d d e abad a b a d abadi a b a d i abadiah a b a d i a h abadikan a b a d i k a n ... There are a few key things to note as well: The lexicon should cover all words in the audio corpus -- no out-of-vocabulary words should be present during the training of the aligner later. Otherwise, this will result in unknown tokens and will likely disrupt the training and duration parsing processes. Include all the punctuations you would want to have in the model later. For instance, I would usually keep . , : ; ? ! because they might imply different pause durations and/or intonations. Every other punctuations not in the lexicon will be stripped during the alignment process. Individual phonemes should be separated with whitespaces, e.g. a is its own phoneme unit, and t\u0283 is also considered as another single phoneme unit despite having 2 characters. Structuring your dataset, preprocessing, and lexicon preparation are arguably the most complicated part of training these kinds of models. But once we're over this step, everything else should be quite easy to follow.","title":"Dataset"},{"location":"guides/lightspeech-mbmelgan/dataset/#dataset","text":"","title":"Dataset"},{"location":"guides/lightspeech-mbmelgan/dataset/#folder-structure","text":"The structure of the training files is fairly simple and straightforward: {YOUR_DATASET}/ \u251c\u2500\u2500 {SPEAKER-1}/ \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-0}.lab \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-0}.wav \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-1}.lab \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-1}.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 {SPEAKER-2}/ \u2502 \u251c\u2500\u2500 {SPEAKER-2}_{UTTERANCE-0}.lab \u2502 \u251c\u2500\u2500 {SPEAKER-2}_{UTTERANCE-0}.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 ... A few key things to note here: Each speaker has its own subfolder within the root dataset folder. The filenames in the speaker subfolders follow the convention of {SPEAKER-#}_{UTTERANCE-#} . It is important that they are delimited by an underscore ( _ ), so make sure that there is no _ within the speaker name and within the utterance ID. Use dashes - instead within them instead. Audios are in wav format and transcripts are of lab format (same content as you expect from a txt file; nothing fancy about it). The reason we use lab is simply to facilitate Montreal Forced Aligner training later.","title":"Folder Structure"},{"location":"guides/lightspeech-mbmelgan/dataset/#example","text":"In the root directory en-bookbot , there are three speakers: en-AU-Zak , en-UK-Thalia , and en-US-Madison . The struture of the files are as follows: en-bookbot/ \u251c\u2500\u2500 en-AU-Zak/ \u2502 \u251c\u2500\u2500 en-AU-Zak_0.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_0.wav \u2502 \u251c\u2500\u2500 en-AU-Zak_1.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_1.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-UK-Thalia/ \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.lab \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.wav \u2502 \u251c\u2500\u2500 ... \u2514\u2500\u2500 en-US-Madison/ \u251c\u2500\u2500 en-US-Madison_0.lab \u251c\u2500\u2500 en-US-Madison_0.wav \u251c\u2500\u2500 ...","title":"Example"},{"location":"guides/lightspeech-mbmelgan/dataset/#lexicon","text":"Another required component for training a Montreal Forced Aligner is a lexicon file (usually named lexicon.txt ). A lexicon simply maps words (graphemes) to phonemes, i.e. a pronunciation dictionary. Later, these phonemes will be aligned to segments of audio that they correpond to, and its duration will be learned by the forced aligner. There are many available lexicons out there, such as ones provided by Montreal Forced Aligner and Open Dict Data . You can either find other pre-existing lexicons, or create your own. Otherwise, another option would to treat each grapheme character as proxy phonemes as done by Meyer et al. (2022) where a lexicon is unavailable in certain languages: Two languages (ewe and yor) were aligned via forced alignment from scratch. Using only the found audio and transcripts (i.e., without a pre-trained acoustic model), an acoustic model was trained and the data aligned with the Montreal Forced Aligner. Graphemes were used as a proxy for phonemes in place of G2P data. In any case, the lexicon file should consist of tab -delimited word-phoneme pairs, which looks like the following: what w \u02c8\u028c t is \u02c8\u026a z biology b a\u026a \u02c8\u0251 l \u0259 d\u0361\u0292 i ? ? the \u00f0 \u0259 study s t \u02c8\u028c d i of \u0259 v living l \u02c8\u026a v \u026a \u014b things \u03b8 \u02c8\u026a \u014b z . . from f \u0279 \u02c8\u028c m ... Another example would be: : : , , . . ! ! ? ? ; ; a a b b e c t\u0283 e d d e abad a b a d abadi a b a d i abadiah a b a d i a h abadikan a b a d i k a n ... There are a few key things to note as well: The lexicon should cover all words in the audio corpus -- no out-of-vocabulary words should be present during the training of the aligner later. Otherwise, this will result in unknown tokens and will likely disrupt the training and duration parsing processes. Include all the punctuations you would want to have in the model later. For instance, I would usually keep . , : ; ? ! because they might imply different pause durations and/or intonations. Every other punctuations not in the lexicon will be stripped during the alignment process. Individual phonemes should be separated with whitespaces, e.g. a is its own phoneme unit, and t\u0283 is also considered as another single phoneme unit despite having 2 characters. Structuring your dataset, preprocessing, and lexicon preparation are arguably the most complicated part of training these kinds of models. But once we're over this step, everything else should be quite easy to follow.","title":"Lexicon"},{"location":"guides/lightspeech-mbmelgan/duration-extraction/","text":"Duration Extraction Forced Alignment Recall that we had prepared a lexicon file that maps graphemes (words) into phonemes. These phonemes will then be aligned to segments of the corresponding audio, whose duration we will then use to feed into models like LightSpeech. The end result of audio alignment would look something like the following: Notice that chunks of audio are aligned to each word and its subsequent phonemes. By the end of the training, TextGrid files will be generated, containing the alignment results. They look like the following: File type = \"ooTextFile\" Object class = \"TextGrid\" xmin = 0 xmax = 7.398163265306122 tiers? <exists> size = 2 item []: item [1]: class = \"IntervalTier\" name = \"words\" xmin = 0 xmax = 7.398163265306122 intervals: size = 34 intervals [1]: xmin = 0 xmax = 0.04 text = \"\" intervals [2]: xmin = 0.04 xmax = 0.26 text = \"he\" intervals [3]: xmin = 0.26 xmax = 0.44 text = \"is\" intervals [4]: xmin = 0.44 xmax = 0.48 text = \"\" intervals [5]: xmin = 0.48 xmax = 0.91 text = \"going\" intervals [6]: xmin = 0.91 xmax = 0.94 text = \"\" intervals [7]: xmin = 0.94 xmax = 1.05 text = \"to\" Montreal Forced Aligner (MFA) is an algorithm and a library that will help train these kinds of acoustic models. The TextGrid files will then be parsed and the durations of phonemes will be obtained. It is these phoneme durations that will then be learned by LightSpeech via L2 loss. External Aligner-free Models Recent development is aiming to completely remove the usage of external-aligner tools like MFA, and for good reasons. It will not only simplify the training pipeline by learning the token duration on-the-fly, but will also improve the speech quality and speed up alignment convergence, especially on longer sequence of text during inference. Most of the latest models that does its own alignment learning normally use a combination of Connectionist Temporal Classification and Monotonic Alignment Search -- for instance Glow-TTS , VITS , and JETS , among many others. As far as I know, these types of models have yet to be integrated into TensorFlowTTS, although there is an existing branch that attempts to implement AlignTTS . Installing Montreal Forced Aligner To begin, we can start by first installing Montreal Forced Aligner. It is much easier to install it via Conda Forge. In the same Conda environment, you can run these commands conda config --add channels conda-forge conda install montreal-forced-aligner If you're not using Conda (e.g. upgrading from non-Conda version, or installing from Source), you can follow the official guide here . To confirm that the installation is successful, you can run mfa version which will return the version you've installed (in my case, it's 2.0.5 ). Training an MFA Aligner With MFA installed, training an aligner model is as simple as running the command mfa train { YOUR_DATASET } { LEXICON } { OUTPUT_ACOUSTIC_MODEL } { TEXTGRID_OUTPUT_DIR } --punctuation \"\" Example Using the same sample dataset and a lexicon file lexicon.txt , the command to run will be similar to the following mfa train ./en-bookbot ./lexicon.txt ./outputs/en_bookbot_acoustic_model.zip ./outputs/parsed --punctuation \"\" Parsing TextGrid Files We can ignore the acoustic model generated by MFA, although you can always keep this for other purposes. What's more important for us is the resultant TextGrid files located in your {TEXTGRID_OUTPUT_DIR} . Then, you could either use the original parser script or what I usually use is a modified version of the script. TxtGridParser import os from dataclasses import dataclass from tqdm.auto import tqdm import textgrid import numpy as np import re @dataclass class TxtGridParser : sample_rate : int multi_speaker : bool txt_grid_path : str hop_size : int output_durations_path : str dataset_path : str training_file : str = \"train.txt\" phones_mapper = { \"sil\" : \"SIL\" , \"\" : \"SIL\" } sil_phones = set ( phones_mapper . keys ()) punctuations = [ \";\" , \"?\" , \"!\" , \".\" , \",\" , \":\" ] def parse ( self ): speakers = ( [ i for i in os . listdir ( self . txt_grid_path ) if os . path . isdir ( os . path . join ( self . txt_grid_path , i )) ] if self . multi_speaker else [] ) data = [] if speakers : for speaker in speakers : file_list = os . listdir ( os . path . join ( self . txt_grid_path , speaker )) self . parse_text_grid ( file_list , data , speaker ) else : file_list = os . listdir ( self . txt_grid_path ) self . parse_text_grid ( file_list , data , \"\" ) with open ( os . path . join ( self . dataset_path , self . training_file ), \"w\" , encoding = \"utf-8\" ) as f : f . writelines ( data ) def parse_text_grid ( self , file_list : list , data : list , speaker_name : str ): for f_name in tqdm ( file_list ): text_grid = textgrid . TextGrid . fromFile ( os . path . join ( self . txt_grid_path , speaker_name , f_name ) ) pha = text_grid [ 1 ] durations = [] phs = [] flags = [] for iterator , interval in enumerate ( pha . intervals ): mark = interval . mark if mark in self . sil_phones or mark in self . punctuations : flags . append ( True ) else : flags . append ( False ) if mark in self . sil_phones : mark = self . phones_mapper [ mark ] dur = interval . duration () * ( self . sample_rate / self . hop_size ) durations . append ( round ( dur )) phs . append ( mark ) new_durations = [] for idx , ( flag , dur ) in enumerate ( zip ( flags , durations )): if len ( new_durations ) == 0 or ( flag and flags [ idx - 1 ] == False ): new_durations . append ( dur ) elif flag : new_durations [ len ( new_durations ) - 1 ] += dur else : new_durations . append ( dur ) full_ph = \" \" . join ( phs ) new_ph = full_ph matches = re . finditer ( \"( ?SIL)* ?([,!\\?\\.;:] ?){1,} ?(SIL ?)*\" , full_ph ) for match in matches : substring = full_ph [ match . start () : match . end ()] new_ph = new_ph . replace ( substring , f \" { substring . replace ( 'SIL' , '' ) . strip ()[ 0 ] } \" , 1 ) . strip () assert new_ph . split () . __len__ () == new_durations . __len__ () # safety check base_name = f_name . split ( \".TextGrid\" )[ 0 ] np . save ( os . path . join ( self . output_durations_path , f \" { base_name } -durations.npy\" ), np . array ( new_durations ) . astype ( np . int32 ), allow_pickle = False , ) data . append ( f \" { speaker_name } / { base_name } | { new_ph } | { speaker_name } \\n \" ) Then to use the modified script above, you just have to specify the arguments to the parser. args = { \"dataset_path\" : \"./en-bookbot\" , \"txt_grid_path\" : \"./outputs/parsed\" , # (1) \"output_durations_path\" : \"./en-bookbot/durations\" , \"sample_rate\" : 44100 , # (2) \"hop_size\" : 512 , # (3) \"multi_speaker\" : True , # (4) \"training_file\" : \"train.txt\" } txt_grid_parser = TxtGridParser ( ** args ) txt_grid_parser . parse () Replace this with whatever your {TEXTGRID_OUTPUT_DIR} was. Set this to the desired sample rate of your text-to-speech model. Set this to the desired hop size of your text-to-speech model. Multi-speaker or not, you can keep this as True . With the duration files located in durations/ and train.txt , we can finally train our own text-to-speech model!","title":"Duration Extraction"},{"location":"guides/lightspeech-mbmelgan/duration-extraction/#duration-extraction","text":"","title":"Duration Extraction"},{"location":"guides/lightspeech-mbmelgan/duration-extraction/#forced-alignment","text":"Recall that we had prepared a lexicon file that maps graphemes (words) into phonemes. These phonemes will then be aligned to segments of the corresponding audio, whose duration we will then use to feed into models like LightSpeech. The end result of audio alignment would look something like the following: Notice that chunks of audio are aligned to each word and its subsequent phonemes. By the end of the training, TextGrid files will be generated, containing the alignment results. They look like the following: File type = \"ooTextFile\" Object class = \"TextGrid\" xmin = 0 xmax = 7.398163265306122 tiers? <exists> size = 2 item []: item [1]: class = \"IntervalTier\" name = \"words\" xmin = 0 xmax = 7.398163265306122 intervals: size = 34 intervals [1]: xmin = 0 xmax = 0.04 text = \"\" intervals [2]: xmin = 0.04 xmax = 0.26 text = \"he\" intervals [3]: xmin = 0.26 xmax = 0.44 text = \"is\" intervals [4]: xmin = 0.44 xmax = 0.48 text = \"\" intervals [5]: xmin = 0.48 xmax = 0.91 text = \"going\" intervals [6]: xmin = 0.91 xmax = 0.94 text = \"\" intervals [7]: xmin = 0.94 xmax = 1.05 text = \"to\" Montreal Forced Aligner (MFA) is an algorithm and a library that will help train these kinds of acoustic models. The TextGrid files will then be parsed and the durations of phonemes will be obtained. It is these phoneme durations that will then be learned by LightSpeech via L2 loss. External Aligner-free Models Recent development is aiming to completely remove the usage of external-aligner tools like MFA, and for good reasons. It will not only simplify the training pipeline by learning the token duration on-the-fly, but will also improve the speech quality and speed up alignment convergence, especially on longer sequence of text during inference. Most of the latest models that does its own alignment learning normally use a combination of Connectionist Temporal Classification and Monotonic Alignment Search -- for instance Glow-TTS , VITS , and JETS , among many others. As far as I know, these types of models have yet to be integrated into TensorFlowTTS, although there is an existing branch that attempts to implement AlignTTS .","title":"Forced Alignment"},{"location":"guides/lightspeech-mbmelgan/duration-extraction/#installing-montreal-forced-aligner","text":"To begin, we can start by first installing Montreal Forced Aligner. It is much easier to install it via Conda Forge. In the same Conda environment, you can run these commands conda config --add channels conda-forge conda install montreal-forced-aligner If you're not using Conda (e.g. upgrading from non-Conda version, or installing from Source), you can follow the official guide here . To confirm that the installation is successful, you can run mfa version which will return the version you've installed (in my case, it's 2.0.5 ).","title":"Installing Montreal Forced Aligner"},{"location":"guides/lightspeech-mbmelgan/duration-extraction/#training-an-mfa-aligner","text":"With MFA installed, training an aligner model is as simple as running the command mfa train { YOUR_DATASET } { LEXICON } { OUTPUT_ACOUSTIC_MODEL } { TEXTGRID_OUTPUT_DIR } --punctuation \"\"","title":"Training an MFA Aligner"},{"location":"guides/lightspeech-mbmelgan/duration-extraction/#example","text":"Using the same sample dataset and a lexicon file lexicon.txt , the command to run will be similar to the following mfa train ./en-bookbot ./lexicon.txt ./outputs/en_bookbot_acoustic_model.zip ./outputs/parsed --punctuation \"\"","title":"Example"},{"location":"guides/lightspeech-mbmelgan/duration-extraction/#parsing-textgrid-files","text":"We can ignore the acoustic model generated by MFA, although you can always keep this for other purposes. What's more important for us is the resultant TextGrid files located in your {TEXTGRID_OUTPUT_DIR} . Then, you could either use the original parser script or what I usually use is a modified version of the script. TxtGridParser import os from dataclasses import dataclass from tqdm.auto import tqdm import textgrid import numpy as np import re @dataclass class TxtGridParser : sample_rate : int multi_speaker : bool txt_grid_path : str hop_size : int output_durations_path : str dataset_path : str training_file : str = \"train.txt\" phones_mapper = { \"sil\" : \"SIL\" , \"\" : \"SIL\" } sil_phones = set ( phones_mapper . keys ()) punctuations = [ \";\" , \"?\" , \"!\" , \".\" , \",\" , \":\" ] def parse ( self ): speakers = ( [ i for i in os . listdir ( self . txt_grid_path ) if os . path . isdir ( os . path . join ( self . txt_grid_path , i )) ] if self . multi_speaker else [] ) data = [] if speakers : for speaker in speakers : file_list = os . listdir ( os . path . join ( self . txt_grid_path , speaker )) self . parse_text_grid ( file_list , data , speaker ) else : file_list = os . listdir ( self . txt_grid_path ) self . parse_text_grid ( file_list , data , \"\" ) with open ( os . path . join ( self . dataset_path , self . training_file ), \"w\" , encoding = \"utf-8\" ) as f : f . writelines ( data ) def parse_text_grid ( self , file_list : list , data : list , speaker_name : str ): for f_name in tqdm ( file_list ): text_grid = textgrid . TextGrid . fromFile ( os . path . join ( self . txt_grid_path , speaker_name , f_name ) ) pha = text_grid [ 1 ] durations = [] phs = [] flags = [] for iterator , interval in enumerate ( pha . intervals ): mark = interval . mark if mark in self . sil_phones or mark in self . punctuations : flags . append ( True ) else : flags . append ( False ) if mark in self . sil_phones : mark = self . phones_mapper [ mark ] dur = interval . duration () * ( self . sample_rate / self . hop_size ) durations . append ( round ( dur )) phs . append ( mark ) new_durations = [] for idx , ( flag , dur ) in enumerate ( zip ( flags , durations )): if len ( new_durations ) == 0 or ( flag and flags [ idx - 1 ] == False ): new_durations . append ( dur ) elif flag : new_durations [ len ( new_durations ) - 1 ] += dur else : new_durations . append ( dur ) full_ph = \" \" . join ( phs ) new_ph = full_ph matches = re . finditer ( \"( ?SIL)* ?([,!\\?\\.;:] ?){1,} ?(SIL ?)*\" , full_ph ) for match in matches : substring = full_ph [ match . start () : match . end ()] new_ph = new_ph . replace ( substring , f \" { substring . replace ( 'SIL' , '' ) . strip ()[ 0 ] } \" , 1 ) . strip () assert new_ph . split () . __len__ () == new_durations . __len__ () # safety check base_name = f_name . split ( \".TextGrid\" )[ 0 ] np . save ( os . path . join ( self . output_durations_path , f \" { base_name } -durations.npy\" ), np . array ( new_durations ) . astype ( np . int32 ), allow_pickle = False , ) data . append ( f \" { speaker_name } / { base_name } | { new_ph } | { speaker_name } \\n \" ) Then to use the modified script above, you just have to specify the arguments to the parser. args = { \"dataset_path\" : \"./en-bookbot\" , \"txt_grid_path\" : \"./outputs/parsed\" , # (1) \"output_durations_path\" : \"./en-bookbot/durations\" , \"sample_rate\" : 44100 , # (2) \"hop_size\" : 512 , # (3) \"multi_speaker\" : True , # (4) \"training_file\" : \"train.txt\" } txt_grid_parser = TxtGridParser ( ** args ) txt_grid_parser . parse () Replace this with whatever your {TEXTGRID_OUTPUT_DIR} was. Set this to the desired sample rate of your text-to-speech model. Set this to the desired hop size of your text-to-speech model. Multi-speaker or not, you can keep this as True . With the duration files located in durations/ and train.txt , we can finally train our own text-to-speech model!","title":"Parsing TextGrid Files"},{"location":"guides/lightspeech-mbmelgan/inference/","text":"Inference Everything here can be followed along in Google Colab! Load Models and Processor Inferencing TensorFlowTTS models are quite straightforward given that you know the expected inputs and outputs of each model. But first, we have to load pre-trained model weights. If you've followed the additional step of pushing model weights to HuggingFace Hub , you can simply load weights stored there! This also includes the processor that comes hand-in-hand with the text2mel model. To be able to load private models, you must first log into HuggingFace Hub. from huggingface_hub import notebook_login notebook_login () Token is valid. Your token has been saved in your configured git credential helpers (store). Your token has been saved to /root/.huggingface/token Login successful You can then load private models by specifying use_auth_token=True . from tensorflow_tts.inference import TFAutoModel , AutoProcessor import tensorflow as tf vocoder = TFAutoModel . from_pretrained ( \"bookbot/mb-melgan-hifi-postnets-en\" , use_auth_token = True ) text2mel = TFAutoModel . from_pretrained ( \"bookbot/lightspeech-mfa-en\" , use_auth_token = True ) processor = AutoProcessor . from_pretrained ( \"bookbot/lightspeech-mfa-en\" , use_auth_token = True ) processor . mode = \"eval\" # change processor from train to eval mode Tokenization Then, we'll need to tokenize raw text into their corresponding input IDs, which we can achieve by simply calling the text_to_sequence method of our processor. text = \"Hello world.\" input_ids = processor . text_to_sequence ( text ) LightSpeech Inference To perform inference of LightSpeech models, you will need to specify 5 different inputs: Input IDs Speaker ID Speed Ratio Pitch Ratio Energy Ratio We already have our input IDs -- we just need to specify which speaker index we'd want to use and optionally, specify other ratios. For now, we'll only be parameterizing the speaker ID and set the rest to 1.0 (default value). Keep in mind that TensorFlow models expect inputs to be batched. This is why we need to use the tf.expand_dims function on our input IDs (to make them of batch size 1) and the other inputs are also lists instead of raw scalar values. Moreover, LightSpeech (at least our implementation) returns 3 things: Mel-Spectrogram Duration Predictions Pitch (F0) Prediction We'll only be keeping the first (index 0) and ignore the rest. Note This is where understanding what our model's outputs are becomes important. We need to know at which index our desired output is. For instance, FastSpeech returns another mel-spectrogram (often called mel_after , \"after\" meaning, after the initial mel-spectrogram prediction is additionally passed through a Tacotron PostNet module), while LightSpeech only has one mel-spectrogram output, located at index 0. speaker_id = 0 mel_spectrogram , * _ = text2mel . inference ( input_ids = tf . expand_dims ( tf . convert_to_tensor ( input_ids , dtype = tf . int32 ), 0 ), speaker_ids = tf . convert_to_tensor ([ speaker_id ], dtype = tf . int32 ), speed_ratios = tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), f0_ratios = tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), energy_ratios = tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), ) Multi-Band MelGAN Inference After that, we can take the output mel-spectrogram generated by LightSpeech and use that as input to our MB-MelGAN model. audio = vocoder . inference ( mel_spectrogram )[ 0 , :, 0 ] Save Synthesized Audio as File Finally, we can save the predicted audio waveforms as a file by utilizing SoundFile. We'll just need to specify several parameters such as the output save name, the audio tensor, sample rate, and subtype. import soundfile as sf sf . write ( \"./audio.wav\" , audio , 44100 , \"PCM_16\" ) And with that, we have just synthesized an audio from pure text! If you're following from a Jupyter Notebook, you can play the saved audio via IPython's Audio widget. from IPython.display import Audio Audio ( \"audio.wav\" ) Your browser does not support the audio element.","title":"Inference"},{"location":"guides/lightspeech-mbmelgan/inference/#inference","text":"Everything here can be followed along in Google Colab!","title":"Inference"},{"location":"guides/lightspeech-mbmelgan/inference/#load-models-and-processor","text":"Inferencing TensorFlowTTS models are quite straightforward given that you know the expected inputs and outputs of each model. But first, we have to load pre-trained model weights. If you've followed the additional step of pushing model weights to HuggingFace Hub , you can simply load weights stored there! This also includes the processor that comes hand-in-hand with the text2mel model. To be able to load private models, you must first log into HuggingFace Hub. from huggingface_hub import notebook_login notebook_login () Token is valid. Your token has been saved in your configured git credential helpers (store). Your token has been saved to /root/.huggingface/token Login successful You can then load private models by specifying use_auth_token=True . from tensorflow_tts.inference import TFAutoModel , AutoProcessor import tensorflow as tf vocoder = TFAutoModel . from_pretrained ( \"bookbot/mb-melgan-hifi-postnets-en\" , use_auth_token = True ) text2mel = TFAutoModel . from_pretrained ( \"bookbot/lightspeech-mfa-en\" , use_auth_token = True ) processor = AutoProcessor . from_pretrained ( \"bookbot/lightspeech-mfa-en\" , use_auth_token = True ) processor . mode = \"eval\" # change processor from train to eval mode","title":"Load Models and Processor"},{"location":"guides/lightspeech-mbmelgan/inference/#tokenization","text":"Then, we'll need to tokenize raw text into their corresponding input IDs, which we can achieve by simply calling the text_to_sequence method of our processor. text = \"Hello world.\" input_ids = processor . text_to_sequence ( text )","title":"Tokenization"},{"location":"guides/lightspeech-mbmelgan/inference/#lightspeech-inference","text":"To perform inference of LightSpeech models, you will need to specify 5 different inputs: Input IDs Speaker ID Speed Ratio Pitch Ratio Energy Ratio We already have our input IDs -- we just need to specify which speaker index we'd want to use and optionally, specify other ratios. For now, we'll only be parameterizing the speaker ID and set the rest to 1.0 (default value). Keep in mind that TensorFlow models expect inputs to be batched. This is why we need to use the tf.expand_dims function on our input IDs (to make them of batch size 1) and the other inputs are also lists instead of raw scalar values. Moreover, LightSpeech (at least our implementation) returns 3 things: Mel-Spectrogram Duration Predictions Pitch (F0) Prediction We'll only be keeping the first (index 0) and ignore the rest. Note This is where understanding what our model's outputs are becomes important. We need to know at which index our desired output is. For instance, FastSpeech returns another mel-spectrogram (often called mel_after , \"after\" meaning, after the initial mel-spectrogram prediction is additionally passed through a Tacotron PostNet module), while LightSpeech only has one mel-spectrogram output, located at index 0. speaker_id = 0 mel_spectrogram , * _ = text2mel . inference ( input_ids = tf . expand_dims ( tf . convert_to_tensor ( input_ids , dtype = tf . int32 ), 0 ), speaker_ids = tf . convert_to_tensor ([ speaker_id ], dtype = tf . int32 ), speed_ratios = tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), f0_ratios = tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), energy_ratios = tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), )","title":"LightSpeech Inference"},{"location":"guides/lightspeech-mbmelgan/inference/#multi-band-melgan-inference","text":"After that, we can take the output mel-spectrogram generated by LightSpeech and use that as input to our MB-MelGAN model. audio = vocoder . inference ( mel_spectrogram )[ 0 , :, 0 ]","title":"Multi-Band MelGAN Inference"},{"location":"guides/lightspeech-mbmelgan/inference/#save-synthesized-audio-as-file","text":"Finally, we can save the predicted audio waveforms as a file by utilizing SoundFile. We'll just need to specify several parameters such as the output save name, the audio tensor, sample rate, and subtype. import soundfile as sf sf . write ( \"./audio.wav\" , audio , 44100 , \"PCM_16\" ) And with that, we have just synthesized an audio from pure text! If you're following from a Jupyter Notebook, you can play the saved audio via IPython's Audio widget. from IPython.display import Audio Audio ( \"audio.wav\" ) Your browser does not support the audio element.","title":"Save Synthesized Audio as File"},{"location":"guides/lightspeech-mbmelgan/intro/","text":"Introduction This guide will explain how to train a LightSpeech acoustic model and a Multi-band MelGAN vocoder model (with a HiFi-GAN Discriminator). In particular, we will be training a 44.1 kHz model, with a hop size of 512. This guide expects you to train an IPA-based model. For now, this tutorial only supports English and Indonesian -- because only these two languages have its correponding IPA-based grapheme-to-phoneme processor added to the custom fork. For English, we use gruut and for Indonesian, we use g2p_id . To add support for other languages, you would need a grapheme-to-phoneme converter for that language, and support it as a processor in TensorFlowTTS. We will introduce a separate tutorial for that in the future. LightSpeech LightSpeech follows the same architecture as FastSpeech2 , except with an optimized model configuration obtained via Neural Architecture Search (NAS). In our case, we don't really perform NAS, but use the previously found best model configuration. Multi-band MelGAN Multi-Band MelGAN is an improvement upon MelGAN that does waveform generation and waveform discrimination on a multi-band basis. HiFi-GAN Discrminator Further, instead of using the original discriminator, we can use the discriminator presented in the HiFi-GAN paper. We specifically use the multi-period discriminator (MPD) on the right.","title":"Introduction"},{"location":"guides/lightspeech-mbmelgan/intro/#introduction","text":"This guide will explain how to train a LightSpeech acoustic model and a Multi-band MelGAN vocoder model (with a HiFi-GAN Discriminator). In particular, we will be training a 44.1 kHz model, with a hop size of 512. This guide expects you to train an IPA-based model. For now, this tutorial only supports English and Indonesian -- because only these two languages have its correponding IPA-based grapheme-to-phoneme processor added to the custom fork. For English, we use gruut and for Indonesian, we use g2p_id . To add support for other languages, you would need a grapheme-to-phoneme converter for that language, and support it as a processor in TensorFlowTTS. We will introduce a separate tutorial for that in the future.","title":"Introduction"},{"location":"guides/lightspeech-mbmelgan/intro/#lightspeech","text":"LightSpeech follows the same architecture as FastSpeech2 , except with an optimized model configuration obtained via Neural Architecture Search (NAS). In our case, we don't really perform NAS, but use the previously found best model configuration.","title":"LightSpeech"},{"location":"guides/lightspeech-mbmelgan/intro/#multi-band-melgan","text":"Multi-Band MelGAN is an improvement upon MelGAN that does waveform generation and waveform discrimination on a multi-band basis.","title":"Multi-band MelGAN"},{"location":"guides/lightspeech-mbmelgan/intro/#hifi-gan-discrminator","text":"Further, instead of using the original discriminator, we can use the discriminator presented in the HiFi-GAN paper. We specifically use the multi-period discriminator (MPD) on the right.","title":"HiFi-GAN Discrminator"},{"location":"guides/lightspeech-mbmelgan/training/","text":"Training Folder Structure Let's revisit the structure of our dataset now that we have new components. We need the audio files, duration files, and metadata file to be located in the same folder. Continuing with the same sample dataset , we should end up with these files by the end of the duration extraction step: en-bookbot/ \u251c\u2500\u2500 durations/ \u2502 \u251c\u2500\u2500 en-AU-Zak_0-durations.npy \u2502 \u251c\u2500\u2500 en-AU-Zal_1-durations.npy \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 en-UK-Thalia_0-durations.npy \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 en-US-Madison_0-durations.npy \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-AU-Zak/ \u2502 \u251c\u2500\u2500 en-AU-Zak_0.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_0.wav \u2502 \u251c\u2500\u2500 en-AU-Zak_1.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_1.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-UK-Thalia/ \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.lab \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-US-Madison/ \u2502 \u251c\u2500\u2500 en-US-Madison_0.lab \u2502 \u251c\u2500\u2500 en-US-Madison_0.wav \u2502 \u251c\u2500\u2500 ... \u2514\u2500\u2500 train.txt Training LightSpeech We start by first preprocessing and normalizing the audio files. These commands will handle tokenization, feature extraction, etc. tensorflow-tts-preprocess --rootdir ./en-bookbot --outdir ./dump --config TensorFlowTTS/preprocess/englishipa_preprocess.yaml --dataset englishipa --verbose 2 tensorflow-tts-normalize --rootdir ./dump --outdir ./dump --config TensorFlowTTS/preprocess/englishipa_preprocess.yaml --dataset englishipa --verbose 2 It's also recommended to fix mis-matching duration files python TensorFlowTTS/examples/mfa_extraction/fix_mismatch.py \\ --base_path ./dump \\ --trimmed_dur_path ./en-bookbot/trimmed-durations \\ --dur_path ./en-bookbot/durations \\ --use_norm t We can then train the LightSpeech model CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/lightspeech/train_lightspeech.py \\ --train-dir ./dump/train/ \\ --dev-dir ./dump/valid/ \\ --outdir ./lightspeech-en-bookbot \\ # (1) --config ./TensorFlowTTS/examples/lightspeech/conf/lightspeech_englishipa.yaml \\ # (2) --use-norm 1 \\ --f0-stat ./dump/stats_f0.npy \\ --energy-stat ./dump/stats_energy.npy \\ --mixed_precision 1 \\ --dataset_config TensorFlowTTS/preprocess/englishipa_preprocess.yaml \\ --dataset_stats dump/stats.npy \\ --dataset_mapping dump/englishipa_mapper.json You can set this to whatever output folder you'd like. This is a pre-configured training configuration. Feel free to customize it, but be careful with setting the sample rate and hop size. Once it's finished, you should end up with the following files: lightspeech-en-bookbot/ \u251c\u2500\u2500 checkpoints/ # (1) \u2502 \u251c\u2500\u2500 ckpt-10000.data-00000-of-00001 \u2502 \u251c\u2500\u2500 ckpt-10000.index \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 model-10000.h5 \u2502 \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 model-200000.h5 # (2) \u251c\u2500\u2500 config.yml \u251c\u2500\u2500 events.out.tfevents.1669084428.bookbot-tf-2.10561.0.v2 \u2514\u2500\u2500 predictions/ # (3) \u251c\u2500\u2500 100000steps/ \u2502 \u251c\u2500\u2500 b 'en-US-Madison_11' .png \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 10000steps/ \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 ... This contains all of the training checkpoints. The final model checkpoint (which we want). This contains all mid-training intermediate predictions (mel-spectrograms). It is missing the processor file, and the final model training checkpoint is still in the checkpoints/ subfolder. For the former, we can simply copy the file from dump to the output training folder. And for the latter, we can just copy the file up a directory. cd lightspeech-en-bookbot cp ../dump/englishipa_mapper.json processor.json cp checkpoints/model-200000.h5 model.h5 Training Multi-band MelGAN We can then continue with the training of our Multi-band MelGAN as our Vocoder model. First of all, you have the option to either: 1. Train to generate speech from original mel-spectrogram, or 2. Train to generate speech from LightSpeech-predicted mel-spectrogram. This is also known as training on PostNets. Selecting option 1 would likely give you a more \"universal\" vocoder, one that would likely retain its performance on unseen mel-spectrograms. However, I often find its performance on small-sized datasets quite poor, and hence why I'd usually opt for the second option instead. Training on PostNets would allow the model to also learn the flaws of the LightSpeech-predicted mel-spectrograms and still aim to generate the best audio quality. To do so, we begin by extracting the PostNets of our LightSpeech models. This means running inference on all of our texts and saving the predicted mel-spectrograms. We can do so using this modified LightSpeech PostNet Extraction Script . With that, we can simply run CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/lightspeech/extractls_postnets.py \\ --rootdir ./dump/train \\ --outdir ./dump/train \\ --config ./TensorFlowTTS/examples/lightspeech/conf/lightspeech_englishipa.yaml \\ --checkpoint ./lightspeech-en-bookbot/model.h5 \\ --dataset_mapping ./lightspeech-en-bookbot/processor.json CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/lightspeech/extractls_postnets.py \\ --rootdir ./dump/valid \\ --outdir ./dump/valid \\ --config ./TensorFlowTTS/examples/lightspeech/conf/lightspeech_englishipa.yaml \\ --checkpoint ./lightspeech-en-bookbot/model.h5 \\ --dataset_mapping ./lightspeech-en-bookbot/processor.json That will perform inference on the training and validation subsets. Finally, we can train the Multi-band MelGAN with the HiFi-GAN Discriminator by doing the following CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/multiband_melgan_hf/train_multiband_melgan_hf.py \\ --train-dir ./dump/train/ \\ --dev-dir ./dump/valid/ \\ --outdir ./mb-melgan-hifi-en-bookbot/ \\ --config ./TensorFlowTTS/examples/multiband_melgan_hf/conf/multiband_melgan_hf.en.v1.yml \\ --use-norm 1 \\ --generator_mixed_precision 1 \\ --postnets 1 \\ --resume \"\" CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/multiband_melgan_hf/train_multiband_melgan_hf.py \\ --train-dir ./dump/train/ \\ --dev-dir ./dump/valid/ \\ --outdir ./mb-melgan-hifi-en-bookbot/ \\ --config ./TensorFlowTTS/examples/multiband_melgan_hf/conf/multiband_melgan_hf.en.v1.yml \\ --use-norm 1 \\ --postnets 1 \\ --resume ./mb-melgan-hifi-en-bookbot/checkpoints/ckpt-200000 Note that this first pre-trains only the generator for 200,000 steps, and then continues the remaining steps with the usual GAN training framework. At the end of training, we should end up with the following files mb-melgan-hifi-en-bookbot/ \u251c\u2500\u2500 checkpoints/ # (1) \u2502 \u251c\u2500\u2500 checkpoint \u2502 \u251c\u2500\u2500 ckpt-100000.data-00000-of-00001 \u2502 \u251c\u2500\u2500 ckpt-100000.index \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 discriminator-100000.h5 \u2502 \u251c\u2500\u2500 discriminator-120000.h5 \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 generator-100000.h5 \u2502 \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 generator-1000000.h5 # (2) \u251c\u2500\u2500 config.yml \u251c\u2500\u2500 events.out.tfevents.1669101534.bookbot-tf-2.21897.0.v2 \u251c\u2500\u2500 events.out.tfevents.1669116847.bookbot-tf-2.31894.0.v2 \u2514\u2500\u2500 predictions/ # (3) \u251c\u2500\u2500 100000steps/ \u2502 \u251c\u2500\u2500 b 'en-US-Madison_11' .png \u2502 \u251c\u2500\u2500 b 'en-US-Madison_11' _gen.wav \u2502 \u251c\u2500\u2500 b 'en-US-Madison_11' _ref.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 10000steps/ \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 ... This contains all of the training checkpoints. The final generator checkpoint (which we want). This contains all mid-training intermediate predictions (waveforms). We can simply copy the final generator model into the output model directory. cd mb-melgan-hifi-en-bookbot cp checkpoints/generator-1000000.h5 model.h5 With that, we are done! Extra: Pushing Models to HuggingFace Hub To wrap things up, it will be very convenient to push our models to HuggingFace Hub . This would allow us to not only load pre-trained model weights stored on the Hub, but also has other goodies such as version control, documentation, etc., that comes with using Git. First, create a new repository on the Hub here . Second, install Git LFS in the machine you're working on sudo apt-get install git-lfs git lfs install The rest is pretty much the same as how you would push files to a Git repository. For example git clone https://huggingface.co/bookbot/lightspeech-mfa-en cp -r lightspeech-en-bookbot/* lightspeech-mfa-en cd lightspeech-mfa-en git add . && git commit -m \"added weights\" && git push git clone https://huggingface.co/bookbot/mb-melgan-hifi-postnets-en cp -r mb-melgan-hifi-en-bookbot/* mb-melgan-hifi-postnets-en cd mb-melgan-hifi-postnets-en git add . && git commit -m \"added weights\" && git push This would allow us to load models like so (in Python): model = TFAutoModel . from_pretrained ( \"bookbot/lightspeech-mfa-en\" )","title":"Training"},{"location":"guides/lightspeech-mbmelgan/training/#training","text":"","title":"Training"},{"location":"guides/lightspeech-mbmelgan/training/#folder-structure","text":"Let's revisit the structure of our dataset now that we have new components. We need the audio files, duration files, and metadata file to be located in the same folder. Continuing with the same sample dataset , we should end up with these files by the end of the duration extraction step: en-bookbot/ \u251c\u2500\u2500 durations/ \u2502 \u251c\u2500\u2500 en-AU-Zak_0-durations.npy \u2502 \u251c\u2500\u2500 en-AU-Zal_1-durations.npy \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 en-UK-Thalia_0-durations.npy \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 en-US-Madison_0-durations.npy \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-AU-Zak/ \u2502 \u251c\u2500\u2500 en-AU-Zak_0.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_0.wav \u2502 \u251c\u2500\u2500 en-AU-Zak_1.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_1.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-UK-Thalia/ \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.lab \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-US-Madison/ \u2502 \u251c\u2500\u2500 en-US-Madison_0.lab \u2502 \u251c\u2500\u2500 en-US-Madison_0.wav \u2502 \u251c\u2500\u2500 ... \u2514\u2500\u2500 train.txt","title":"Folder Structure"},{"location":"guides/lightspeech-mbmelgan/training/#training-lightspeech","text":"We start by first preprocessing and normalizing the audio files. These commands will handle tokenization, feature extraction, etc. tensorflow-tts-preprocess --rootdir ./en-bookbot --outdir ./dump --config TensorFlowTTS/preprocess/englishipa_preprocess.yaml --dataset englishipa --verbose 2 tensorflow-tts-normalize --rootdir ./dump --outdir ./dump --config TensorFlowTTS/preprocess/englishipa_preprocess.yaml --dataset englishipa --verbose 2 It's also recommended to fix mis-matching duration files python TensorFlowTTS/examples/mfa_extraction/fix_mismatch.py \\ --base_path ./dump \\ --trimmed_dur_path ./en-bookbot/trimmed-durations \\ --dur_path ./en-bookbot/durations \\ --use_norm t We can then train the LightSpeech model CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/lightspeech/train_lightspeech.py \\ --train-dir ./dump/train/ \\ --dev-dir ./dump/valid/ \\ --outdir ./lightspeech-en-bookbot \\ # (1) --config ./TensorFlowTTS/examples/lightspeech/conf/lightspeech_englishipa.yaml \\ # (2) --use-norm 1 \\ --f0-stat ./dump/stats_f0.npy \\ --energy-stat ./dump/stats_energy.npy \\ --mixed_precision 1 \\ --dataset_config TensorFlowTTS/preprocess/englishipa_preprocess.yaml \\ --dataset_stats dump/stats.npy \\ --dataset_mapping dump/englishipa_mapper.json You can set this to whatever output folder you'd like. This is a pre-configured training configuration. Feel free to customize it, but be careful with setting the sample rate and hop size. Once it's finished, you should end up with the following files: lightspeech-en-bookbot/ \u251c\u2500\u2500 checkpoints/ # (1) \u2502 \u251c\u2500\u2500 ckpt-10000.data-00000-of-00001 \u2502 \u251c\u2500\u2500 ckpt-10000.index \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 model-10000.h5 \u2502 \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 model-200000.h5 # (2) \u251c\u2500\u2500 config.yml \u251c\u2500\u2500 events.out.tfevents.1669084428.bookbot-tf-2.10561.0.v2 \u2514\u2500\u2500 predictions/ # (3) \u251c\u2500\u2500 100000steps/ \u2502 \u251c\u2500\u2500 b 'en-US-Madison_11' .png \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 10000steps/ \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 ... This contains all of the training checkpoints. The final model checkpoint (which we want). This contains all mid-training intermediate predictions (mel-spectrograms). It is missing the processor file, and the final model training checkpoint is still in the checkpoints/ subfolder. For the former, we can simply copy the file from dump to the output training folder. And for the latter, we can just copy the file up a directory. cd lightspeech-en-bookbot cp ../dump/englishipa_mapper.json processor.json cp checkpoints/model-200000.h5 model.h5","title":"Training LightSpeech"},{"location":"guides/lightspeech-mbmelgan/training/#training-multi-band-melgan","text":"We can then continue with the training of our Multi-band MelGAN as our Vocoder model. First of all, you have the option to either: 1. Train to generate speech from original mel-spectrogram, or 2. Train to generate speech from LightSpeech-predicted mel-spectrogram. This is also known as training on PostNets. Selecting option 1 would likely give you a more \"universal\" vocoder, one that would likely retain its performance on unseen mel-spectrograms. However, I often find its performance on small-sized datasets quite poor, and hence why I'd usually opt for the second option instead. Training on PostNets would allow the model to also learn the flaws of the LightSpeech-predicted mel-spectrograms and still aim to generate the best audio quality. To do so, we begin by extracting the PostNets of our LightSpeech models. This means running inference on all of our texts and saving the predicted mel-spectrograms. We can do so using this modified LightSpeech PostNet Extraction Script . With that, we can simply run CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/lightspeech/extractls_postnets.py \\ --rootdir ./dump/train \\ --outdir ./dump/train \\ --config ./TensorFlowTTS/examples/lightspeech/conf/lightspeech_englishipa.yaml \\ --checkpoint ./lightspeech-en-bookbot/model.h5 \\ --dataset_mapping ./lightspeech-en-bookbot/processor.json CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/lightspeech/extractls_postnets.py \\ --rootdir ./dump/valid \\ --outdir ./dump/valid \\ --config ./TensorFlowTTS/examples/lightspeech/conf/lightspeech_englishipa.yaml \\ --checkpoint ./lightspeech-en-bookbot/model.h5 \\ --dataset_mapping ./lightspeech-en-bookbot/processor.json That will perform inference on the training and validation subsets. Finally, we can train the Multi-band MelGAN with the HiFi-GAN Discriminator by doing the following CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/multiband_melgan_hf/train_multiband_melgan_hf.py \\ --train-dir ./dump/train/ \\ --dev-dir ./dump/valid/ \\ --outdir ./mb-melgan-hifi-en-bookbot/ \\ --config ./TensorFlowTTS/examples/multiband_melgan_hf/conf/multiband_melgan_hf.en.v1.yml \\ --use-norm 1 \\ --generator_mixed_precision 1 \\ --postnets 1 \\ --resume \"\" CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/multiband_melgan_hf/train_multiband_melgan_hf.py \\ --train-dir ./dump/train/ \\ --dev-dir ./dump/valid/ \\ --outdir ./mb-melgan-hifi-en-bookbot/ \\ --config ./TensorFlowTTS/examples/multiband_melgan_hf/conf/multiband_melgan_hf.en.v1.yml \\ --use-norm 1 \\ --postnets 1 \\ --resume ./mb-melgan-hifi-en-bookbot/checkpoints/ckpt-200000 Note that this first pre-trains only the generator for 200,000 steps, and then continues the remaining steps with the usual GAN training framework. At the end of training, we should end up with the following files mb-melgan-hifi-en-bookbot/ \u251c\u2500\u2500 checkpoints/ # (1) \u2502 \u251c\u2500\u2500 checkpoint \u2502 \u251c\u2500\u2500 ckpt-100000.data-00000-of-00001 \u2502 \u251c\u2500\u2500 ckpt-100000.index \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 discriminator-100000.h5 \u2502 \u251c\u2500\u2500 discriminator-120000.h5 \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 generator-100000.h5 \u2502 \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 generator-1000000.h5 # (2) \u251c\u2500\u2500 config.yml \u251c\u2500\u2500 events.out.tfevents.1669101534.bookbot-tf-2.21897.0.v2 \u251c\u2500\u2500 events.out.tfevents.1669116847.bookbot-tf-2.31894.0.v2 \u2514\u2500\u2500 predictions/ # (3) \u251c\u2500\u2500 100000steps/ \u2502 \u251c\u2500\u2500 b 'en-US-Madison_11' .png \u2502 \u251c\u2500\u2500 b 'en-US-Madison_11' _gen.wav \u2502 \u251c\u2500\u2500 b 'en-US-Madison_11' _ref.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 10000steps/ \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 ... This contains all of the training checkpoints. The final generator checkpoint (which we want). This contains all mid-training intermediate predictions (waveforms). We can simply copy the final generator model into the output model directory. cd mb-melgan-hifi-en-bookbot cp checkpoints/generator-1000000.h5 model.h5 With that, we are done!","title":"Training Multi-band MelGAN"},{"location":"guides/lightspeech-mbmelgan/training/#extra-pushing-models-to-huggingface-hub","text":"To wrap things up, it will be very convenient to push our models to HuggingFace Hub . This would allow us to not only load pre-trained model weights stored on the Hub, but also has other goodies such as version control, documentation, etc., that comes with using Git. First, create a new repository on the Hub here . Second, install Git LFS in the machine you're working on sudo apt-get install git-lfs git lfs install The rest is pretty much the same as how you would push files to a Git repository. For example git clone https://huggingface.co/bookbot/lightspeech-mfa-en cp -r lightspeech-en-bookbot/* lightspeech-mfa-en cd lightspeech-mfa-en git add . && git commit -m \"added weights\" && git push git clone https://huggingface.co/bookbot/mb-melgan-hifi-postnets-en cp -r mb-melgan-hifi-en-bookbot/* mb-melgan-hifi-postnets-en cd mb-melgan-hifi-postnets-en git add . && git commit -m \"added weights\" && git push This would allow us to load models like so (in Python): model = TFAutoModel . from_pretrained ( \"bookbot/lightspeech-mfa-en\" )","title":"Extra: Pushing Models to HuggingFace Hub"},{"location":"models/english/","text":"English Acoustic Models Model SR (kHz) Tokenizer Dataset FastSpeech2 EN v2 22.05 Character-based Azure FastSpeech2 EN v3 44.10 g2p_en (ARPA) Azure FastSpeech2 MFA EN v2 22.05 g2p_en (ARPA) Azure FastSpeech2 MFA EN v3 44.10 gruut (IPA) Azure FastSpeech2 MFA EN v4 44.10 gruut (IPA) Azure (Mastered) FastSpeech2 MFA EN ESD Angry 44.10 gruut (IPA) Emotional Speech Dataset - Angry LightSpeech MFA EN 44.10 gruut (IPA) Azure (Mastered) LightSpeech MFA EN v2 44.10 gruut (IPA) Azure (Mastered) LightSpeech MFA EN v3 44.10 gruut (IPA) Azure (Mastered) LightSpeech MFA EN ESD 44.10 gruut (IPA) Emotional Speech Dataset - 0013 Vocoder Models Model SR (kHz) Dataset MB-MelGAN EN 22.05 Azure MB-MelGAN HiFi EN 22.05 Azure MB-MelGAN HiFi PostNets EN 22.05 Azure MB-MelGAN HiFi PostNets EN v2 22.05 Azure MB-MelGAN HiFi PostNets EN v3 44.10 Azure MB-MelGAN HiFi PostNets EN v5 44.10 Azure MB-MelGAN HiFi PostNets EN v6 44.10 Azure (Mastered) MB-MelGAN HiFi PostNets EN v7 44.10 Azure (Mastered) MB-MelGAN HiFi PostNets EN v8 44.10 Azure (Mastered) MB-MelGAN HiFi PostNets EN ESD Angry 44.10 Emotional Speech Dataset - Angry MB-MelGAN HiFi PostNets EN ESD 44.10 Emotional Speech Dataset - 0013","title":"English"},{"location":"models/english/#english","text":"","title":"English"},{"location":"models/english/#acoustic-models","text":"Model SR (kHz) Tokenizer Dataset FastSpeech2 EN v2 22.05 Character-based Azure FastSpeech2 EN v3 44.10 g2p_en (ARPA) Azure FastSpeech2 MFA EN v2 22.05 g2p_en (ARPA) Azure FastSpeech2 MFA EN v3 44.10 gruut (IPA) Azure FastSpeech2 MFA EN v4 44.10 gruut (IPA) Azure (Mastered) FastSpeech2 MFA EN ESD Angry 44.10 gruut (IPA) Emotional Speech Dataset - Angry LightSpeech MFA EN 44.10 gruut (IPA) Azure (Mastered) LightSpeech MFA EN v2 44.10 gruut (IPA) Azure (Mastered) LightSpeech MFA EN v3 44.10 gruut (IPA) Azure (Mastered) LightSpeech MFA EN ESD 44.10 gruut (IPA) Emotional Speech Dataset - 0013","title":"Acoustic Models"},{"location":"models/english/#vocoder-models","text":"Model SR (kHz) Dataset MB-MelGAN EN 22.05 Azure MB-MelGAN HiFi EN 22.05 Azure MB-MelGAN HiFi PostNets EN 22.05 Azure MB-MelGAN HiFi PostNets EN v2 22.05 Azure MB-MelGAN HiFi PostNets EN v3 44.10 Azure MB-MelGAN HiFi PostNets EN v5 44.10 Azure MB-MelGAN HiFi PostNets EN v6 44.10 Azure (Mastered) MB-MelGAN HiFi PostNets EN v7 44.10 Azure (Mastered) MB-MelGAN HiFi PostNets EN v8 44.10 Azure (Mastered) MB-MelGAN HiFi PostNets EN ESD Angry 44.10 Emotional Speech Dataset - Angry MB-MelGAN HiFi PostNets EN ESD 44.10 Emotional Speech Dataset - 0013","title":"Vocoder Models"},{"location":"models/indonesian/","text":"Indonesian Acoustic Models Model SR (kHz) Tokenizer Dataset FastSpeech2 ID 22.05 Character-based Azure + WaveNet + Weildan FastSpeech2 MFA ID v4 44.10 g2p_id (IPA) Weildan FastSpeech2 MFA ID v5 44.10 g2p_id (IPA) Weildan (Mastered) FastSpeech2 MFA ID v7 44.10 g2p_id (IPA) Azure LightSpeech MFA ID 44.10 g2p_id (IPA) Azure LightSpeech MFA ID v2 22.05 g2p_id (IPA) Azure LightSpeech MFA ID v3 32.00 g2p_id (IPA) Azure LightSpeech MFA ID v5 44.10 g2p_id (IPA) Althaf Tacotron2 ID 22.05 Character-based Azure Tacotron2 ID v2 44.10 Character-based Azure Vocoder Models Model SR (kHz) Dataset MB-MelGAN HiFi PostNets ID 22.05 Azure + WaveNet + Weildan MB-MelGAN HiFi PostNets ID v4 44.10 Weildan MB-MelGAN HiFi PostNets ID v5 44.10 Weildan (Mastered) MB-MelGAN HiFi PostNets ID v7 44.10 Azure MB-MelGAN HiFi PostNets ID v8 44.10 Azure MB-MelGAN HiFi PostNets ID v9 22.05 Azure MB-MelGAN HiFi PostNets ID v10 32.00 Azure MB-MelGAN HiFi PostNets ID v12 44.10 Althaf","title":"Indonesian"},{"location":"models/indonesian/#indonesian","text":"","title":"Indonesian"},{"location":"models/indonesian/#acoustic-models","text":"Model SR (kHz) Tokenizer Dataset FastSpeech2 ID 22.05 Character-based Azure + WaveNet + Weildan FastSpeech2 MFA ID v4 44.10 g2p_id (IPA) Weildan FastSpeech2 MFA ID v5 44.10 g2p_id (IPA) Weildan (Mastered) FastSpeech2 MFA ID v7 44.10 g2p_id (IPA) Azure LightSpeech MFA ID 44.10 g2p_id (IPA) Azure LightSpeech MFA ID v2 22.05 g2p_id (IPA) Azure LightSpeech MFA ID v3 32.00 g2p_id (IPA) Azure LightSpeech MFA ID v5 44.10 g2p_id (IPA) Althaf Tacotron2 ID 22.05 Character-based Azure Tacotron2 ID v2 44.10 Character-based Azure","title":"Acoustic Models"},{"location":"models/indonesian/#vocoder-models","text":"Model SR (kHz) Dataset MB-MelGAN HiFi PostNets ID 22.05 Azure + WaveNet + Weildan MB-MelGAN HiFi PostNets ID v4 44.10 Weildan MB-MelGAN HiFi PostNets ID v5 44.10 Weildan (Mastered) MB-MelGAN HiFi PostNets ID v7 44.10 Azure MB-MelGAN HiFi PostNets ID v8 44.10 Azure MB-MelGAN HiFi PostNets ID v9 22.05 Azure MB-MelGAN HiFi PostNets ID v10 32.00 Azure MB-MelGAN HiFi PostNets ID v12 44.10 Althaf","title":"Vocoder Models"},{"location":"models/javanese/","text":"Javanese Acoustic Models Model SR (kHz) Tokenizer Dataset LightSpeech MFA JV 44.10 Character-based SLR41 Vocoder Models Model SR (kHz) Dataset MB-MelGAN HiFi PostNets JV 44.10 SLR41","title":"Javanese"},{"location":"models/javanese/#javanese","text":"","title":"Javanese"},{"location":"models/javanese/#acoustic-models","text":"Model SR (kHz) Tokenizer Dataset LightSpeech MFA JV 44.10 Character-based SLR41","title":"Acoustic Models"},{"location":"models/javanese/#vocoder-models","text":"Model SR (kHz) Dataset MB-MelGAN HiFi PostNets JV 44.10 SLR41","title":"Vocoder Models"},{"location":"results/english/","text":"English Azure She says that she has more books as well. Original Audio FastSpeech2 MFA EN v4 & MB-MelGAN HiFi PostNets EN v6 LightSpeech MFA EN v2 & MB-MelGAN HiFi PostNets EN v 8 They are the types of dragons that give you a fright to look at. Original Audio FastSpeech2 MFA EN v4 & MB-MelGAN HiFi PostNets EN v6 LightSpeech MFA EN v2 & MB-MelGAN HiFi PostNets EN v 8 The mouse is asleep in his bed, unaware of the devious spirit stalking him in the night. Original Audio FastSpeech2 MFA EN v4 & MB-MelGAN HiFi PostNets EN v6 LightSpeech MFA EN v2 & MB-MelGAN HiFi PostNets EN v 8 Emotional Speech Dataset - 0013 The nine the eggs, I keep. Emotion Original Audio LightSpeech MFA EN ESD & MB-MelGAN HiFi PostNets EN ESD Angry Happy Neutral Surprise Sad","title":"English"},{"location":"results/english/#english","text":"","title":"English"},{"location":"results/english/#azure","text":"She says that she has more books as well. Original Audio FastSpeech2 MFA EN v4 & MB-MelGAN HiFi PostNets EN v6 LightSpeech MFA EN v2 & MB-MelGAN HiFi PostNets EN v 8 They are the types of dragons that give you a fright to look at. Original Audio FastSpeech2 MFA EN v4 & MB-MelGAN HiFi PostNets EN v6 LightSpeech MFA EN v2 & MB-MelGAN HiFi PostNets EN v 8 The mouse is asleep in his bed, unaware of the devious spirit stalking him in the night. Original Audio FastSpeech2 MFA EN v4 & MB-MelGAN HiFi PostNets EN v6 LightSpeech MFA EN v2 & MB-MelGAN HiFi PostNets EN v 8","title":"Azure"},{"location":"results/english/#emotional-speech-dataset-0013","text":"The nine the eggs, I keep. Emotion Original Audio LightSpeech MFA EN ESD & MB-MelGAN HiFi PostNets EN ESD Angry Happy Neutral Surprise Sad","title":"Emotional Speech Dataset - 0013"},{"location":"results/indonesian/","text":"Indonesian Azure Sepulang sekolah, Fitri sangat lapar. Original Audio LightSpeech MFA ID & MB-MelGAN HiFi PostNets ID v8 Ia harus mengganti baju dahulu. Original Audio LightSpeech MFA ID & MB-MelGAN HiFi PostNets ID v8 Ketika Nina kembali ke gua, ia terlalu besar untuk bisa masuk ke dalamnya! Original Audio LightSpeech MFA ID & MB-MelGAN HiFi PostNets ID v8","title":"Indonesian"},{"location":"results/indonesian/#indonesian","text":"","title":"Indonesian"},{"location":"results/indonesian/#azure","text":"Sepulang sekolah, Fitri sangat lapar. Original Audio LightSpeech MFA ID & MB-MelGAN HiFi PostNets ID v8 Ia harus mengganti baju dahulu. Original Audio LightSpeech MFA ID & MB-MelGAN HiFi PostNets ID v8 Ketika Nina kembali ke gua, ia terlalu besar untuk bisa masuk ke dalamnya! Original Audio LightSpeech MFA ID & MB-MelGAN HiFi PostNets ID v8","title":"Azure"}]}