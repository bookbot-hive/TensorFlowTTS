{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home This documentation web page aims to fully cover the end-to-end pipeline of using TensorFlowTTS to train and deploy text-to-speech models. Since the original repository seems to be inactive, these set of tutorials will work on this fork developed by Wilson Wongso which contains new features. Newly Added Features IPA -based Acoustic models for English and Indonesian LightSpeech Guides Installing TensorFlowTTS Recipes LightSpeech & Multi-band MelGAN Adding Custom Processor Inference/Deployment Converting to TensorFlow Lite Inference using TensorFlow Lite","title":"Home"},{"location":"#home","text":"This documentation web page aims to fully cover the end-to-end pipeline of using TensorFlowTTS to train and deploy text-to-speech models. Since the original repository seems to be inactive, these set of tutorials will work on this fork developed by Wilson Wongso which contains new features.","title":"Home"},{"location":"#newly-added-features","text":"IPA -based Acoustic models for English and Indonesian LightSpeech","title":"Newly Added Features"},{"location":"#guides","text":"Installing TensorFlowTTS Recipes LightSpeech & Multi-band MelGAN Adding Custom Processor Inference/Deployment Converting to TensorFlow Lite Inference using TensorFlow Lite","title":"Guides"},{"location":"installation/","text":"Installation Local Machines I highly recommend installing TensorFlowTTS (and TensorFlow) on a designated Conda environment. I personally prefer Miniconda over Anaconda, but either one works. To begin with, follow this guide to install Conda, and then create a new Python 3.9 environment, which I will call tensorflow . conda create -n tensorflow python = 3 .9 conda activate tensorflow In the new environment, I will install TensorFlow v2.3.1 which I have found to work for training and inference later. You can install it via pip . pip install tensorflow == 2 .3.1 Afterwards, clone the forked repository and install the library plus all of its requirements. git clone https://github.com/w11wo/TensorFlowTTS.git cd TensorFlowTTS pip install . Google Cloud Virtual Machines Installing TensorFlowTTS on a Google Cloud VM is similar to installing on a local machine. To make things easier, Google has provided us with a list of pre-built VM images that comes with TensorFlow and support for GPUs. I would go for the image: Debian 10 based Deep Learning VM for TensorFlow Enterprise 2.6 with CUDA 11.0 . Because the image already has TensorFlow installed, we just need to install the main library like the steps above git clone https://github.com/w11wo/TensorFlowTTS.git cd TensorFlowTTS pip install . For some reason, there will be a bug involving Numba, which we can easily solve by upgrading NumPy to the latest version pip install -U numpy And also install libsndfile1 via apt sudo apt-get install libsndfile1 Checking for a Successful Install A way to check if your installation is correct is by importing the library through Python. We can do so through command line. python -c \"import tensorflow_tts\" If no errors are raised, then we should be good to go!","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#local-machines","text":"I highly recommend installing TensorFlowTTS (and TensorFlow) on a designated Conda environment. I personally prefer Miniconda over Anaconda, but either one works. To begin with, follow this guide to install Conda, and then create a new Python 3.9 environment, which I will call tensorflow . conda create -n tensorflow python = 3 .9 conda activate tensorflow In the new environment, I will install TensorFlow v2.3.1 which I have found to work for training and inference later. You can install it via pip . pip install tensorflow == 2 .3.1 Afterwards, clone the forked repository and install the library plus all of its requirements. git clone https://github.com/w11wo/TensorFlowTTS.git cd TensorFlowTTS pip install .","title":"Local Machines"},{"location":"installation/#google-cloud-virtual-machines","text":"Installing TensorFlowTTS on a Google Cloud VM is similar to installing on a local machine. To make things easier, Google has provided us with a list of pre-built VM images that comes with TensorFlow and support for GPUs. I would go for the image: Debian 10 based Deep Learning VM for TensorFlow Enterprise 2.6 with CUDA 11.0 . Because the image already has TensorFlow installed, we just need to install the main library like the steps above git clone https://github.com/w11wo/TensorFlowTTS.git cd TensorFlowTTS pip install . For some reason, there will be a bug involving Numba, which we can easily solve by upgrading NumPy to the latest version pip install -U numpy And also install libsndfile1 via apt sudo apt-get install libsndfile1","title":"Google Cloud Virtual Machines"},{"location":"installation/#checking-for-a-successful-install","text":"A way to check if your installation is correct is by importing the library through Python. We can do so through command line. python -c \"import tensorflow_tts\" If no errors are raised, then we should be good to go!","title":"Checking for a Successful Install"},{"location":"models/english/","text":"English Acoustic Models Model SR (kHz) Tokenizer Dataset FastSpeech2 EN v2 22.05 Character-based Azure FastSpeech2 EN v3 44.1 g2p_en (ARPA) Azure FastSpeech2 MFA EN v2 22.05 g2p_en (ARPA) Azure FastSpeech2 MFA EN v3 44.1 gruut (IPA) Azure FastSpeech2 MFA EN v4 44.1 gruut (IPA) Azure (Mastered) FastSpeech2 MFA EN ESD Angry 44.1 gruut (IPA) Emotional Speech Dataset - Angry LightSpeech MFA EN 44.1 gruut (IPA) Azure (Mastered) LightSpeech MFA EN v2 44.1 gruut (IPA) Azure (Mastered) LightSpeech MFA EN ESD 44.1 gruut (IPA) Emotional Speech Dataset - 0013 Vocoder Models Model SR (kHz) Dataset MB-MelGAN EN 22.05 Azure MB-MelGAN HiFi EN 22.05 Azure MB-MelGAN HiFi PostNets EN 22.05 Azure MB-MelGAN HiFi PostNets EN v2 22.05 Azure MB-MelGAN HiFi PostNets EN v3 44.1 Azure MB-MelGAN HiFi PostNets EN v5 44.1 Azure MB-MelGAN HiFi PostNets EN v6 44.1 Azure (Mastered) MB-MelGAN HiFi PostNets EN v7 44.1 Azure (Mastered) MB-MelGAN HiFi PostNets EN v8 44.1 Azure (Mastered) MB-MelGAN HiFi PostNets EN ESD Angry 44.1 Emotional Speech Dataset - Angry MB-MelGAN HiFi PostNets EN ESD 44.1 Emotional Speech Dataset - 0013","title":"English"},{"location":"models/english/#english","text":"","title":"English"},{"location":"models/english/#acoustic-models","text":"Model SR (kHz) Tokenizer Dataset FastSpeech2 EN v2 22.05 Character-based Azure FastSpeech2 EN v3 44.1 g2p_en (ARPA) Azure FastSpeech2 MFA EN v2 22.05 g2p_en (ARPA) Azure FastSpeech2 MFA EN v3 44.1 gruut (IPA) Azure FastSpeech2 MFA EN v4 44.1 gruut (IPA) Azure (Mastered) FastSpeech2 MFA EN ESD Angry 44.1 gruut (IPA) Emotional Speech Dataset - Angry LightSpeech MFA EN 44.1 gruut (IPA) Azure (Mastered) LightSpeech MFA EN v2 44.1 gruut (IPA) Azure (Mastered) LightSpeech MFA EN ESD 44.1 gruut (IPA) Emotional Speech Dataset - 0013","title":"Acoustic Models"},{"location":"models/english/#vocoder-models","text":"Model SR (kHz) Dataset MB-MelGAN EN 22.05 Azure MB-MelGAN HiFi EN 22.05 Azure MB-MelGAN HiFi PostNets EN 22.05 Azure MB-MelGAN HiFi PostNets EN v2 22.05 Azure MB-MelGAN HiFi PostNets EN v3 44.1 Azure MB-MelGAN HiFi PostNets EN v5 44.1 Azure MB-MelGAN HiFi PostNets EN v6 44.1 Azure (Mastered) MB-MelGAN HiFi PostNets EN v7 44.1 Azure (Mastered) MB-MelGAN HiFi PostNets EN v8 44.1 Azure (Mastered) MB-MelGAN HiFi PostNets EN ESD Angry 44.1 Emotional Speech Dataset - Angry MB-MelGAN HiFi PostNets EN ESD 44.1 Emotional Speech Dataset - 0013","title":"Vocoder Models"},{"location":"models/indonesian/","text":"Indonesian Acoustic Models Model SR (kHz) Tokenizer Dataset FastSpeech2 ID 22.05 Character-based Azure + WaveNet + Weildan FastSpeech2 MFA ID v4 44.1 g2p_id (IPA) Weildan FastSpeech2 MFA ID v5 44.1 g2p_id (IPA) Weildan (Mastered) FastSpeech2 MFA ID v7 44.1 g2p_id (IPA) Azure LightSpeech MFA ID 44.1 g2p_id (IPA) Azure Tacotron2 ID 22.05 Character-based Azure Tacotron2 ID v2 44.1 Character-based Azure Vocoder Models Model SR (kHz) Dataset MB-MelGAN HiFi PostNets ID 22.05 Azure + WaveNet + Weildan MB-MelGAN HiFi PostNets ID v4 44.1 Weildan MB-MelGAN HiFi PostNets ID v5 44.1 Weildan (Mastered) MB-MelGAN HiFi PostNets ID v7 44.1 Azure MB-MelGAN HiFi PostNets ID v8 44.1 Azure","title":"Indonesian"},{"location":"models/indonesian/#indonesian","text":"","title":"Indonesian"},{"location":"models/indonesian/#acoustic-models","text":"Model SR (kHz) Tokenizer Dataset FastSpeech2 ID 22.05 Character-based Azure + WaveNet + Weildan FastSpeech2 MFA ID v4 44.1 g2p_id (IPA) Weildan FastSpeech2 MFA ID v5 44.1 g2p_id (IPA) Weildan (Mastered) FastSpeech2 MFA ID v7 44.1 g2p_id (IPA) Azure LightSpeech MFA ID 44.1 g2p_id (IPA) Azure Tacotron2 ID 22.05 Character-based Azure Tacotron2 ID v2 44.1 Character-based Azure","title":"Acoustic Models"},{"location":"models/indonesian/#vocoder-models","text":"Model SR (kHz) Dataset MB-MelGAN HiFi PostNets ID 22.05 Azure + WaveNet + Weildan MB-MelGAN HiFi PostNets ID v4 44.1 Weildan MB-MelGAN HiFi PostNets ID v5 44.1 Weildan (Mastered) MB-MelGAN HiFi PostNets ID v7 44.1 Azure MB-MelGAN HiFi PostNets ID v8 44.1 Azure","title":"Vocoder Models"},{"location":"recipes/lightspeech-mbmelgan/dataset/","text":"Dataset Folder Structure The structure of the training files is fairly simple and straightforward: {YOUR_DATASET}/ \u251c\u2500\u2500 {SPEAKER-1}/ \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-0}.lab \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-0}.wav \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-1}.lab \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-1}.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 {SPEAKER-2}/ \u2502 \u251c\u2500\u2500 {SPEAKER-2}_{UTTERANCE-0}.lab \u2502 \u251c\u2500\u2500 {SPEAKER-2}_{UTTERANCE-0}.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 ... A few key things to note here: Each speaker has its own subfolder within the root dataset folder. The filenames in the speaker subfolders follow the convention of {SPEAKER-#}_{UTTERANCE-#} . It is important that they are delimited by an underscore ( _ ), so make sure that there is no _ within the speaker name and within the utterance ID. Use dashes - instead within them instead. Audios are in wav format and transcripts are of lab format (same content as you expect from a txt file; nothing fancy about it). The reason we use lab is simply to facilitate Montreal Forced Aligner training later. Example In the root directory en-bookbot , there are three speakers: en-AU-Zak , en-UK-Thalia , and en-US-Madison . The struture of the files are as follows: en-bookbot/ \u251c\u2500\u2500 en-AU-Zak/ \u2502 \u251c\u2500\u2500 en-AU-Zak_0.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_0.wav \u2502 \u251c\u2500\u2500 en-AU-Zak_1.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_1.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-UK-Thalia/ \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.lab \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.wav \u2502 \u251c\u2500\u2500 ... \u2514\u2500\u2500 en-US-Madison/ \u251c\u2500\u2500 en-US-Madison_0.lab \u251c\u2500\u2500 en-US-Madison_0.wav \u251c\u2500\u2500 ... Lexicon Another required component for training a Montreal Forced Aligner is a lexicon file (usually named lexicon.txt ). A lexicon simply maps words (graphemes) to phonemes, i.e. a pronunciation dictionary. Later, these phonemes will be aligned to segments of audio that they correpond to, and its duration will be learned by the forced aligner. There are many available lexicons out there, such as ones provided by Montreal Forced Aligner and Open Dict Data . You can either find other pre-existing lexicons, or create your own. Otherwise, another option would to treat each grapheme character as proxy phonemes as done by Meyer et al. (2022) where a lexicon is unavailable in certain languages: Two languages (ewe and yor) were aligned via forced alignment from scratch. Using only the found audio and transcripts (i.e., without a pre-trained acoustic model), an acoustic model was trained and the data aligned with the Montreal Forced Aligner. Graphemes were used as a proxy for phonemes in place of G2P data. In any case, the lexicon file should consist of tab -delimited word-phoneme pairs, which looks like the following: what w \u02c8\u028c t is \u02c8\u026a z biology b a\u026a \u02c8\u0251 l \u0259 d\u0361\u0292 i ? ? the \u00f0 \u0259 study s t \u02c8\u028c d i of \u0259 v living l \u02c8\u026a v \u026a \u014b things \u03b8 \u02c8\u026a \u014b z . . from f \u0279 \u02c8\u028c m ... Another example would be: : : , , . . ! ! ? ? ; ; a a b b e c t\u0283 e d d e abad a b a d abadi a b a d i abadiah a b a d i a h abadikan a b a d i k a n ... There are a few key things to note as well: The lexicon should cover all words in the audio corpus -- no out-of-vocabulary words should be present during the training of the aligner later. Otherwise, this will result in unknown tokens and will likely disrupt the training and duration parsing processes. Include all the punctuations you would want to have in the model later. For instance, I would usually keep . , : ; ? ! because they might imply different pause durations and/or intonations. Every other punctuations not in the lexicon will be stripped during the alignment process. Individual phonemes should be separated with whitespaces, e.g. a is its own phoneme unit, and t\u0283 is also considered as another single phoneme unit despite having 2 characters. Structuring your dataset, preprocessing, and lexicon preparation are arguably the most complicated part of training these kinds of models. But once we're over this step, everything else should be quite easy to follow.","title":"Dataset"},{"location":"recipes/lightspeech-mbmelgan/dataset/#dataset","text":"","title":"Dataset"},{"location":"recipes/lightspeech-mbmelgan/dataset/#folder-structure","text":"The structure of the training files is fairly simple and straightforward: {YOUR_DATASET}/ \u251c\u2500\u2500 {SPEAKER-1}/ \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-0}.lab \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-0}.wav \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-1}.lab \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-1}.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 {SPEAKER-2}/ \u2502 \u251c\u2500\u2500 {SPEAKER-2}_{UTTERANCE-0}.lab \u2502 \u251c\u2500\u2500 {SPEAKER-2}_{UTTERANCE-0}.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 ... A few key things to note here: Each speaker has its own subfolder within the root dataset folder. The filenames in the speaker subfolders follow the convention of {SPEAKER-#}_{UTTERANCE-#} . It is important that they are delimited by an underscore ( _ ), so make sure that there is no _ within the speaker name and within the utterance ID. Use dashes - instead within them instead. Audios are in wav format and transcripts are of lab format (same content as you expect from a txt file; nothing fancy about it). The reason we use lab is simply to facilitate Montreal Forced Aligner training later.","title":"Folder Structure"},{"location":"recipes/lightspeech-mbmelgan/dataset/#example","text":"In the root directory en-bookbot , there are three speakers: en-AU-Zak , en-UK-Thalia , and en-US-Madison . The struture of the files are as follows: en-bookbot/ \u251c\u2500\u2500 en-AU-Zak/ \u2502 \u251c\u2500\u2500 en-AU-Zak_0.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_0.wav \u2502 \u251c\u2500\u2500 en-AU-Zak_1.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_1.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-UK-Thalia/ \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.lab \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.wav \u2502 \u251c\u2500\u2500 ... \u2514\u2500\u2500 en-US-Madison/ \u251c\u2500\u2500 en-US-Madison_0.lab \u251c\u2500\u2500 en-US-Madison_0.wav \u251c\u2500\u2500 ...","title":"Example"},{"location":"recipes/lightspeech-mbmelgan/dataset/#lexicon","text":"Another required component for training a Montreal Forced Aligner is a lexicon file (usually named lexicon.txt ). A lexicon simply maps words (graphemes) to phonemes, i.e. a pronunciation dictionary. Later, these phonemes will be aligned to segments of audio that they correpond to, and its duration will be learned by the forced aligner. There are many available lexicons out there, such as ones provided by Montreal Forced Aligner and Open Dict Data . You can either find other pre-existing lexicons, or create your own. Otherwise, another option would to treat each grapheme character as proxy phonemes as done by Meyer et al. (2022) where a lexicon is unavailable in certain languages: Two languages (ewe and yor) were aligned via forced alignment from scratch. Using only the found audio and transcripts (i.e., without a pre-trained acoustic model), an acoustic model was trained and the data aligned with the Montreal Forced Aligner. Graphemes were used as a proxy for phonemes in place of G2P data. In any case, the lexicon file should consist of tab -delimited word-phoneme pairs, which looks like the following: what w \u02c8\u028c t is \u02c8\u026a z biology b a\u026a \u02c8\u0251 l \u0259 d\u0361\u0292 i ? ? the \u00f0 \u0259 study s t \u02c8\u028c d i of \u0259 v living l \u02c8\u026a v \u026a \u014b things \u03b8 \u02c8\u026a \u014b z . . from f \u0279 \u02c8\u028c m ... Another example would be: : : , , . . ! ! ? ? ; ; a a b b e c t\u0283 e d d e abad a b a d abadi a b a d i abadiah a b a d i a h abadikan a b a d i k a n ... There are a few key things to note as well: The lexicon should cover all words in the audio corpus -- no out-of-vocabulary words should be present during the training of the aligner later. Otherwise, this will result in unknown tokens and will likely disrupt the training and duration parsing processes. Include all the punctuations you would want to have in the model later. For instance, I would usually keep . , : ; ? ! because they might imply different pause durations and/or intonations. Every other punctuations not in the lexicon will be stripped during the alignment process. Individual phonemes should be separated with whitespaces, e.g. a is its own phoneme unit, and t\u0283 is also considered as another single phoneme unit despite having 2 characters. Structuring your dataset, preprocessing, and lexicon preparation are arguably the most complicated part of training these kinds of models. But once we're over this step, everything else should be quite easy to follow.","title":"Lexicon"},{"location":"recipes/lightspeech-mbmelgan/duration-extraction/","text":"Duration Extraction Forced Alignment Recall that we had prepared a lexicon file that maps graphemes (words) into phonemes. These phonemes will then be aligned to segments of the corresponding audio, whose duration we will then use to feed into models like LightSpeech. The end result of audio alignment would look something like the following: Notice that chunks of audio are aligned to each word and its subsequent phonemes. By the end of the training, TextGrid files will be generated, containing the alignment results. They look like the following: File type = \"ooTextFile\" Object class = \"TextGrid\" xmin = 0 xmax = 7.398163265306122 tiers? <exists> size = 2 item []: item [1]: class = \"IntervalTier\" name = \"words\" xmin = 0 xmax = 7.398163265306122 intervals: size = 34 intervals [1]: xmin = 0 xmax = 0.04 text = \"\" intervals [2]: xmin = 0.04 xmax = 0.26 text = \"he\" intervals [3]: xmin = 0.26 xmax = 0.44 text = \"is\" intervals [4]: xmin = 0.44 xmax = 0.48 text = \"\" intervals [5]: xmin = 0.48 xmax = 0.91 text = \"going\" intervals [6]: xmin = 0.91 xmax = 0.94 text = \"\" intervals [7]: xmin = 0.94 xmax = 1.05 text = \"to\" Montreal Forced Aligner (MFA) is an algorithm and a library that will help train these kinds of acoustic models. The TextGrid files will then be parsed and the durations of phonemes will be obtained. It is these phoneme durations that will then be learned by LightSpeech via L2 loss. External Aligner-free Models Recent development is aiming to completely remove the usage of external-aligner tools like MFA, and for good reasons. It will not only simplify the training pipeline by learning the token duration on-the-fly, but will also improve the speech quality and speed up alignment convergence, especially on longer sequence of text during inference. Most of the latest models that does its own alignment learning normally use a combination of Connectionist Temporal Classification and Monotonic Alignment Search -- for instance Glow-TTS , VITS , and JETS , among many others. As far as I know, these types of models have yet to be integrated into TensorFlowTTS, although there is an existing branch that attempts to implement AlignTTS . Installing Montreal Forced Aligner To begin, we can start by first installing Montreal Forced Aligner. It is much easier to install it via Conda Forge. In the same Conda environment, you can run these commands conda config --add channels conda-forge conda install montreal-forced-aligner If you're not using Conda (e.g. upgrading from non-Conda version, or installing from Source), you can follow the official guide here . To confirm that the installation is successful, you can run mfa version which will return the version you've installed (in my case, it's 2.0.5 ). Training an MFA Aligner With MFA installed, training an aligner model is as simple as running the command mfa train { YOUR_DATASET } { LEXICON } { OUTPUT_ACOUSTIC_MODEL } { TEXTGRID_OUTPUT_DIR } --punctuation \"\" Example Using the same sample dataset and a lexicon file lexicon.txt , the command to run will be similar to the following mfa train ./en-bookbot ./lexicon.txt ./outputs/en_bookbot_acoustic_model.zip ./outputs/parsed --punctuation \"\" Parsing TextGrid Files We can ignore the acoustic model generated by MFA, although you can always keep this for other purposes. What's more important for us is the resultant TextGrid files located in your {TEXTGRID_OUTPUT_DIR} . Then, you could either use the original parser script or what I usually use is a modified version of the script. TxtGridParser import os from dataclasses import dataclass from tqdm.auto import tqdm import textgrid import numpy as np import re @dataclass class TxtGridParser : sample_rate : int multi_speaker : bool txt_grid_path : str hop_size : int output_durations_path : str dataset_path : str training_file : str = \"train.txt\" phones_mapper = { \"sil\" : \"SIL\" , \"\" : \"SIL\" } sil_phones = set ( phones_mapper . keys ()) punctuations = [ \";\" , \"?\" , \"!\" , \".\" , \",\" , \":\" ] def parse ( self ): speakers = ( [ i for i in os . listdir ( self . txt_grid_path ) if os . path . isdir ( os . path . join ( self . txt_grid_path , i )) ] if self . multi_speaker else [] ) data = [] if speakers : for speaker in speakers : file_list = os . listdir ( os . path . join ( self . txt_grid_path , speaker )) self . parse_text_grid ( file_list , data , speaker ) else : file_list = os . listdir ( self . txt_grid_path ) self . parse_text_grid ( file_list , data , \"\" ) with open ( os . path . join ( self . dataset_path , self . training_file ), \"w\" , encoding = \"utf-8\" ) as f : f . writelines ( data ) def parse_text_grid ( self , file_list : list , data : list , speaker_name : str ): for f_name in tqdm ( file_list ): text_grid = textgrid . TextGrid . fromFile ( os . path . join ( self . txt_grid_path , speaker_name , f_name ) ) pha = text_grid [ 1 ] durations = [] phs = [] flags = [] for iterator , interval in enumerate ( pha . intervals ): mark = interval . mark if mark in self . sil_phones or mark in self . punctuations : flags . append ( True ) else : flags . append ( False ) if mark in self . sil_phones : mark = self . phones_mapper [ mark ] dur = interval . duration () * ( self . sample_rate / self . hop_size ) durations . append ( round ( dur )) phs . append ( mark ) new_durations = [] for idx , ( flag , dur ) in enumerate ( zip ( flags , durations )): if len ( new_durations ) == 0 or ( flag and flags [ idx - 1 ] == False ): new_durations . append ( dur ) elif flag : new_durations [ len ( new_durations ) - 1 ] += dur else : new_durations . append ( dur ) full_ph = \" \" . join ( phs ) new_ph = full_ph matches = re . finditer ( \"( ?SIL)* ?([,!\\?\\.;:] ?){1,} ?(SIL ?)*\" , full_ph ) for match in matches : substring = full_ph [ match . start () : match . end ()] new_ph = new_ph . replace ( substring , f \" { substring . replace ( 'SIL' , '' ) . strip ()[ 0 ] } \" , 1 ) . strip () assert new_ph . split () . __len__ () == new_durations . __len__ () # safety check base_name = f_name . split ( \".TextGrid\" )[ 0 ] np . save ( os . path . join ( self . output_durations_path , f \" { base_name } -durations.npy\" ), np . array ( new_durations ) . astype ( np . int32 ), allow_pickle = False , ) data . append ( f \" { speaker_name } / { base_name } | { new_ph } | { speaker_name } \\n \" ) Then to use the modified script above, you just have to specify the arguments to the parser. args = { \"dataset_path\" : \"./en-bookbot\" , \"txt_grid_path\" : \"./outputs/parsed\" , # (1) \"output_durations_path\" : \"./en-bookbot/durations\" , \"sample_rate\" : 44100 , # (2) \"hop_size\" : 512 , # (3) \"multi_speaker\" : True , # (4) \"training_file\" : \"train.txt\" } txt_grid_parser = TxtGridParser ( ** args ) txt_grid_parser . parse () Replace this with whatever your {TEXTGRID_OUTPUT_DIR} was. Set this to the desired sample rate of your text-to-speech model. Set this to the desired hop size of your text-to-speech model. Multi-speaker or not, you can keep this as True . With the duration files located in durations/ and train.txt , we can finally train our own text-to-speech model!","title":"Duration Extraction"},{"location":"recipes/lightspeech-mbmelgan/duration-extraction/#duration-extraction","text":"","title":"Duration Extraction"},{"location":"recipes/lightspeech-mbmelgan/duration-extraction/#forced-alignment","text":"Recall that we had prepared a lexicon file that maps graphemes (words) into phonemes. These phonemes will then be aligned to segments of the corresponding audio, whose duration we will then use to feed into models like LightSpeech. The end result of audio alignment would look something like the following: Notice that chunks of audio are aligned to each word and its subsequent phonemes. By the end of the training, TextGrid files will be generated, containing the alignment results. They look like the following: File type = \"ooTextFile\" Object class = \"TextGrid\" xmin = 0 xmax = 7.398163265306122 tiers? <exists> size = 2 item []: item [1]: class = \"IntervalTier\" name = \"words\" xmin = 0 xmax = 7.398163265306122 intervals: size = 34 intervals [1]: xmin = 0 xmax = 0.04 text = \"\" intervals [2]: xmin = 0.04 xmax = 0.26 text = \"he\" intervals [3]: xmin = 0.26 xmax = 0.44 text = \"is\" intervals [4]: xmin = 0.44 xmax = 0.48 text = \"\" intervals [5]: xmin = 0.48 xmax = 0.91 text = \"going\" intervals [6]: xmin = 0.91 xmax = 0.94 text = \"\" intervals [7]: xmin = 0.94 xmax = 1.05 text = \"to\" Montreal Forced Aligner (MFA) is an algorithm and a library that will help train these kinds of acoustic models. The TextGrid files will then be parsed and the durations of phonemes will be obtained. It is these phoneme durations that will then be learned by LightSpeech via L2 loss. External Aligner-free Models Recent development is aiming to completely remove the usage of external-aligner tools like MFA, and for good reasons. It will not only simplify the training pipeline by learning the token duration on-the-fly, but will also improve the speech quality and speed up alignment convergence, especially on longer sequence of text during inference. Most of the latest models that does its own alignment learning normally use a combination of Connectionist Temporal Classification and Monotonic Alignment Search -- for instance Glow-TTS , VITS , and JETS , among many others. As far as I know, these types of models have yet to be integrated into TensorFlowTTS, although there is an existing branch that attempts to implement AlignTTS .","title":"Forced Alignment"},{"location":"recipes/lightspeech-mbmelgan/duration-extraction/#installing-montreal-forced-aligner","text":"To begin, we can start by first installing Montreal Forced Aligner. It is much easier to install it via Conda Forge. In the same Conda environment, you can run these commands conda config --add channels conda-forge conda install montreal-forced-aligner If you're not using Conda (e.g. upgrading from non-Conda version, or installing from Source), you can follow the official guide here . To confirm that the installation is successful, you can run mfa version which will return the version you've installed (in my case, it's 2.0.5 ).","title":"Installing Montreal Forced Aligner"},{"location":"recipes/lightspeech-mbmelgan/duration-extraction/#training-an-mfa-aligner","text":"With MFA installed, training an aligner model is as simple as running the command mfa train { YOUR_DATASET } { LEXICON } { OUTPUT_ACOUSTIC_MODEL } { TEXTGRID_OUTPUT_DIR } --punctuation \"\"","title":"Training an MFA Aligner"},{"location":"recipes/lightspeech-mbmelgan/duration-extraction/#example","text":"Using the same sample dataset and a lexicon file lexicon.txt , the command to run will be similar to the following mfa train ./en-bookbot ./lexicon.txt ./outputs/en_bookbot_acoustic_model.zip ./outputs/parsed --punctuation \"\"","title":"Example"},{"location":"recipes/lightspeech-mbmelgan/duration-extraction/#parsing-textgrid-files","text":"We can ignore the acoustic model generated by MFA, although you can always keep this for other purposes. What's more important for us is the resultant TextGrid files located in your {TEXTGRID_OUTPUT_DIR} . Then, you could either use the original parser script or what I usually use is a modified version of the script. TxtGridParser import os from dataclasses import dataclass from tqdm.auto import tqdm import textgrid import numpy as np import re @dataclass class TxtGridParser : sample_rate : int multi_speaker : bool txt_grid_path : str hop_size : int output_durations_path : str dataset_path : str training_file : str = \"train.txt\" phones_mapper = { \"sil\" : \"SIL\" , \"\" : \"SIL\" } sil_phones = set ( phones_mapper . keys ()) punctuations = [ \";\" , \"?\" , \"!\" , \".\" , \",\" , \":\" ] def parse ( self ): speakers = ( [ i for i in os . listdir ( self . txt_grid_path ) if os . path . isdir ( os . path . join ( self . txt_grid_path , i )) ] if self . multi_speaker else [] ) data = [] if speakers : for speaker in speakers : file_list = os . listdir ( os . path . join ( self . txt_grid_path , speaker )) self . parse_text_grid ( file_list , data , speaker ) else : file_list = os . listdir ( self . txt_grid_path ) self . parse_text_grid ( file_list , data , \"\" ) with open ( os . path . join ( self . dataset_path , self . training_file ), \"w\" , encoding = \"utf-8\" ) as f : f . writelines ( data ) def parse_text_grid ( self , file_list : list , data : list , speaker_name : str ): for f_name in tqdm ( file_list ): text_grid = textgrid . TextGrid . fromFile ( os . path . join ( self . txt_grid_path , speaker_name , f_name ) ) pha = text_grid [ 1 ] durations = [] phs = [] flags = [] for iterator , interval in enumerate ( pha . intervals ): mark = interval . mark if mark in self . sil_phones or mark in self . punctuations : flags . append ( True ) else : flags . append ( False ) if mark in self . sil_phones : mark = self . phones_mapper [ mark ] dur = interval . duration () * ( self . sample_rate / self . hop_size ) durations . append ( round ( dur )) phs . append ( mark ) new_durations = [] for idx , ( flag , dur ) in enumerate ( zip ( flags , durations )): if len ( new_durations ) == 0 or ( flag and flags [ idx - 1 ] == False ): new_durations . append ( dur ) elif flag : new_durations [ len ( new_durations ) - 1 ] += dur else : new_durations . append ( dur ) full_ph = \" \" . join ( phs ) new_ph = full_ph matches = re . finditer ( \"( ?SIL)* ?([,!\\?\\.;:] ?){1,} ?(SIL ?)*\" , full_ph ) for match in matches : substring = full_ph [ match . start () : match . end ()] new_ph = new_ph . replace ( substring , f \" { substring . replace ( 'SIL' , '' ) . strip ()[ 0 ] } \" , 1 ) . strip () assert new_ph . split () . __len__ () == new_durations . __len__ () # safety check base_name = f_name . split ( \".TextGrid\" )[ 0 ] np . save ( os . path . join ( self . output_durations_path , f \" { base_name } -durations.npy\" ), np . array ( new_durations ) . astype ( np . int32 ), allow_pickle = False , ) data . append ( f \" { speaker_name } / { base_name } | { new_ph } | { speaker_name } \\n \" ) Then to use the modified script above, you just have to specify the arguments to the parser. args = { \"dataset_path\" : \"./en-bookbot\" , \"txt_grid_path\" : \"./outputs/parsed\" , # (1) \"output_durations_path\" : \"./en-bookbot/durations\" , \"sample_rate\" : 44100 , # (2) \"hop_size\" : 512 , # (3) \"multi_speaker\" : True , # (4) \"training_file\" : \"train.txt\" } txt_grid_parser = TxtGridParser ( ** args ) txt_grid_parser . parse () Replace this with whatever your {TEXTGRID_OUTPUT_DIR} was. Set this to the desired sample rate of your text-to-speech model. Set this to the desired hop size of your text-to-speech model. Multi-speaker or not, you can keep this as True . With the duration files located in durations/ and train.txt , we can finally train our own text-to-speech model!","title":"Parsing TextGrid Files"},{"location":"recipes/lightspeech-mbmelgan/intro/","text":"Introduction This guide will explain how to train a LightSpeech acoustic model and a Multi-band MelGAN vocoder model (with a HiFi-GAN Discriminator). In particular, we will be training a 44.1 kHz model, with a hop size of 512. This guide expects you to train an IPA-based model. For now, this tutorial only supports English and Indonesian -- because only these two languages have its correponding IPA-based grapheme-to-phoneme processor added to the custom fork. For English, we use gruut and for Indonesian, we use g2p_id . To add support for other languages, you would need a grapheme-to-phoneme converter for that language, and support it as a processor in TensorFlowTTS. We will introduce a separate tutorial for that in the future. LightSpeech LightSpeech follows the same architecture as FastSpeech2 , except with an optimized model configuration obtained via Neural Architecture Search (NAS). In our case, we don't really perform NAS, but use the previously found best model configuration. Multi-band MelGAN Multi-Band MelGAN is an improvement upon MelGAN that does waveform generation and waveform discrimination on a multi-band basis. HiFi-GAN Discrminator Further, instead of using the original discriminator, we can use the discriminator presented in the HiFi-GAN paper. We specifically use the multi-period discriminator (MPD) on the right.","title":"Introduction"},{"location":"recipes/lightspeech-mbmelgan/intro/#introduction","text":"This guide will explain how to train a LightSpeech acoustic model and a Multi-band MelGAN vocoder model (with a HiFi-GAN Discriminator). In particular, we will be training a 44.1 kHz model, with a hop size of 512. This guide expects you to train an IPA-based model. For now, this tutorial only supports English and Indonesian -- because only these two languages have its correponding IPA-based grapheme-to-phoneme processor added to the custom fork. For English, we use gruut and for Indonesian, we use g2p_id . To add support for other languages, you would need a grapheme-to-phoneme converter for that language, and support it as a processor in TensorFlowTTS. We will introduce a separate tutorial for that in the future.","title":"Introduction"},{"location":"recipes/lightspeech-mbmelgan/intro/#lightspeech","text":"LightSpeech follows the same architecture as FastSpeech2 , except with an optimized model configuration obtained via Neural Architecture Search (NAS). In our case, we don't really perform NAS, but use the previously found best model configuration.","title":"LightSpeech"},{"location":"recipes/lightspeech-mbmelgan/intro/#multi-band-melgan","text":"Multi-Band MelGAN is an improvement upon MelGAN that does waveform generation and waveform discrimination on a multi-band basis.","title":"Multi-band MelGAN"},{"location":"recipes/lightspeech-mbmelgan/intro/#hifi-gan-discrminator","text":"Further, instead of using the original discriminator, we can use the discriminator presented in the HiFi-GAN paper. We specifically use the multi-period discriminator (MPD) on the right.","title":"HiFi-GAN Discrminator"},{"location":"recipes/lightspeech-mbmelgan/training/","text":"Training Folder Structure Let's revisit the structure of our dataset now that we have new components. We need the audio files, duration files, and metadata file to be located in the same folder. Continuing with the same sample dataset , we should end up with these files by the end of the duration extraction step: en-bookbot/ \u251c\u2500\u2500 durations/ \u2502 \u251c\u2500\u2500 en-AU-Zak_0-durations.npy \u2502 \u251c\u2500\u2500 en-AU-Zal_1-durations.npy \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 en-UK-Thalia_0-durations.npy \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 en-US-Madison_0-durations.npy \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-AU-Zak/ \u2502 \u251c\u2500\u2500 en-AU-Zak_0.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_0.wav \u2502 \u251c\u2500\u2500 en-AU-Zak_1.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_1.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-UK-Thalia/ \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.lab \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-US-Madison/ \u2502 \u251c\u2500\u2500 en-US-Madison_0.lab \u2502 \u251c\u2500\u2500 en-US-Madison_0.wav \u2502 \u251c\u2500\u2500 ... \u2514\u2500\u2500 train.txt Training LightSpeech We start by first preprocessing and normalizing the audio files. These commands will handle tokenization, feature extraction, etc. tensorflow-tts-preprocess --rootdir ./en-bookbot --outdir ./dump --config TensorFlowTTS/preprocess/englishipa_preprocess.yaml --dataset englishipa --verbose 2 tensorflow-tts-normalize --rootdir ./dump --outdir ./dump --config TensorFlowTTS/preprocess/englishipa_preprocess.yaml --dataset englishipa --verbose 2 It's also recommended to fix mis-matching duration files python TensorFlowTTS/examples/mfa_extraction/fix_mismatch.py \\ --base_path ./dump \\ --trimmed_dur_path ./en-bookbot/trimmed-durations \\ --dur_path ./en-bookbot/durations \\ --use_norm t We can then train the LightSpeech model CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/lightspeech/train_lightspeech.py \\ --train-dir ./dump/train/ \\ --dev-dir ./dump/valid/ \\ --outdir ./lightspeech-en-bookbot \\ # (1) --config ./TensorFlowTTS/examples/lightspeech/conf/lightspeech_englishipa.yaml \\ # (2) --use-norm 1 \\ --f0-stat ./dump/stats_f0.npy \\ --energy-stat ./dump/stats_energy.npy \\ --mixed_precision 1 \\ --dataset_config TensorFlowTTS/preprocess/englishipa_preprocess.yaml \\ --dataset_stats dump/stats.npy \\ --dataset_mapping dump/englishipa_mapper.json You can set this to whatever output folder you'd like. This is a pre-configured training configuration. Feel free to customize it, but be careful with setting the sample rate and hop size. Once it's finished, you should end up with the following files: lightspeech-en-bookbot/ \u251c\u2500\u2500 checkpoints/ # (1) \u2502 \u251c\u2500\u2500 ckpt-10000.data-00000-of-00001 \u2502 \u251c\u2500\u2500 ckpt-10000.index \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 model-10000.h5 \u2502 \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 model-200000.h5 # (2) \u251c\u2500\u2500 config.yml \u251c\u2500\u2500 events.out.tfevents.1669084428.bookbot-tf-2.10561.0.v2 \u2514\u2500\u2500 predictions/ # (3) \u251c\u2500\u2500 100000steps/ \u251c\u2500\u2500 10000steps/ \u251c\u2500\u2500 ... This contains all of the training checkpoints. The final model checkpoint (which we want). This contains all mid-training intermediate predictions (mel-spectrograms). It is missing the processor file, and the final model training checkpoint is still in the checkpoints/ subfolder. For the former, we can simply copy the file from dump to the output training folder. And for the latter, we can just copy the file up a directory. cd lightspeech-en-bookbot cp ../dump/englishipa_mapper.json processor.json cp checkpoints/model-200000.h5 model.h5 Training Multi-band MelGAN We can then continue with the training of our Multi-band MelGAN as our Vocoder model. First of all, you have the option to either: 1. Train to generate speech from original mel-spectrogram, or 2. Train to generate speech from LightSpeech-predicted mel-spectrogram. This is also known as training on PostNets. Selecting option 1 would likely give you a more \"universal\" vocoder, one that would likely retain its performance on unseen mel-spectrograms. However, I often find its performance on small-sized datasets quite poor, and hence why I'd usually opt for the second option instead. Training on PostNets would allow the model to also learn the flaws of the LightSpeech-predicted mel-spectrograms and still aim to generate the best audio quality. To do so, we begin by extracting the PostNets of our LightSpeech models. This means running inference on all of our texts and saving the predicted mel-spectrograms. We can do so using this modified LightSpeech PostNet Extraction Script . With that, we can simply run CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/lightspeech/extractls_postnets.py \\ --rootdir ./dump/train \\ --outdir ./dump/train \\ --config ./TensorFlowTTS/examples/lightspeech/conf/lightspeech_englishipa.yaml \\ --checkpoint ./lightspeech-en-bookbot/model.h5 \\ --dataset_mapping ./lightspeech-en-bookbot/processor.json CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/lightspeech/extractls_postnets.py \\ --rootdir ./dump/valid \\ --outdir ./dump/valid \\ --config ./TensorFlowTTS/examples/lightspeech/conf/lightspeech_englishipa.yaml \\ --checkpoint ./lightspeech-en-bookbot/model.h5 \\ --dataset_mapping ./lightspeech-en-bookbot/processor.json That will perform inference on the training and validation subsets. Finally, we can train the Multi-band MelGAN with the HiFi-GAN Discriminator by doing the following CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/multiband_melgan_hf/train_multiband_melgan_hf.py \\ --train-dir ./dump/train/ \\ --dev-dir ./dump/valid/ \\ --outdir ./mb-melgan-hifi-en-bookbot/ \\ --config ./TensorFlowTTS/examples/multiband_melgan_hf/conf/multiband_melgan_hf.en.v1.yml \\ --use-norm 1 \\ --generator_mixed_precision 1 \\ --postnets 1 \\ --resume \"\" CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/multiband_melgan_hf/train_multiband_melgan_hf.py \\ --train-dir ./dump/train/ \\ --dev-dir ./dump/valid/ \\ --outdir ./mb-melgan-hifi-en-bookbot/ \\ --config ./TensorFlowTTS/examples/multiband_melgan_hf/conf/multiband_melgan_hf.en.v1.yml \\ --use-norm 1 \\ --postnets 1 \\ --resume ./mb-melgan-hifi-en-bookbot/checkpoints/ckpt-200000 Note that this first pre-trains only the generator for 200,000 steps, and then continues the remaining steps with the usual GAN training framework. With that, we are done!","title":"Training"},{"location":"recipes/lightspeech-mbmelgan/training/#training","text":"","title":"Training"},{"location":"recipes/lightspeech-mbmelgan/training/#folder-structure","text":"Let's revisit the structure of our dataset now that we have new components. We need the audio files, duration files, and metadata file to be located in the same folder. Continuing with the same sample dataset , we should end up with these files by the end of the duration extraction step: en-bookbot/ \u251c\u2500\u2500 durations/ \u2502 \u251c\u2500\u2500 en-AU-Zak_0-durations.npy \u2502 \u251c\u2500\u2500 en-AU-Zal_1-durations.npy \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 en-UK-Thalia_0-durations.npy \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 en-US-Madison_0-durations.npy \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-AU-Zak/ \u2502 \u251c\u2500\u2500 en-AU-Zak_0.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_0.wav \u2502 \u251c\u2500\u2500 en-AU-Zak_1.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_1.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-UK-Thalia/ \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.lab \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-US-Madison/ \u2502 \u251c\u2500\u2500 en-US-Madison_0.lab \u2502 \u251c\u2500\u2500 en-US-Madison_0.wav \u2502 \u251c\u2500\u2500 ... \u2514\u2500\u2500 train.txt","title":"Folder Structure"},{"location":"recipes/lightspeech-mbmelgan/training/#training-lightspeech","text":"We start by first preprocessing and normalizing the audio files. These commands will handle tokenization, feature extraction, etc. tensorflow-tts-preprocess --rootdir ./en-bookbot --outdir ./dump --config TensorFlowTTS/preprocess/englishipa_preprocess.yaml --dataset englishipa --verbose 2 tensorflow-tts-normalize --rootdir ./dump --outdir ./dump --config TensorFlowTTS/preprocess/englishipa_preprocess.yaml --dataset englishipa --verbose 2 It's also recommended to fix mis-matching duration files python TensorFlowTTS/examples/mfa_extraction/fix_mismatch.py \\ --base_path ./dump \\ --trimmed_dur_path ./en-bookbot/trimmed-durations \\ --dur_path ./en-bookbot/durations \\ --use_norm t We can then train the LightSpeech model CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/lightspeech/train_lightspeech.py \\ --train-dir ./dump/train/ \\ --dev-dir ./dump/valid/ \\ --outdir ./lightspeech-en-bookbot \\ # (1) --config ./TensorFlowTTS/examples/lightspeech/conf/lightspeech_englishipa.yaml \\ # (2) --use-norm 1 \\ --f0-stat ./dump/stats_f0.npy \\ --energy-stat ./dump/stats_energy.npy \\ --mixed_precision 1 \\ --dataset_config TensorFlowTTS/preprocess/englishipa_preprocess.yaml \\ --dataset_stats dump/stats.npy \\ --dataset_mapping dump/englishipa_mapper.json You can set this to whatever output folder you'd like. This is a pre-configured training configuration. Feel free to customize it, but be careful with setting the sample rate and hop size. Once it's finished, you should end up with the following files: lightspeech-en-bookbot/ \u251c\u2500\u2500 checkpoints/ # (1) \u2502 \u251c\u2500\u2500 ckpt-10000.data-00000-of-00001 \u2502 \u251c\u2500\u2500 ckpt-10000.index \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 model-10000.h5 \u2502 \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 model-200000.h5 # (2) \u251c\u2500\u2500 config.yml \u251c\u2500\u2500 events.out.tfevents.1669084428.bookbot-tf-2.10561.0.v2 \u2514\u2500\u2500 predictions/ # (3) \u251c\u2500\u2500 100000steps/ \u251c\u2500\u2500 10000steps/ \u251c\u2500\u2500 ... This contains all of the training checkpoints. The final model checkpoint (which we want). This contains all mid-training intermediate predictions (mel-spectrograms). It is missing the processor file, and the final model training checkpoint is still in the checkpoints/ subfolder. For the former, we can simply copy the file from dump to the output training folder. And for the latter, we can just copy the file up a directory. cd lightspeech-en-bookbot cp ../dump/englishipa_mapper.json processor.json cp checkpoints/model-200000.h5 model.h5","title":"Training LightSpeech"},{"location":"recipes/lightspeech-mbmelgan/training/#training-multi-band-melgan","text":"We can then continue with the training of our Multi-band MelGAN as our Vocoder model. First of all, you have the option to either: 1. Train to generate speech from original mel-spectrogram, or 2. Train to generate speech from LightSpeech-predicted mel-spectrogram. This is also known as training on PostNets. Selecting option 1 would likely give you a more \"universal\" vocoder, one that would likely retain its performance on unseen mel-spectrograms. However, I often find its performance on small-sized datasets quite poor, and hence why I'd usually opt for the second option instead. Training on PostNets would allow the model to also learn the flaws of the LightSpeech-predicted mel-spectrograms and still aim to generate the best audio quality. To do so, we begin by extracting the PostNets of our LightSpeech models. This means running inference on all of our texts and saving the predicted mel-spectrograms. We can do so using this modified LightSpeech PostNet Extraction Script . With that, we can simply run CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/lightspeech/extractls_postnets.py \\ --rootdir ./dump/train \\ --outdir ./dump/train \\ --config ./TensorFlowTTS/examples/lightspeech/conf/lightspeech_englishipa.yaml \\ --checkpoint ./lightspeech-en-bookbot/model.h5 \\ --dataset_mapping ./lightspeech-en-bookbot/processor.json CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/lightspeech/extractls_postnets.py \\ --rootdir ./dump/valid \\ --outdir ./dump/valid \\ --config ./TensorFlowTTS/examples/lightspeech/conf/lightspeech_englishipa.yaml \\ --checkpoint ./lightspeech-en-bookbot/model.h5 \\ --dataset_mapping ./lightspeech-en-bookbot/processor.json That will perform inference on the training and validation subsets. Finally, we can train the Multi-band MelGAN with the HiFi-GAN Discriminator by doing the following CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/multiband_melgan_hf/train_multiband_melgan_hf.py \\ --train-dir ./dump/train/ \\ --dev-dir ./dump/valid/ \\ --outdir ./mb-melgan-hifi-en-bookbot/ \\ --config ./TensorFlowTTS/examples/multiband_melgan_hf/conf/multiband_melgan_hf.en.v1.yml \\ --use-norm 1 \\ --generator_mixed_precision 1 \\ --postnets 1 \\ --resume \"\" CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/multiband_melgan_hf/train_multiband_melgan_hf.py \\ --train-dir ./dump/train/ \\ --dev-dir ./dump/valid/ \\ --outdir ./mb-melgan-hifi-en-bookbot/ \\ --config ./TensorFlowTTS/examples/multiband_melgan_hf/conf/multiband_melgan_hf.en.v1.yml \\ --use-norm 1 \\ --postnets 1 \\ --resume ./mb-melgan-hifi-en-bookbot/checkpoints/ckpt-200000 Note that this first pre-trains only the generator for 200,000 steps, and then continues the remaining steps with the usual GAN training framework. With that, we are done!","title":"Training Multi-band MelGAN"}]}