{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home This documentation web page aims to fully cover the end-to-end pipeline of using TensorFlowTTS to train and deploy text-to-speech models. Since the original repository seems to be inactive, these set of tutorials will work on this fork developed by Wilson Wongso which contains new features. Newly Added Features IPA -based Acoustic models for English and Indonesian LightSpeech Guides Installing TensorFlowTTS Recipes LightSpeech & Multi-band MelGAN Adding Custom Processor Inference/Deployment Converting to TensorFlow Lite Inference using TensorFlow Lite Pre-trained Models","title":"Home"},{"location":"#home","text":"This documentation web page aims to fully cover the end-to-end pipeline of using TensorFlowTTS to train and deploy text-to-speech models. Since the original repository seems to be inactive, these set of tutorials will work on this fork developed by Wilson Wongso which contains new features.","title":"Home"},{"location":"#newly-added-features","text":"IPA -based Acoustic models for English and Indonesian LightSpeech","title":"Newly Added Features"},{"location":"#guides","text":"Installing TensorFlowTTS Recipes LightSpeech & Multi-band MelGAN Adding Custom Processor Inference/Deployment Converting to TensorFlow Lite Inference using TensorFlow Lite","title":"Guides"},{"location":"#pre-trained-models","text":"","title":"Pre-trained Models"},{"location":"installation/","text":"Installation Local Machines I highly recommend installing TensorFlowTTS (and TensorFlow) on a designated Conda environment. I personally prefer Miniconda over Anaconda, but either one works. To begin with, follow this guide to install Conda, and then create a new Python 3.9 environment, which I will call tensorflow . conda create -n tensorflow python=3.9 conda activate tensorflow In the new environment, I will install TensorFlow v2.3.1 which I have found to work for training and inference later. You can install it via pip . pip install tensorflow==2.3.1 Afterwards, clone the forked repository and install the library plus all of its requirements. git clone https://github.com/w11wo/TensorFlowTTS.git cd TensorFlowTTS pip install . Google Cloud Virtual Machines Installing TensorFlowTTS on a Google Cloud VM is similar to installing on a local machine. To make things easier, Google has provided us with a list of pre-built VM images that comes with TensorFlow and support for GPUs. I would go for the image: Debian 10 based Deep Learning VM for TensorFlow Enterprise 2.6 with CUDA 11.0 . Because the image already has TensorFlow installed, we just need to install the main library like the steps above git clone https://github.com/w11wo/TensorFlowTTS.git cd TensorFlowTTS pip install . For some reason, there will be a bug involving Numba, which we can easily solve by upgrading NumPy to the latest version pip install -U numpy And also install libsndfile1 via apt sudo apt-get install libsndfile1 Checking for a Successful Install A way to check if your installation is correct is by importing the library through Python. We can do so through command line. python -c \"import tensorflow_tts\" If no errors are raised, then we should be good to go!","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#local-machines","text":"I highly recommend installing TensorFlowTTS (and TensorFlow) on a designated Conda environment. I personally prefer Miniconda over Anaconda, but either one works. To begin with, follow this guide to install Conda, and then create a new Python 3.9 environment, which I will call tensorflow . conda create -n tensorflow python=3.9 conda activate tensorflow In the new environment, I will install TensorFlow v2.3.1 which I have found to work for training and inference later. You can install it via pip . pip install tensorflow==2.3.1 Afterwards, clone the forked repository and install the library plus all of its requirements. git clone https://github.com/w11wo/TensorFlowTTS.git cd TensorFlowTTS pip install .","title":"Local Machines"},{"location":"installation/#google-cloud-virtual-machines","text":"Installing TensorFlowTTS on a Google Cloud VM is similar to installing on a local machine. To make things easier, Google has provided us with a list of pre-built VM images that comes with TensorFlow and support for GPUs. I would go for the image: Debian 10 based Deep Learning VM for TensorFlow Enterprise 2.6 with CUDA 11.0 . Because the image already has TensorFlow installed, we just need to install the main library like the steps above git clone https://github.com/w11wo/TensorFlowTTS.git cd TensorFlowTTS pip install . For some reason, there will be a bug involving Numba, which we can easily solve by upgrading NumPy to the latest version pip install -U numpy And also install libsndfile1 via apt sudo apt-get install libsndfile1","title":"Google Cloud Virtual Machines"},{"location":"installation/#checking-for-a-successful-install","text":"A way to check if your installation is correct is by importing the library through Python. We can do so through command line. python -c \"import tensorflow_tts\" If no errors are raised, then we should be good to go!","title":"Checking for a Successful Install"},{"location":"recipes/lightspeech-mbmelgan/dataset-structure/","text":"Dataset Structure The structure of the training files is fairly simple and straightforward: |- {YOUR_DATASET}/ | |- {SPEAKER-1}/ | |- |- {SPEAKER-1}_{UTTERANCE-0}.lab | |- |- {SPEAKER-1}_{UTTERANCE-0}.wav | |- |- {SPEAKER-1}_{UTTERANCE-1}.lab | |- |- {SPEAKER-1}_{UTTERANCE-1}.wav | |- |- ... | |- {SPEAKER-2}/ | |- |- {SPEAKER-2}_{UTTERANCE-0}.lab | |- |- {SPEAKER-2}_{UTTERANCE-0}.wav | |- |- ... | |- ... A few key things to note here: Each speaker has its own subfolder within the root dataset folder. The filenames in the speaker subfolders follow the convention of {SPEAKER-#}_{UTTERANCE#} . It is important that they are delimited by an underscore ( _ ), so make sure that there is no _ within the speaker name and within the utterance ID. Use dashes - instead within them instead. Audios are in wav format and transcripts are of lab format (same content as you expect from a txt file; nothing fancy about it). The reason we use lab is simply to facilitate Montreal Forced Aligner training later.","title":"Dataset Structure"},{"location":"recipes/lightspeech-mbmelgan/dataset-structure/#dataset-structure","text":"The structure of the training files is fairly simple and straightforward: |- {YOUR_DATASET}/ | |- {SPEAKER-1}/ | |- |- {SPEAKER-1}_{UTTERANCE-0}.lab | |- |- {SPEAKER-1}_{UTTERANCE-0}.wav | |- |- {SPEAKER-1}_{UTTERANCE-1}.lab | |- |- {SPEAKER-1}_{UTTERANCE-1}.wav | |- |- ... | |- {SPEAKER-2}/ | |- |- {SPEAKER-2}_{UTTERANCE-0}.lab | |- |- {SPEAKER-2}_{UTTERANCE-0}.wav | |- |- ... | |- ... A few key things to note here: Each speaker has its own subfolder within the root dataset folder. The filenames in the speaker subfolders follow the convention of {SPEAKER-#}_{UTTERANCE#} . It is important that they are delimited by an underscore ( _ ), so make sure that there is no _ within the speaker name and within the utterance ID. Use dashes - instead within them instead. Audios are in wav format and transcripts are of lab format (same content as you expect from a txt file; nothing fancy about it). The reason we use lab is simply to facilitate Montreal Forced Aligner training later.","title":"Dataset Structure"},{"location":"recipes/lightspeech-mbmelgan/intro/","text":"Introduction This guide will explain how to train a LightSpeech acoustic model and a Multi-band MelGAN vocoder model (with a HiFi-GAN Discriminator). This guide expects you to train an IPA-based model. For now, this tutorial only supports English and Indonesian -- because only these two languages have its correponding IPA-based grapheme-to-phoneme processor added to the custom fork. For English, we use gruut and for Indonesian, we use g2p_id . To add support for other languages, you would need a grapheme-to-phoneme converter for that language, and support it as a processor in TensorFlowTTS. We will introduce a separate tutorial for that in the future. LightSpeech LightSpeech follows the same architecture as FastSpeech2 , except with an optimized model configuration obtained via Neural Architecture Search (NAS). In our case, we don't really perform NAS, but use the previously found best model configuration. Multi-band MelGAN Multi-Band MelGAN is an improvement upon MelGAN that does waveform generation and waveform discrimination on a multi-band basis. HiFi-GAN Discrminator Further, instead of using the original discriminator, we can use the discriminator presented in the HiFi-GAN paper. We specifically use the multi-period discriminator (MPD) on the right.","title":"Introduction"},{"location":"recipes/lightspeech-mbmelgan/intro/#introduction","text":"This guide will explain how to train a LightSpeech acoustic model and a Multi-band MelGAN vocoder model (with a HiFi-GAN Discriminator). This guide expects you to train an IPA-based model. For now, this tutorial only supports English and Indonesian -- because only these two languages have its correponding IPA-based grapheme-to-phoneme processor added to the custom fork. For English, we use gruut and for Indonesian, we use g2p_id . To add support for other languages, you would need a grapheme-to-phoneme converter for that language, and support it as a processor in TensorFlowTTS. We will introduce a separate tutorial for that in the future.","title":"Introduction"},{"location":"recipes/lightspeech-mbmelgan/intro/#lightspeech","text":"LightSpeech follows the same architecture as FastSpeech2 , except with an optimized model configuration obtained via Neural Architecture Search (NAS). In our case, we don't really perform NAS, but use the previously found best model configuration.","title":"LightSpeech"},{"location":"recipes/lightspeech-mbmelgan/intro/#multi-band-melgan","text":"Multi-Band MelGAN is an improvement upon MelGAN that does waveform generation and waveform discrimination on a multi-band basis.","title":"Multi-band MelGAN"},{"location":"recipes/lightspeech-mbmelgan/intro/#hifi-gan-discrminator","text":"Further, instead of using the original discriminator, we can use the discriminator presented in the HiFi-GAN paper. We specifically use the multi-period discriminator (MPD) on the right.","title":"HiFi-GAN Discrminator"}]}