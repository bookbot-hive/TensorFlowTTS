{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home This documentation web page aims to fully cover the end-to-end pipeline of using TensorFlowTTS to train and deploy text-to-speech models. Since the original repository seems to be inactive, these set of tutorials will work on this fork developed by Wilson Wongso which contains new features. Newly Added Features IPA-based Acoustic models for English and Indonesian LightSpeech Guides Installing TensorFlowTTS Train LightSpeech & Multi-band MelGAN Convert and Infer Models on TensorFlowLite Implementing Custom Processor","title":"Home"},{"location":"#home","text":"This documentation web page aims to fully cover the end-to-end pipeline of using TensorFlowTTS to train and deploy text-to-speech models. Since the original repository seems to be inactive, these set of tutorials will work on this fork developed by Wilson Wongso which contains new features.","title":"Home"},{"location":"#newly-added-features","text":"IPA-based Acoustic models for English and Indonesian LightSpeech","title":"Newly Added Features"},{"location":"#guides","text":"Installing TensorFlowTTS Train LightSpeech & Multi-band MelGAN Convert and Infer Models on TensorFlowLite Implementing Custom Processor","title":"Guides"},{"location":"guides/installation/","text":"Installation Local Machines I highly recommend installing TensorFlowTTS (and TensorFlow) on a designated Conda environment. I personally prefer Miniconda over Anaconda, but either one works. To begin with, follow this guide to install Conda, and then create a new Python 3.9 environment, which I will call tensorflow . conda create -n tensorflow python = 3 .9 conda activate tensorflow In the new environment, I will install TensorFlow v2.3.1 which I have found to work for training and inference later. You can install it via pip . pip install tensorflow == 2 .3.1 Afterwards, clone the forked repository and install the library plus all of its requirements. git clone https://github.com/w11wo/TensorFlowTTS.git cd TensorFlowTTS pip install . Google Cloud Virtual Machines Installing TensorFlowTTS on a Google Cloud VM is similar to installing on a local machine. To make things easier, Google has provided us with a list of pre-built VM images that comes with TensorFlow and support for GPUs. I would go for the image: Debian 10 based Deep Learning VM for TensorFlow Enterprise 2.6 with CUDA 11.0 . Because the image already has TensorFlow installed, we just need to install the main library like the steps above git clone https://github.com/w11wo/TensorFlowTTS.git cd TensorFlowTTS pip install . For some reason, there will be a bug involving Numba, which we can easily solve by upgrading NumPy to the latest version pip install -U numpy And also install libsndfile1 via apt sudo apt-get install libsndfile1 Checking for a Successful Install A way to check if your installation is correct is by importing the library through Python. We can do so through command line. python -c \"import tensorflow_tts\" If no errors are raised, then we should be good to go!","title":"Installation"},{"location":"guides/installation/#installation","text":"","title":"Installation"},{"location":"guides/installation/#local-machines","text":"I highly recommend installing TensorFlowTTS (and TensorFlow) on a designated Conda environment. I personally prefer Miniconda over Anaconda, but either one works. To begin with, follow this guide to install Conda, and then create a new Python 3.9 environment, which I will call tensorflow . conda create -n tensorflow python = 3 .9 conda activate tensorflow In the new environment, I will install TensorFlow v2.3.1 which I have found to work for training and inference later. You can install it via pip . pip install tensorflow == 2 .3.1 Afterwards, clone the forked repository and install the library plus all of its requirements. git clone https://github.com/w11wo/TensorFlowTTS.git cd TensorFlowTTS pip install .","title":"Local Machines"},{"location":"guides/installation/#google-cloud-virtual-machines","text":"Installing TensorFlowTTS on a Google Cloud VM is similar to installing on a local machine. To make things easier, Google has provided us with a list of pre-built VM images that comes with TensorFlow and support for GPUs. I would go for the image: Debian 10 based Deep Learning VM for TensorFlow Enterprise 2.6 with CUDA 11.0 . Because the image already has TensorFlow installed, we just need to install the main library like the steps above git clone https://github.com/w11wo/TensorFlowTTS.git cd TensorFlowTTS pip install . For some reason, there will be a bug involving Numba, which we can easily solve by upgrading NumPy to the latest version pip install -U numpy And also install libsndfile1 via apt sudo apt-get install libsndfile1","title":"Google Cloud Virtual Machines"},{"location":"guides/installation/#checking-for-a-successful-install","text":"A way to check if your installation is correct is by importing the library through Python. We can do so through command line. python -c \"import tensorflow_tts\" If no errors are raised, then we should be good to go!","title":"Checking for a Successful Install"},{"location":"guides/tensorflowlite/","text":"Convert and Infer Models on TensorFlowLite Everything here can be followed along in Google Colab! Installation and Setup We need to first install TensorFlowTTS, which is a fork of the original repository developed by w11wo . ! git clone - q https : // github . com / w11wo / TensorFlowTTS . git ! cd TensorFlowTTS && pip install - q . > / dev / null Then, we'll need to downgrade TensorFlow to version 2.3.1 . This is the version that I found to be working well all the way to mobile deployment. You could probably get away with the newer versions, nonetheless. ! pip install - q tensorflow - gpu == 2.3.1 We also need to downgrade NumPy to the right version for this TensorFlow version. ! pip install - q numpy == 1.20.3 IMPORTANT : after re-installing TensorFlow and NumPy, be sure to restart your Colab Runtime! Log into HuggingFace Hub If you have previously saved your model weights in HuggingFace Hub, it'll be immensely easier to load them back. Private Hub models can also be loaded, so long as you first log in to the Hub, which we'll do via notebook_login . from huggingface_hub import notebook_login notebook_login () Token is valid. Your token has been saved in your configured git credential helpers (store). Your token has been saved to /root/.huggingface/token Login successful To load a private Hub model, you just have to specify use_auth_token=True later. Convert Models Typically, converting to a TensorFlowLite model involves these steps: Loading the model weights Getting the concrete function of the model's forward pass Setting up the converter Specifying optimizations Convert and save TFLite model import tensorflow as tf from tensorflow_tts.inference import TFAutoModel /usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.3.0 (nightly versions are not supported). The versions of TensorFlow you are currently using is 2.3.1 and is not supported. Some things might work, some things might not. If you were to encounter a bug, do not file an issue. If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. You can find the compatibility matrix in TensorFlow Addon's readme: https://github.com/tensorflow/addons UserWarning, [nltk_data] Downloading package averaged_perceptron_tagger to [nltk_data] /root/nltk_data... [nltk_data] Unzipping taggers/averaged_perceptron_tagger.zip. [nltk_data] Downloading package cmudict to /root/nltk_data... [nltk_data] Unzipping corpora/cmudict.zip. [nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data] Unzipping tokenizers/punkt.zip. def convert_text2mel_tflite ( model_path : str , save_name : str , use_auth_token : bool = False ) -> float : \"\"\"Convert text2mel model to TFLite. Args: model_path (str): Pretrained model checkpoint in HuggingFace Hub. save_name (str): TFLite file savename. use_auth_token (bool, optional): Use HF Hub Token. Defaults to False. Returns: float: Model size in Megabytes. \"\"\" # load pretrained model model = TFAutoModel . from_pretrained ( model_path , enable_tflite_convertible = True , use_auth_token = use_auth_token ) # setup model concrete function concrete_function = model . inference_tflite . get_concrete_function () converter = tf . lite . TFLiteConverter . from_concrete_functions ([ concrete_function ]) # specify optimizations converter . optimizations = [ tf . lite . Optimize . DEFAULT ] converter . target_spec . supported_ops = [ tf . lite . OpsSet . TFLITE_BUILTINS , # quantize tf . lite . OpsSet . SELECT_TF_OPS , ] # convert and save model to TensorFlowLite tflite_model = converter . convert () with open ( save_name , \"wb\" ) as f : f . write ( tflite_model ) size = len ( tflite_model ) / 1024 / 1024.0 return size def convert_vocoder_tflite ( model_path : str , save_name : str , use_auth_token : bool = False ) -> float : \"\"\"Convert vocoder model to TFLite. Args: model_path (str): Pretrained model checkpoint in HuggingFace Hub. save_name (str): TFLite file savename. use_auth_token (bool, optional): Use HF Hub Token. Defaults to False. Returns: float: Model size in Megabytes. \"\"\" # load pretrained model model = TFAutoModel . from_pretrained ( model_path , use_auth_token = use_auth_token ) # setup model concrete function concrete_function = model . inference_tflite . get_concrete_function () converter = tf . lite . TFLiteConverter . from_concrete_functions ([ concrete_function ]) # specify optimizations converter . optimizations = [ tf . lite . Optimize . DEFAULT ] converter . target_spec . supported_ops = [ tf . lite . OpsSet . SELECT_TF_OPS ] converter . target_spec . supported_types = [ tf . float16 ] # fp16 ops # convert and save model to TensorFlowLite tflite_model = converter . convert () with open ( save_name , \"wb\" ) as f : f . write ( tflite_model ) size = len ( tflite_model ) / 1024 / 1024.0 return size text2mel = convert_text2mel_tflite ( model_path = \"bookbot/lightspeech-mfa-id-v3\" , save_name = \"lightspeech_quant.tflite\" , use_auth_token = True , ) vocoder = convert_vocoder_tflite ( model_path = \"bookbot/mb-melgan-hifi-postnets-id-v10\" , save_name = \"mbmelgan.tflite\" , use_auth_token = True , ) /usr/local/lib/python3.7/dist-packages/huggingface_hub/file_download.py:595: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download` FutureWarning, Downloading: 0%| | 0.00/19.5M [00:00<?, ?B/s] Downloading: 0%| | 0.00/1.89k [00:00<?, ?B/s] Downloading: 0%| | 0.00/10.2M [00:00<?, ?B/s] Downloading: 0%| | 0.00/2.53k [00:00<?, ?B/s] print ( f \"Text2mel: { text2mel } MBs \\n Vocoder: { vocoder } MBs\" ) Text2mel: 4.323883056640625 MBs Vocoder: 5.0258941650390625 MBs Conversion Script We also provide an script version of the conversion steps above, which can be found here . To use it, you just have to specify the arguments through the command line. An example is as follows: !python TensorFlowTTS/examples/tensorflowlite/convert_tflite.py \\ --text2mel_path = \"bookbot/lightspeech-mfa-id-v3\" \\ --text2mel_savename = \"lightspeech_quant.tflite\" \\ --vocoder_path = \"bookbot/mb-melgan-hifi-postnets-id-v10\" \\ --vocoder_savename = \"mbmelgan.tflite\" \\ --use_auth_token Inference With the converted TFLite models, we can then perform inference on TFLite Runtime. Here, we'll only be presenting a way to perform inference for LightSpeech + Multi-band MelGAN. Other models might differ (e.g. FastSpeech2 has different model outputs compared to LightSpeech). However, adapting the inference code to other models should be fairly doable given that you know the outputs of each model. Tokenization To apply tokenization to our raw text, we can simply load the processor (tokenizer) we used during training. Again, if it's stored to the HuggingFace Hub, you can conveniently load it from it during inference. You could optionally specify if it's private, and load it the same way as you would load a private Hub model. from tensorflow_tts.inference import AutoProcessor processor = AutoProcessor . from_pretrained ( \"bookbot/lightspeech-mfa-id-v3\" , use_auth_token = True ) processor . mode = \"eval\" # change processor from train to eval mode Downloading: 0%| | 0.00/1.04k [00:00<?, ?B/s] With the processor, we can then tokenize any input text and convert them to its correponding input ids (list of token IDs). from typing import List , Tuple def tokenize ( text : str , processor : AutoProcessor ) -> List [ int ]: \"\"\"Tokenize text to input ids. Args: text (str): Input text to tokenize. processor (AutoProcessor): Processor for tokenization. Returns: List[int]: List of input (token) ids. \"\"\" return processor . text_to_sequence ( text ) text = \"Halo, bagaimana kabar mu?\" input_ids = tokenize ( text , processor ) input_ids [8, 1, 12, 15, 32, 2, 1, 7, 1, 9, 13, 1, 14, 1, 11, 1, 2, 1, 17, 13, 20, 34] Prepare LightSpeech Input LightSpeech expects 5 inputs for inference, namely: Input IDs Speaker ID Speed Ratio Pitch Ratio Energy Ratio Speaker ID is only relevant for a multi-speaker model, with each index (starting from 0) corresponding to different speaker embeddings for the text2mel model to use. You can also alter other options such as speed, which serves like a duration multiplier (i.e. speed ratio of 2 is half the normal speed). You could also alter the pitch and energy in a similar way. For simplicity, I'll just be parameterizing the speaker ID. def prepare_input ( input_ids : List [ str ], speaker : int ) -> Tuple [ tf . Tensor , tf . Tensor , tf . Tensor , tf . Tensor , tf . Tensor ]: \"\"\"Prepares input for LightSpeech TFLite inference. Args: input_ids (List[str]): Phoneme input ids according to processor. speaker (int): Speaker ID. Returns: Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]: Tuple of tensors consisting of: 1. Input IDs 2. Speaker ID 3. Speed Ratio 4. Pitch Ratio 5. Energy Ratio \"\"\" input_ids = tf . expand_dims ( tf . convert_to_tensor ( input_ids , dtype = tf . int32 ), 0 ) return ( input_ids , tf . convert_to_tensor ([ speaker ], tf . int32 ), tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), ) Inference To perform inference on a TensorFlowLite Runtime, the general flow is as follows: Load model weights to TFLite Interpreter Resize interpreter input tensors according to actual input Allocate tensors according to Interpreter's inputs Set the actual tensor values as inputs to the Interpreter Invoke interpreter Return output tensors We can follow the steps outlined above and create an inference function for LightSpeech and MB-MelGAN. def ls_infer ( input_ids : List [ str ], speaker : int , lightspeech_path : str ) -> Tuple [ tf . Tensor , tf . Tensor , tf . Tensor ]: \"\"\"Performs LightSpeech inference. Args: input_ids (List[str]): Phoneme input ids according to processor. speaker (int): Speaker ID. lightspeech_path (str): Path to LightSpeech weights. Returns: Tuple[tf.Tensor, tf.Tensor, tf.Tensor]: Tuple of tensors consisting of: 1. Mel-spectrogram output 2. Durations array \"\"\" # load model to Interpreter lightspeech = tf . lite . Interpreter ( model_path = lightspeech_path ) input_details = lightspeech . get_input_details () output_details = lightspeech . get_output_details () # resize input tensors according to actual shape lightspeech . resize_tensor_input ( input_details [ 0 ][ \"index\" ], [ 1 , len ( input_ids )]) lightspeech . resize_tensor_input ( input_details [ 1 ][ \"index\" ], [ 1 ]) lightspeech . resize_tensor_input ( input_details [ 2 ][ \"index\" ], [ 1 ]) lightspeech . resize_tensor_input ( input_details [ 3 ][ \"index\" ], [ 1 ]) lightspeech . resize_tensor_input ( input_details [ 4 ][ \"index\" ], [ 1 ]) # allocate tensors lightspeech . allocate_tensors () input_data = prepare_input ( input_ids , speaker ) # set input tensors for i , detail in enumerate ( input_details ): lightspeech . set_tensor ( detail [ \"index\" ], input_data [ i ]) # invoke interpreter lightspeech . invoke () # return outputs return ( lightspeech . get_tensor ( output_details [ 0 ][ \"index\" ]), lightspeech . get_tensor ( output_details [ 1 ][ \"index\" ]), ) def melgan_infer ( melspectrogram : tf . Tensor , mb_melgan_path : str ) -> tf . Tensor : \"\"\"Performs MB-MelGAN inference. Args: melspectrogram (tf.Tensor): Mel-spectrogram to synthesize. mb_melgan_path (str): Path to MB-MelGAN weights. Returns: tf.Tensor: Synthesized audio output tensor. \"\"\" # load model to Interpreter mb_melgan = tf . lite . Interpreter ( model_path = mb_melgan_path ) input_details = mb_melgan . get_input_details () output_details = mb_melgan . get_output_details () # resize input tensors according to actual shape mb_melgan . resize_tensor_input ( input_details [ 0 ][ \"index\" ], [ 1 , melspectrogram . shape [ 1 ], melspectrogram . shape [ 2 ]], strict = True , ) # allocate tensors mb_melgan . allocate_tensors () # set input tensors mb_melgan . set_tensor ( input_details [ 0 ][ \"index\" ], melspectrogram ) # invoke interpreter mb_melgan . invoke () # return output return mb_melgan . get_tensor ( output_details [ 0 ][ \"index\" ]) Finally, we can perform inference with the model weights which we have converted earlier and run an end-to-end inference. mel_output_tflite , * _ = ls_infer ( input_ids , speaker = 0 , lightspeech_path = \"lightspeech_quant.tflite\" ) audio_tflite = melgan_infer ( mel_output_tflite , mb_melgan_path = \"mbmelgan.tflite\" )[ 0 , :, 0 ] To listen to the synthesized output via Jupyter, we can directly pass the outputs to IPython's Audio widget, while specifying the sample rate of the audio. from IPython.display import Audio Audio ( data = audio_tflite , rate = 32000 ) Your browser does not support the audio element. Alternatively, we can write the output audio tensors to file. import soundfile as sf sf . write ( \"./audio.wav\" , audio_tflite , 32000 , \"PCM_16\" ) from IPython.display import Audio Audio ( \"audio.wav\" ) Your browser does not support the audio element.","title":"Convert and Infer Models on TensorFlowLite"},{"location":"guides/tensorflowlite/#convert-and-infer-models-on-tensorflowlite","text":"Everything here can be followed along in Google Colab!","title":"Convert and Infer Models on TensorFlowLite"},{"location":"guides/tensorflowlite/#installation-and-setup","text":"We need to first install TensorFlowTTS, which is a fork of the original repository developed by w11wo . ! git clone - q https : // github . com / w11wo / TensorFlowTTS . git ! cd TensorFlowTTS && pip install - q . > / dev / null Then, we'll need to downgrade TensorFlow to version 2.3.1 . This is the version that I found to be working well all the way to mobile deployment. You could probably get away with the newer versions, nonetheless. ! pip install - q tensorflow - gpu == 2.3.1 We also need to downgrade NumPy to the right version for this TensorFlow version. ! pip install - q numpy == 1.20.3 IMPORTANT : after re-installing TensorFlow and NumPy, be sure to restart your Colab Runtime!","title":"Installation and Setup"},{"location":"guides/tensorflowlite/#log-into-huggingface-hub","text":"If you have previously saved your model weights in HuggingFace Hub, it'll be immensely easier to load them back. Private Hub models can also be loaded, so long as you first log in to the Hub, which we'll do via notebook_login . from huggingface_hub import notebook_login notebook_login () Token is valid. Your token has been saved in your configured git credential helpers (store). Your token has been saved to /root/.huggingface/token Login successful To load a private Hub model, you just have to specify use_auth_token=True later.","title":"Log into HuggingFace Hub"},{"location":"guides/tensorflowlite/#convert-models","text":"Typically, converting to a TensorFlowLite model involves these steps: Loading the model weights Getting the concrete function of the model's forward pass Setting up the converter Specifying optimizations Convert and save TFLite model import tensorflow as tf from tensorflow_tts.inference import TFAutoModel /usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.3.0 (nightly versions are not supported). The versions of TensorFlow you are currently using is 2.3.1 and is not supported. Some things might work, some things might not. If you were to encounter a bug, do not file an issue. If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. You can find the compatibility matrix in TensorFlow Addon's readme: https://github.com/tensorflow/addons UserWarning, [nltk_data] Downloading package averaged_perceptron_tagger to [nltk_data] /root/nltk_data... [nltk_data] Unzipping taggers/averaged_perceptron_tagger.zip. [nltk_data] Downloading package cmudict to /root/nltk_data... [nltk_data] Unzipping corpora/cmudict.zip. [nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data] Unzipping tokenizers/punkt.zip. def convert_text2mel_tflite ( model_path : str , save_name : str , use_auth_token : bool = False ) -> float : \"\"\"Convert text2mel model to TFLite. Args: model_path (str): Pretrained model checkpoint in HuggingFace Hub. save_name (str): TFLite file savename. use_auth_token (bool, optional): Use HF Hub Token. Defaults to False. Returns: float: Model size in Megabytes. \"\"\" # load pretrained model model = TFAutoModel . from_pretrained ( model_path , enable_tflite_convertible = True , use_auth_token = use_auth_token ) # setup model concrete function concrete_function = model . inference_tflite . get_concrete_function () converter = tf . lite . TFLiteConverter . from_concrete_functions ([ concrete_function ]) # specify optimizations converter . optimizations = [ tf . lite . Optimize . DEFAULT ] converter . target_spec . supported_ops = [ tf . lite . OpsSet . TFLITE_BUILTINS , # quantize tf . lite . OpsSet . SELECT_TF_OPS , ] # convert and save model to TensorFlowLite tflite_model = converter . convert () with open ( save_name , \"wb\" ) as f : f . write ( tflite_model ) size = len ( tflite_model ) / 1024 / 1024.0 return size def convert_vocoder_tflite ( model_path : str , save_name : str , use_auth_token : bool = False ) -> float : \"\"\"Convert vocoder model to TFLite. Args: model_path (str): Pretrained model checkpoint in HuggingFace Hub. save_name (str): TFLite file savename. use_auth_token (bool, optional): Use HF Hub Token. Defaults to False. Returns: float: Model size in Megabytes. \"\"\" # load pretrained model model = TFAutoModel . from_pretrained ( model_path , use_auth_token = use_auth_token ) # setup model concrete function concrete_function = model . inference_tflite . get_concrete_function () converter = tf . lite . TFLiteConverter . from_concrete_functions ([ concrete_function ]) # specify optimizations converter . optimizations = [ tf . lite . Optimize . DEFAULT ] converter . target_spec . supported_ops = [ tf . lite . OpsSet . SELECT_TF_OPS ] converter . target_spec . supported_types = [ tf . float16 ] # fp16 ops # convert and save model to TensorFlowLite tflite_model = converter . convert () with open ( save_name , \"wb\" ) as f : f . write ( tflite_model ) size = len ( tflite_model ) / 1024 / 1024.0 return size text2mel = convert_text2mel_tflite ( model_path = \"bookbot/lightspeech-mfa-id-v3\" , save_name = \"lightspeech_quant.tflite\" , use_auth_token = True , ) vocoder = convert_vocoder_tflite ( model_path = \"bookbot/mb-melgan-hifi-postnets-id-v10\" , save_name = \"mbmelgan.tflite\" , use_auth_token = True , ) /usr/local/lib/python3.7/dist-packages/huggingface_hub/file_download.py:595: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download` FutureWarning, Downloading: 0%| | 0.00/19.5M [00:00<?, ?B/s] Downloading: 0%| | 0.00/1.89k [00:00<?, ?B/s] Downloading: 0%| | 0.00/10.2M [00:00<?, ?B/s] Downloading: 0%| | 0.00/2.53k [00:00<?, ?B/s] print ( f \"Text2mel: { text2mel } MBs \\n Vocoder: { vocoder } MBs\" ) Text2mel: 4.323883056640625 MBs Vocoder: 5.0258941650390625 MBs","title":"Convert Models"},{"location":"guides/tensorflowlite/#conversion-script","text":"We also provide an script version of the conversion steps above, which can be found here . To use it, you just have to specify the arguments through the command line. An example is as follows: !python TensorFlowTTS/examples/tensorflowlite/convert_tflite.py \\ --text2mel_path = \"bookbot/lightspeech-mfa-id-v3\" \\ --text2mel_savename = \"lightspeech_quant.tflite\" \\ --vocoder_path = \"bookbot/mb-melgan-hifi-postnets-id-v10\" \\ --vocoder_savename = \"mbmelgan.tflite\" \\ --use_auth_token","title":"Conversion Script"},{"location":"guides/tensorflowlite/#inference","text":"With the converted TFLite models, we can then perform inference on TFLite Runtime. Here, we'll only be presenting a way to perform inference for LightSpeech + Multi-band MelGAN. Other models might differ (e.g. FastSpeech2 has different model outputs compared to LightSpeech). However, adapting the inference code to other models should be fairly doable given that you know the outputs of each model.","title":"Inference"},{"location":"guides/tensorflowlite/#tokenization","text":"To apply tokenization to our raw text, we can simply load the processor (tokenizer) we used during training. Again, if it's stored to the HuggingFace Hub, you can conveniently load it from it during inference. You could optionally specify if it's private, and load it the same way as you would load a private Hub model. from tensorflow_tts.inference import AutoProcessor processor = AutoProcessor . from_pretrained ( \"bookbot/lightspeech-mfa-id-v3\" , use_auth_token = True ) processor . mode = \"eval\" # change processor from train to eval mode Downloading: 0%| | 0.00/1.04k [00:00<?, ?B/s] With the processor, we can then tokenize any input text and convert them to its correponding input ids (list of token IDs). from typing import List , Tuple def tokenize ( text : str , processor : AutoProcessor ) -> List [ int ]: \"\"\"Tokenize text to input ids. Args: text (str): Input text to tokenize. processor (AutoProcessor): Processor for tokenization. Returns: List[int]: List of input (token) ids. \"\"\" return processor . text_to_sequence ( text ) text = \"Halo, bagaimana kabar mu?\" input_ids = tokenize ( text , processor ) input_ids [8, 1, 12, 15, 32, 2, 1, 7, 1, 9, 13, 1, 14, 1, 11, 1, 2, 1, 17, 13, 20, 34]","title":"Tokenization"},{"location":"guides/tensorflowlite/#prepare-lightspeech-input","text":"LightSpeech expects 5 inputs for inference, namely: Input IDs Speaker ID Speed Ratio Pitch Ratio Energy Ratio Speaker ID is only relevant for a multi-speaker model, with each index (starting from 0) corresponding to different speaker embeddings for the text2mel model to use. You can also alter other options such as speed, which serves like a duration multiplier (i.e. speed ratio of 2 is half the normal speed). You could also alter the pitch and energy in a similar way. For simplicity, I'll just be parameterizing the speaker ID. def prepare_input ( input_ids : List [ str ], speaker : int ) -> Tuple [ tf . Tensor , tf . Tensor , tf . Tensor , tf . Tensor , tf . Tensor ]: \"\"\"Prepares input for LightSpeech TFLite inference. Args: input_ids (List[str]): Phoneme input ids according to processor. speaker (int): Speaker ID. Returns: Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]: Tuple of tensors consisting of: 1. Input IDs 2. Speaker ID 3. Speed Ratio 4. Pitch Ratio 5. Energy Ratio \"\"\" input_ids = tf . expand_dims ( tf . convert_to_tensor ( input_ids , dtype = tf . int32 ), 0 ) return ( input_ids , tf . convert_to_tensor ([ speaker ], tf . int32 ), tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), )","title":"Prepare LightSpeech Input"},{"location":"guides/tensorflowlite/#inference_1","text":"To perform inference on a TensorFlowLite Runtime, the general flow is as follows: Load model weights to TFLite Interpreter Resize interpreter input tensors according to actual input Allocate tensors according to Interpreter's inputs Set the actual tensor values as inputs to the Interpreter Invoke interpreter Return output tensors We can follow the steps outlined above and create an inference function for LightSpeech and MB-MelGAN. def ls_infer ( input_ids : List [ str ], speaker : int , lightspeech_path : str ) -> Tuple [ tf . Tensor , tf . Tensor , tf . Tensor ]: \"\"\"Performs LightSpeech inference. Args: input_ids (List[str]): Phoneme input ids according to processor. speaker (int): Speaker ID. lightspeech_path (str): Path to LightSpeech weights. Returns: Tuple[tf.Tensor, tf.Tensor, tf.Tensor]: Tuple of tensors consisting of: 1. Mel-spectrogram output 2. Durations array \"\"\" # load model to Interpreter lightspeech = tf . lite . Interpreter ( model_path = lightspeech_path ) input_details = lightspeech . get_input_details () output_details = lightspeech . get_output_details () # resize input tensors according to actual shape lightspeech . resize_tensor_input ( input_details [ 0 ][ \"index\" ], [ 1 , len ( input_ids )]) lightspeech . resize_tensor_input ( input_details [ 1 ][ \"index\" ], [ 1 ]) lightspeech . resize_tensor_input ( input_details [ 2 ][ \"index\" ], [ 1 ]) lightspeech . resize_tensor_input ( input_details [ 3 ][ \"index\" ], [ 1 ]) lightspeech . resize_tensor_input ( input_details [ 4 ][ \"index\" ], [ 1 ]) # allocate tensors lightspeech . allocate_tensors () input_data = prepare_input ( input_ids , speaker ) # set input tensors for i , detail in enumerate ( input_details ): lightspeech . set_tensor ( detail [ \"index\" ], input_data [ i ]) # invoke interpreter lightspeech . invoke () # return outputs return ( lightspeech . get_tensor ( output_details [ 0 ][ \"index\" ]), lightspeech . get_tensor ( output_details [ 1 ][ \"index\" ]), ) def melgan_infer ( melspectrogram : tf . Tensor , mb_melgan_path : str ) -> tf . Tensor : \"\"\"Performs MB-MelGAN inference. Args: melspectrogram (tf.Tensor): Mel-spectrogram to synthesize. mb_melgan_path (str): Path to MB-MelGAN weights. Returns: tf.Tensor: Synthesized audio output tensor. \"\"\" # load model to Interpreter mb_melgan = tf . lite . Interpreter ( model_path = mb_melgan_path ) input_details = mb_melgan . get_input_details () output_details = mb_melgan . get_output_details () # resize input tensors according to actual shape mb_melgan . resize_tensor_input ( input_details [ 0 ][ \"index\" ], [ 1 , melspectrogram . shape [ 1 ], melspectrogram . shape [ 2 ]], strict = True , ) # allocate tensors mb_melgan . allocate_tensors () # set input tensors mb_melgan . set_tensor ( input_details [ 0 ][ \"index\" ], melspectrogram ) # invoke interpreter mb_melgan . invoke () # return output return mb_melgan . get_tensor ( output_details [ 0 ][ \"index\" ]) Finally, we can perform inference with the model weights which we have converted earlier and run an end-to-end inference. mel_output_tflite , * _ = ls_infer ( input_ids , speaker = 0 , lightspeech_path = \"lightspeech_quant.tflite\" ) audio_tflite = melgan_infer ( mel_output_tflite , mb_melgan_path = \"mbmelgan.tflite\" )[ 0 , :, 0 ] To listen to the synthesized output via Jupyter, we can directly pass the outputs to IPython's Audio widget, while specifying the sample rate of the audio. from IPython.display import Audio Audio ( data = audio_tflite , rate = 32000 ) Your browser does not support the audio element. Alternatively, we can write the output audio tensors to file. import soundfile as sf sf . write ( \"./audio.wav\" , audio_tflite , 32000 , \"PCM_16\" ) from IPython.display import Audio Audio ( \"audio.wav\" ) Your browser does not support the audio element.","title":"Inference"},{"location":"guides/lightspeech-mbmelgan/dataset/","text":"Dataset Folder Structure The structure of the training files is fairly simple and straightforward: {YOUR_DATASET}/ \u251c\u2500\u2500 {SPEAKER-1}/ \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-0}.lab \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-0}.wav \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-1}.lab \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-1}.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 {SPEAKER-2}/ \u2502 \u251c\u2500\u2500 {SPEAKER-2}_{UTTERANCE-0}.lab \u2502 \u251c\u2500\u2500 {SPEAKER-2}_{UTTERANCE-0}.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 ... A few key things to note here: Each speaker has its own subfolder within the root dataset folder. The filenames in the speaker subfolders follow the convention of {SPEAKER-#}_{UTTERANCE-#} . It is important that they are delimited by an underscore ( _ ), so make sure that there is no _ within the speaker name and within the utterance ID. Use dashes - instead within them instead. Audios are in wav format and transcripts are of lab format (same content as you expect from a txt file; nothing fancy about it). The reason we use lab is simply to facilitate Montreal Forced Aligner training later. Example In the root directory en-bookbot , there are three speakers: en-AU-Zak , en-UK-Thalia , and en-US-Madison . The struture of the files are as follows: en-bookbot/ \u251c\u2500\u2500 en-AU-Zak/ \u2502 \u251c\u2500\u2500 en-AU-Zak_0.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_0.wav \u2502 \u251c\u2500\u2500 en-AU-Zak_1.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_1.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-UK-Thalia/ \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.lab \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.wav \u2502 \u251c\u2500\u2500 ... \u2514\u2500\u2500 en-US-Madison/ \u251c\u2500\u2500 en-US-Madison_0.lab \u251c\u2500\u2500 en-US-Madison_0.wav \u251c\u2500\u2500 ... Lexicon Another required component for training a Montreal Forced Aligner is a lexicon file (usually named lexicon.txt ). A lexicon simply maps words (graphemes) to phonemes, i.e. a pronunciation dictionary. Later, these phonemes will be aligned to segments of audio that they correpond to, and its duration will be learned by the forced aligner. There are many available lexicons out there, such as ones provided by Montreal Forced Aligner and Open Dict Data . You can either find other pre-existing lexicons, or create your own. Otherwise, another option would to treat each grapheme character as proxy phonemes as done by Meyer et al. (2022) where a lexicon is unavailable in certain languages: Two languages (ewe and yor) were aligned via forced alignment from scratch. Using only the found audio and transcripts (i.e., without a pre-trained acoustic model), an acoustic model was trained and the data aligned with the Montreal Forced Aligner. Graphemes were used as a proxy for phonemes in place of G2P data. In any case, the lexicon file should consist of tab -delimited word-phoneme pairs, which looks like the following: what w \u02c8\u028c t is \u02c8\u026a z biology b a\u026a \u02c8\u0251 l \u0259 d\u0361\u0292 i ? ? the \u00f0 \u0259 study s t \u02c8\u028c d i of \u0259 v living l \u02c8\u026a v \u026a \u014b things \u03b8 \u02c8\u026a \u014b z . . from f \u0279 \u02c8\u028c m ... Another example would be: : : , , . . ! ! ? ? ; ; a a b b e c t\u0283 e d d e abad a b a d abadi a b a d i abadiah a b a d i a h abadikan a b a d i k a n ... There are a few key things to note as well: The lexicon should cover all words in the audio corpus -- no out-of-vocabulary words should be present during the training of the aligner later. Otherwise, this will result in unknown tokens and will likely disrupt the training and duration parsing processes. Include all the punctuations you would want to have in the model later. For instance, I would usually keep . , : ; ? ! because they might imply different pause durations and/or intonations. Every other punctuations not in the lexicon will be stripped during the alignment process. Individual phonemes should be separated with whitespaces, e.g. a is its own phoneme unit, and t\u0283 is also considered as another single phoneme unit despite having 2 characters. Structuring your dataset, preprocessing, and lexicon preparation are arguably the most complicated part of training these kinds of models. But once we're over this step, everything else should be quite easy to follow.","title":"Dataset"},{"location":"guides/lightspeech-mbmelgan/dataset/#dataset","text":"","title":"Dataset"},{"location":"guides/lightspeech-mbmelgan/dataset/#folder-structure","text":"The structure of the training files is fairly simple and straightforward: {YOUR_DATASET}/ \u251c\u2500\u2500 {SPEAKER-1}/ \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-0}.lab \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-0}.wav \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-1}.lab \u2502 \u251c\u2500\u2500 {SPEAKER-1}_{UTTERANCE-1}.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 {SPEAKER-2}/ \u2502 \u251c\u2500\u2500 {SPEAKER-2}_{UTTERANCE-0}.lab \u2502 \u251c\u2500\u2500 {SPEAKER-2}_{UTTERANCE-0}.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 ... A few key things to note here: Each speaker has its own subfolder within the root dataset folder. The filenames in the speaker subfolders follow the convention of {SPEAKER-#}_{UTTERANCE-#} . It is important that they are delimited by an underscore ( _ ), so make sure that there is no _ within the speaker name and within the utterance ID. Use dashes - instead within them instead. Audios are in wav format and transcripts are of lab format (same content as you expect from a txt file; nothing fancy about it). The reason we use lab is simply to facilitate Montreal Forced Aligner training later.","title":"Folder Structure"},{"location":"guides/lightspeech-mbmelgan/dataset/#example","text":"In the root directory en-bookbot , there are three speakers: en-AU-Zak , en-UK-Thalia , and en-US-Madison . The struture of the files are as follows: en-bookbot/ \u251c\u2500\u2500 en-AU-Zak/ \u2502 \u251c\u2500\u2500 en-AU-Zak_0.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_0.wav \u2502 \u251c\u2500\u2500 en-AU-Zak_1.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_1.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-UK-Thalia/ \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.lab \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.wav \u2502 \u251c\u2500\u2500 ... \u2514\u2500\u2500 en-US-Madison/ \u251c\u2500\u2500 en-US-Madison_0.lab \u251c\u2500\u2500 en-US-Madison_0.wav \u251c\u2500\u2500 ...","title":"Example"},{"location":"guides/lightspeech-mbmelgan/dataset/#lexicon","text":"Another required component for training a Montreal Forced Aligner is a lexicon file (usually named lexicon.txt ). A lexicon simply maps words (graphemes) to phonemes, i.e. a pronunciation dictionary. Later, these phonemes will be aligned to segments of audio that they correpond to, and its duration will be learned by the forced aligner. There are many available lexicons out there, such as ones provided by Montreal Forced Aligner and Open Dict Data . You can either find other pre-existing lexicons, or create your own. Otherwise, another option would to treat each grapheme character as proxy phonemes as done by Meyer et al. (2022) where a lexicon is unavailable in certain languages: Two languages (ewe and yor) were aligned via forced alignment from scratch. Using only the found audio and transcripts (i.e., without a pre-trained acoustic model), an acoustic model was trained and the data aligned with the Montreal Forced Aligner. Graphemes were used as a proxy for phonemes in place of G2P data. In any case, the lexicon file should consist of tab -delimited word-phoneme pairs, which looks like the following: what w \u02c8\u028c t is \u02c8\u026a z biology b a\u026a \u02c8\u0251 l \u0259 d\u0361\u0292 i ? ? the \u00f0 \u0259 study s t \u02c8\u028c d i of \u0259 v living l \u02c8\u026a v \u026a \u014b things \u03b8 \u02c8\u026a \u014b z . . from f \u0279 \u02c8\u028c m ... Another example would be: : : , , . . ! ! ? ? ; ; a a b b e c t\u0283 e d d e abad a b a d abadi a b a d i abadiah a b a d i a h abadikan a b a d i k a n ... There are a few key things to note as well: The lexicon should cover all words in the audio corpus -- no out-of-vocabulary words should be present during the training of the aligner later. Otherwise, this will result in unknown tokens and will likely disrupt the training and duration parsing processes. Include all the punctuations you would want to have in the model later. For instance, I would usually keep . , : ; ? ! because they might imply different pause durations and/or intonations. Every other punctuations not in the lexicon will be stripped during the alignment process. Individual phonemes should be separated with whitespaces, e.g. a is its own phoneme unit, and t\u0283 is also considered as another single phoneme unit despite having 2 characters. Structuring your dataset, preprocessing, and lexicon preparation are arguably the most complicated part of training these kinds of models. But once we're over this step, everything else should be quite easy to follow.","title":"Lexicon"},{"location":"guides/lightspeech-mbmelgan/duration-extraction/","text":"Duration Extraction Forced Alignment Recall that we had prepared a lexicon file that maps graphemes (words) into phonemes. These phonemes will then be aligned to segments of the corresponding audio, whose duration we will then use to feed into models like LightSpeech. The end result of audio alignment would look something like the following: Notice that chunks of audio are aligned to each word and its subsequent phonemes. By the end of the training, TextGrid files will be generated, containing the alignment results. They look like the following: File type = \"ooTextFile\" Object class = \"TextGrid\" xmin = 0 xmax = 7.398163265306122 tiers? <exists> size = 2 item []: item [1]: class = \"IntervalTier\" name = \"words\" xmin = 0 xmax = 7.398163265306122 intervals: size = 34 intervals [1]: xmin = 0 xmax = 0.04 text = \"\" intervals [2]: xmin = 0.04 xmax = 0.26 text = \"he\" intervals [3]: xmin = 0.26 xmax = 0.44 text = \"is\" intervals [4]: xmin = 0.44 xmax = 0.48 text = \"\" intervals [5]: xmin = 0.48 xmax = 0.91 text = \"going\" intervals [6]: xmin = 0.91 xmax = 0.94 text = \"\" intervals [7]: xmin = 0.94 xmax = 1.05 text = \"to\" Montreal Forced Aligner (MFA) is an algorithm and a library that will help train these kinds of acoustic models. The TextGrid files will then be parsed and the durations of phonemes will be obtained. It is these phoneme durations that will then be learned by LightSpeech via L2 loss. External Aligner-free Models Recent development is aiming to completely remove the usage of external-aligner tools like MFA, and for good reasons. It will not only simplify the training pipeline by learning the token duration on-the-fly, but will also improve the speech quality and speed up alignment convergence, especially on longer sequence of text during inference. Most of the latest models that does its own alignment learning normally use a combination of Connectionist Temporal Classification and Monotonic Alignment Search -- for instance Glow-TTS , VITS , and JETS , among many others. As far as I know, these types of models have yet to be integrated into TensorFlowTTS, although there is an existing branch that attempts to implement AlignTTS . Installing Montreal Forced Aligner To begin, we can start by first installing Montreal Forced Aligner. It is much easier to install it via Conda Forge. In the same Conda environment, you can run these commands conda config --add channels conda-forge conda install montreal-forced-aligner If you're not using Conda (e.g. upgrading from non-Conda version, or installing from Source), you can follow the official guide here . To confirm that the installation is successful, you can run mfa version which will return the version you've installed (in my case, it's 2.0.5 ). Training an MFA Aligner With MFA installed, training an aligner model is as simple as running the command mfa train { YOUR_DATASET } { LEXICON } { OUTPUT_ACOUSTIC_MODEL } { TEXTGRID_OUTPUT_DIR } --punctuation \"\" Example Using the same sample dataset and a lexicon file lexicon.txt , the command to run will be similar to the following mfa train ./en-bookbot ./lexicon.txt ./outputs/en_bookbot_acoustic_model.zip ./outputs/parsed --punctuation \"\" Parsing TextGrid Files We can ignore the acoustic model generated by MFA, although you can always keep this for other purposes. What's more important for us is the resultant TextGrid files located in your {TEXTGRID_OUTPUT_DIR} . Then, you could either use the original parser script or what I usually use is a modified version of the script. TxtGridParser import os from dataclasses import dataclass from tqdm.auto import tqdm import textgrid import numpy as np import re @dataclass class TxtGridParser : sample_rate : int multi_speaker : bool txt_grid_path : str hop_size : int output_durations_path : str dataset_path : str training_file : str = \"train.txt\" phones_mapper = { \"sil\" : \"SIL\" , \"\" : \"SIL\" } sil_phones = set ( phones_mapper . keys ()) punctuations = [ \";\" , \"?\" , \"!\" , \".\" , \",\" , \":\" ] def parse ( self ): speakers = ( [ i for i in os . listdir ( self . txt_grid_path ) if os . path . isdir ( os . path . join ( self . txt_grid_path , i )) ] if self . multi_speaker else [] ) data = [] if speakers : for speaker in speakers : file_list = os . listdir ( os . path . join ( self . txt_grid_path , speaker )) self . parse_text_grid ( file_list , data , speaker ) else : file_list = os . listdir ( self . txt_grid_path ) self . parse_text_grid ( file_list , data , \"\" ) with open ( os . path . join ( self . dataset_path , self . training_file ), \"w\" , encoding = \"utf-8\" ) as f : f . writelines ( data ) def parse_text_grid ( self , file_list : list , data : list , speaker_name : str ): for f_name in tqdm ( file_list ): text_grid = textgrid . TextGrid . fromFile ( os . path . join ( self . txt_grid_path , speaker_name , f_name ) ) pha = text_grid [ 1 ] durations = [] phs = [] flags = [] for iterator , interval in enumerate ( pha . intervals ): mark = interval . mark if mark in self . sil_phones or mark in self . punctuations : flags . append ( True ) else : flags . append ( False ) if mark in self . sil_phones : mark = self . phones_mapper [ mark ] dur = interval . duration () * ( self . sample_rate / self . hop_size ) durations . append ( round ( dur )) phs . append ( mark ) new_durations = [] for idx , ( flag , dur ) in enumerate ( zip ( flags , durations )): if len ( new_durations ) == 0 or ( flag and flags [ idx - 1 ] == False ): new_durations . append ( dur ) elif flag : new_durations [ len ( new_durations ) - 1 ] += dur else : new_durations . append ( dur ) full_ph = \" \" . join ( phs ) new_ph = full_ph matches = re . finditer ( \"( ?SIL)* ?([,!\\?\\.;:] ?){1,} ?(SIL ?)*\" , full_ph ) for match in matches : substring = full_ph [ match . start () : match . end ()] new_ph = new_ph . replace ( substring , f \" { substring . replace ( 'SIL' , '' ) . strip ()[ 0 ] } \" , 1 ) . strip () assert new_ph . split () . __len__ () == new_durations . __len__ () # safety check base_name = f_name . split ( \".TextGrid\" )[ 0 ] np . save ( os . path . join ( self . output_durations_path , f \" { base_name } -durations.npy\" ), np . array ( new_durations ) . astype ( np . int32 ), allow_pickle = False , ) data . append ( f \" { speaker_name } / { base_name } | { new_ph } | { speaker_name } \\n \" ) Then to use the modified script above, you just have to specify the arguments to the parser. args = { \"dataset_path\" : \"./en-bookbot\" , \"txt_grid_path\" : \"./outputs/parsed\" , # (1) \"output_durations_path\" : \"./en-bookbot/durations\" , \"sample_rate\" : 44100 , # (2) \"hop_size\" : 512 , # (3) \"multi_speaker\" : True , # (4) \"training_file\" : \"train.txt\" } txt_grid_parser = TxtGridParser ( ** args ) txt_grid_parser . parse () Replace this with whatever your {TEXTGRID_OUTPUT_DIR} was. Set this to the desired sample rate of your text-to-speech model. Set this to the desired hop size of your text-to-speech model. Multi-speaker or not, you can keep this as True . With the duration files located in durations/ and train.txt , we can finally train our own text-to-speech model!","title":"Duration Extraction"},{"location":"guides/lightspeech-mbmelgan/duration-extraction/#duration-extraction","text":"","title":"Duration Extraction"},{"location":"guides/lightspeech-mbmelgan/duration-extraction/#forced-alignment","text":"Recall that we had prepared a lexicon file that maps graphemes (words) into phonemes. These phonemes will then be aligned to segments of the corresponding audio, whose duration we will then use to feed into models like LightSpeech. The end result of audio alignment would look something like the following: Notice that chunks of audio are aligned to each word and its subsequent phonemes. By the end of the training, TextGrid files will be generated, containing the alignment results. They look like the following: File type = \"ooTextFile\" Object class = \"TextGrid\" xmin = 0 xmax = 7.398163265306122 tiers? <exists> size = 2 item []: item [1]: class = \"IntervalTier\" name = \"words\" xmin = 0 xmax = 7.398163265306122 intervals: size = 34 intervals [1]: xmin = 0 xmax = 0.04 text = \"\" intervals [2]: xmin = 0.04 xmax = 0.26 text = \"he\" intervals [3]: xmin = 0.26 xmax = 0.44 text = \"is\" intervals [4]: xmin = 0.44 xmax = 0.48 text = \"\" intervals [5]: xmin = 0.48 xmax = 0.91 text = \"going\" intervals [6]: xmin = 0.91 xmax = 0.94 text = \"\" intervals [7]: xmin = 0.94 xmax = 1.05 text = \"to\" Montreal Forced Aligner (MFA) is an algorithm and a library that will help train these kinds of acoustic models. The TextGrid files will then be parsed and the durations of phonemes will be obtained. It is these phoneme durations that will then be learned by LightSpeech via L2 loss. External Aligner-free Models Recent development is aiming to completely remove the usage of external-aligner tools like MFA, and for good reasons. It will not only simplify the training pipeline by learning the token duration on-the-fly, but will also improve the speech quality and speed up alignment convergence, especially on longer sequence of text during inference. Most of the latest models that does its own alignment learning normally use a combination of Connectionist Temporal Classification and Monotonic Alignment Search -- for instance Glow-TTS , VITS , and JETS , among many others. As far as I know, these types of models have yet to be integrated into TensorFlowTTS, although there is an existing branch that attempts to implement AlignTTS .","title":"Forced Alignment"},{"location":"guides/lightspeech-mbmelgan/duration-extraction/#installing-montreal-forced-aligner","text":"To begin, we can start by first installing Montreal Forced Aligner. It is much easier to install it via Conda Forge. In the same Conda environment, you can run these commands conda config --add channels conda-forge conda install montreal-forced-aligner If you're not using Conda (e.g. upgrading from non-Conda version, or installing from Source), you can follow the official guide here . To confirm that the installation is successful, you can run mfa version which will return the version you've installed (in my case, it's 2.0.5 ).","title":"Installing Montreal Forced Aligner"},{"location":"guides/lightspeech-mbmelgan/duration-extraction/#training-an-mfa-aligner","text":"With MFA installed, training an aligner model is as simple as running the command mfa train { YOUR_DATASET } { LEXICON } { OUTPUT_ACOUSTIC_MODEL } { TEXTGRID_OUTPUT_DIR } --punctuation \"\"","title":"Training an MFA Aligner"},{"location":"guides/lightspeech-mbmelgan/duration-extraction/#example","text":"Using the same sample dataset and a lexicon file lexicon.txt , the command to run will be similar to the following mfa train ./en-bookbot ./lexicon.txt ./outputs/en_bookbot_acoustic_model.zip ./outputs/parsed --punctuation \"\"","title":"Example"},{"location":"guides/lightspeech-mbmelgan/duration-extraction/#parsing-textgrid-files","text":"We can ignore the acoustic model generated by MFA, although you can always keep this for other purposes. What's more important for us is the resultant TextGrid files located in your {TEXTGRID_OUTPUT_DIR} . Then, you could either use the original parser script or what I usually use is a modified version of the script. TxtGridParser import os from dataclasses import dataclass from tqdm.auto import tqdm import textgrid import numpy as np import re @dataclass class TxtGridParser : sample_rate : int multi_speaker : bool txt_grid_path : str hop_size : int output_durations_path : str dataset_path : str training_file : str = \"train.txt\" phones_mapper = { \"sil\" : \"SIL\" , \"\" : \"SIL\" } sil_phones = set ( phones_mapper . keys ()) punctuations = [ \";\" , \"?\" , \"!\" , \".\" , \",\" , \":\" ] def parse ( self ): speakers = ( [ i for i in os . listdir ( self . txt_grid_path ) if os . path . isdir ( os . path . join ( self . txt_grid_path , i )) ] if self . multi_speaker else [] ) data = [] if speakers : for speaker in speakers : file_list = os . listdir ( os . path . join ( self . txt_grid_path , speaker )) self . parse_text_grid ( file_list , data , speaker ) else : file_list = os . listdir ( self . txt_grid_path ) self . parse_text_grid ( file_list , data , \"\" ) with open ( os . path . join ( self . dataset_path , self . training_file ), \"w\" , encoding = \"utf-8\" ) as f : f . writelines ( data ) def parse_text_grid ( self , file_list : list , data : list , speaker_name : str ): for f_name in tqdm ( file_list ): text_grid = textgrid . TextGrid . fromFile ( os . path . join ( self . txt_grid_path , speaker_name , f_name ) ) pha = text_grid [ 1 ] durations = [] phs = [] flags = [] for iterator , interval in enumerate ( pha . intervals ): mark = interval . mark if mark in self . sil_phones or mark in self . punctuations : flags . append ( True ) else : flags . append ( False ) if mark in self . sil_phones : mark = self . phones_mapper [ mark ] dur = interval . duration () * ( self . sample_rate / self . hop_size ) durations . append ( round ( dur )) phs . append ( mark ) new_durations = [] for idx , ( flag , dur ) in enumerate ( zip ( flags , durations )): if len ( new_durations ) == 0 or ( flag and flags [ idx - 1 ] == False ): new_durations . append ( dur ) elif flag : new_durations [ len ( new_durations ) - 1 ] += dur else : new_durations . append ( dur ) full_ph = \" \" . join ( phs ) new_ph = full_ph matches = re . finditer ( \"( ?SIL)* ?([,!\\?\\.;:] ?){1,} ?(SIL ?)*\" , full_ph ) for match in matches : substring = full_ph [ match . start () : match . end ()] new_ph = new_ph . replace ( substring , f \" { substring . replace ( 'SIL' , '' ) . strip ()[ 0 ] } \" , 1 ) . strip () assert new_ph . split () . __len__ () == new_durations . __len__ () # safety check base_name = f_name . split ( \".TextGrid\" )[ 0 ] np . save ( os . path . join ( self . output_durations_path , f \" { base_name } -durations.npy\" ), np . array ( new_durations ) . astype ( np . int32 ), allow_pickle = False , ) data . append ( f \" { speaker_name } / { base_name } | { new_ph } | { speaker_name } \\n \" ) Then to use the modified script above, you just have to specify the arguments to the parser. args = { \"dataset_path\" : \"./en-bookbot\" , \"txt_grid_path\" : \"./outputs/parsed\" , # (1) \"output_durations_path\" : \"./en-bookbot/durations\" , \"sample_rate\" : 44100 , # (2) \"hop_size\" : 512 , # (3) \"multi_speaker\" : True , # (4) \"training_file\" : \"train.txt\" } txt_grid_parser = TxtGridParser ( ** args ) txt_grid_parser . parse () Replace this with whatever your {TEXTGRID_OUTPUT_DIR} was. Set this to the desired sample rate of your text-to-speech model. Set this to the desired hop size of your text-to-speech model. Multi-speaker or not, you can keep this as True . With the duration files located in durations/ and train.txt , we can finally train our own text-to-speech model!","title":"Parsing TextGrid Files"},{"location":"guides/lightspeech-mbmelgan/inference/","text":"Inference Everything here can be followed along in Google Colab! Load Models and Processor Inferencing TensorFlowTTS models are quite straightforward given that you know the expected inputs and outputs of each model. But first, we have to load pre-trained model weights. If you've followed the additional step of pushing model weights to HuggingFace Hub , you can simply load weights stored there! This also includes the processor that comes hand-in-hand with the text2mel model. To be able to load private models, you must first log into HuggingFace Hub. from huggingface_hub import notebook_login notebook_login () Token is valid. Your token has been saved in your configured git credential helpers (store). Your token has been saved to /root/.huggingface/token Login successful You can then load private models by specifying use_auth_token=True . from tensorflow_tts.inference import TFAutoModel , AutoProcessor import tensorflow as tf vocoder = TFAutoModel . from_pretrained ( \"bookbot/mb-melgan-hifi-postnets-en\" , use_auth_token = True ) text2mel = TFAutoModel . from_pretrained ( \"bookbot/lightspeech-mfa-en\" , use_auth_token = True ) processor = AutoProcessor . from_pretrained ( \"bookbot/lightspeech-mfa-en\" , use_auth_token = True ) processor . mode = \"eval\" # change processor from train to eval mode Tokenization Then, we'll need to tokenize raw text into their corresponding input IDs, which we can achieve by simply calling the text_to_sequence method of our processor. text = \"Hello world.\" input_ids = processor . text_to_sequence ( text ) LightSpeech Inference To perform inference of LightSpeech models, you will need to specify 5 different inputs: Input IDs Speaker ID Speed Ratio Pitch Ratio Energy Ratio We already have our input IDs -- we just need to specify which speaker index we'd want to use and optionally, specify other ratios. For now, we'll only be parameterizing the speaker ID and set the rest to 1.0 (default value). Keep in mind that TensorFlow models expect inputs to be batched. This is why we need to use the tf.expand_dims function on our input IDs (to make them of batch size 1) and the other inputs are also lists instead of raw scalar values. Moreover, LightSpeech (at least our implementation) returns 3 things: Mel-Spectrogram Duration Predictions Pitch (F0) Prediction We'll only be keeping the first (index 0) and ignore the rest. Note This is where understanding what our model's outputs are becomes important. We need to know at which index our desired output is. For instance, FastSpeech returns another mel-spectrogram (often called mel_after , \"after\" meaning, after the initial mel-spectrogram prediction is additionally passed through a Tacotron PostNet module), while LightSpeech only has one mel-spectrogram output, located at index 0. speaker_id = 0 mel_spectrogram , * _ = text2mel . inference ( input_ids = tf . expand_dims ( tf . convert_to_tensor ( input_ids , dtype = tf . int32 ), 0 ), speaker_ids = tf . convert_to_tensor ([ speaker_id ], dtype = tf . int32 ), speed_ratios = tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), f0_ratios = tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), energy_ratios = tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), ) Multi-Band MelGAN Inference After that, we can take the output mel-spectrogram generated by LightSpeech and use that as input to our MB-MelGAN model. audio = vocoder . inference ( mel_spectrogram )[ 0 , :, 0 ] Save Synthesized Audio as File Finally, we can save the predicted audio waveforms as a file by utilizing SoundFile. We'll just need to specify several parameters such as the output save name, the audio tensor, sample rate, and subtype. import soundfile as sf sf . write ( \"./audio.wav\" , audio , 44100 , \"PCM_16\" ) And with that, we have just synthesized an audio from pure text! If you're following from a Jupyter Notebook, you can play the saved audio via IPython's Audio widget. from IPython.display import Audio Audio ( \"audio.wav\" ) Your browser does not support the audio element.","title":"Inference"},{"location":"guides/lightspeech-mbmelgan/inference/#inference","text":"Everything here can be followed along in Google Colab!","title":"Inference"},{"location":"guides/lightspeech-mbmelgan/inference/#load-models-and-processor","text":"Inferencing TensorFlowTTS models are quite straightforward given that you know the expected inputs and outputs of each model. But first, we have to load pre-trained model weights. If you've followed the additional step of pushing model weights to HuggingFace Hub , you can simply load weights stored there! This also includes the processor that comes hand-in-hand with the text2mel model. To be able to load private models, you must first log into HuggingFace Hub. from huggingface_hub import notebook_login notebook_login () Token is valid. Your token has been saved in your configured git credential helpers (store). Your token has been saved to /root/.huggingface/token Login successful You can then load private models by specifying use_auth_token=True . from tensorflow_tts.inference import TFAutoModel , AutoProcessor import tensorflow as tf vocoder = TFAutoModel . from_pretrained ( \"bookbot/mb-melgan-hifi-postnets-en\" , use_auth_token = True ) text2mel = TFAutoModel . from_pretrained ( \"bookbot/lightspeech-mfa-en\" , use_auth_token = True ) processor = AutoProcessor . from_pretrained ( \"bookbot/lightspeech-mfa-en\" , use_auth_token = True ) processor . mode = \"eval\" # change processor from train to eval mode","title":"Load Models and Processor"},{"location":"guides/lightspeech-mbmelgan/inference/#tokenization","text":"Then, we'll need to tokenize raw text into their corresponding input IDs, which we can achieve by simply calling the text_to_sequence method of our processor. text = \"Hello world.\" input_ids = processor . text_to_sequence ( text )","title":"Tokenization"},{"location":"guides/lightspeech-mbmelgan/inference/#lightspeech-inference","text":"To perform inference of LightSpeech models, you will need to specify 5 different inputs: Input IDs Speaker ID Speed Ratio Pitch Ratio Energy Ratio We already have our input IDs -- we just need to specify which speaker index we'd want to use and optionally, specify other ratios. For now, we'll only be parameterizing the speaker ID and set the rest to 1.0 (default value). Keep in mind that TensorFlow models expect inputs to be batched. This is why we need to use the tf.expand_dims function on our input IDs (to make them of batch size 1) and the other inputs are also lists instead of raw scalar values. Moreover, LightSpeech (at least our implementation) returns 3 things: Mel-Spectrogram Duration Predictions Pitch (F0) Prediction We'll only be keeping the first (index 0) and ignore the rest. Note This is where understanding what our model's outputs are becomes important. We need to know at which index our desired output is. For instance, FastSpeech returns another mel-spectrogram (often called mel_after , \"after\" meaning, after the initial mel-spectrogram prediction is additionally passed through a Tacotron PostNet module), while LightSpeech only has one mel-spectrogram output, located at index 0. speaker_id = 0 mel_spectrogram , * _ = text2mel . inference ( input_ids = tf . expand_dims ( tf . convert_to_tensor ( input_ids , dtype = tf . int32 ), 0 ), speaker_ids = tf . convert_to_tensor ([ speaker_id ], dtype = tf . int32 ), speed_ratios = tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), f0_ratios = tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), energy_ratios = tf . convert_to_tensor ([ 1.0 ], dtype = tf . float32 ), )","title":"LightSpeech Inference"},{"location":"guides/lightspeech-mbmelgan/inference/#multi-band-melgan-inference","text":"After that, we can take the output mel-spectrogram generated by LightSpeech and use that as input to our MB-MelGAN model. audio = vocoder . inference ( mel_spectrogram )[ 0 , :, 0 ]","title":"Multi-Band MelGAN Inference"},{"location":"guides/lightspeech-mbmelgan/inference/#save-synthesized-audio-as-file","text":"Finally, we can save the predicted audio waveforms as a file by utilizing SoundFile. We'll just need to specify several parameters such as the output save name, the audio tensor, sample rate, and subtype. import soundfile as sf sf . write ( \"./audio.wav\" , audio , 44100 , \"PCM_16\" ) And with that, we have just synthesized an audio from pure text! If you're following from a Jupyter Notebook, you can play the saved audio via IPython's Audio widget. from IPython.display import Audio Audio ( \"audio.wav\" ) Your browser does not support the audio element.","title":"Save Synthesized Audio as File"},{"location":"guides/lightspeech-mbmelgan/intro/","text":"Introduction This guide will explain how to train a LightSpeech acoustic model and a Multi-band MelGAN vocoder model (with a HiFi-GAN Discriminator). In particular, we will be training a 44.1 kHz model, with a hop size of 512. This guide expects you to train an IPA-based model. For now, this tutorial only supports English and Indonesian -- because only these two languages have its correponding IPA-based grapheme-to-phoneme processor added to the custom fork. For English, we use gruut and for Indonesian, we use g2p_id . To add support for other languages, you would need a grapheme-to-phoneme converter for that language, and support it as a processor in TensorFlowTTS. We will introduce a separate tutorial for that in the future. LightSpeech LightSpeech follows the same architecture as FastSpeech2 , except with an optimized model configuration obtained via Neural Architecture Search (NAS). In our case, we don't really perform NAS, but use the previously found best model configuration. Multi-band MelGAN Multi-Band MelGAN is an improvement upon MelGAN that does waveform generation and waveform discrimination on a multi-band basis. HiFi-GAN Discrminator Further, instead of using the original discriminator, we can use the discriminator presented in the HiFi-GAN paper. We specifically use the multi-period discriminator (MPD) on the right.","title":"Introduction"},{"location":"guides/lightspeech-mbmelgan/intro/#introduction","text":"This guide will explain how to train a LightSpeech acoustic model and a Multi-band MelGAN vocoder model (with a HiFi-GAN Discriminator). In particular, we will be training a 44.1 kHz model, with a hop size of 512. This guide expects you to train an IPA-based model. For now, this tutorial only supports English and Indonesian -- because only these two languages have its correponding IPA-based grapheme-to-phoneme processor added to the custom fork. For English, we use gruut and for Indonesian, we use g2p_id . To add support for other languages, you would need a grapheme-to-phoneme converter for that language, and support it as a processor in TensorFlowTTS. We will introduce a separate tutorial for that in the future.","title":"Introduction"},{"location":"guides/lightspeech-mbmelgan/intro/#lightspeech","text":"LightSpeech follows the same architecture as FastSpeech2 , except with an optimized model configuration obtained via Neural Architecture Search (NAS). In our case, we don't really perform NAS, but use the previously found best model configuration.","title":"LightSpeech"},{"location":"guides/lightspeech-mbmelgan/intro/#multi-band-melgan","text":"Multi-Band MelGAN is an improvement upon MelGAN that does waveform generation and waveform discrimination on a multi-band basis.","title":"Multi-band MelGAN"},{"location":"guides/lightspeech-mbmelgan/intro/#hifi-gan-discrminator","text":"Further, instead of using the original discriminator, we can use the discriminator presented in the HiFi-GAN paper. We specifically use the multi-period discriminator (MPD) on the right.","title":"HiFi-GAN Discrminator"},{"location":"guides/lightspeech-mbmelgan/training/","text":"Training Folder Structure Let's revisit the structure of our dataset now that we have new components. We need the audio files, duration files, and metadata file to be located in the same folder. Continuing with the same sample dataset , we should end up with these files by the end of the duration extraction step: en-bookbot/ \u251c\u2500\u2500 durations/ \u2502 \u251c\u2500\u2500 en-AU-Zak_0-durations.npy \u2502 \u251c\u2500\u2500 en-AU-Zal_1-durations.npy \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 en-UK-Thalia_0-durations.npy \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 en-US-Madison_0-durations.npy \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-AU-Zak/ \u2502 \u251c\u2500\u2500 en-AU-Zak_0.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_0.wav \u2502 \u251c\u2500\u2500 en-AU-Zak_1.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_1.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-UK-Thalia/ \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.lab \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-US-Madison/ \u2502 \u251c\u2500\u2500 en-US-Madison_0.lab \u2502 \u251c\u2500\u2500 en-US-Madison_0.wav \u2502 \u251c\u2500\u2500 ... \u2514\u2500\u2500 train.txt Training LightSpeech We start by first preprocessing and normalizing the audio files. These commands will handle tokenization, feature extraction, etc. tensorflow-tts-preprocess --rootdir ./en-bookbot --outdir ./dump --config TensorFlowTTS/preprocess/englishipa_preprocess.yaml --dataset englishipa --verbose 2 tensorflow-tts-normalize --rootdir ./dump --outdir ./dump --config TensorFlowTTS/preprocess/englishipa_preprocess.yaml --dataset englishipa --verbose 2 It's also recommended to fix mis-matching duration files python TensorFlowTTS/examples/mfa_extraction/fix_mismatch.py \\ --base_path ./dump \\ --trimmed_dur_path ./en-bookbot/trimmed-durations \\ --dur_path ./en-bookbot/durations \\ --use_norm t We can then train the LightSpeech model CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/lightspeech/train_lightspeech.py \\ --train-dir ./dump/train/ \\ --dev-dir ./dump/valid/ \\ --outdir ./lightspeech-en-bookbot \\ # (1) --config ./TensorFlowTTS/examples/lightspeech/conf/lightspeech_englishipa.yaml \\ # (2) --use-norm 1 \\ --f0-stat ./dump/stats_f0.npy \\ --energy-stat ./dump/stats_energy.npy \\ --mixed_precision 1 \\ --dataset_config TensorFlowTTS/preprocess/englishipa_preprocess.yaml \\ --dataset_stats dump/stats.npy \\ --dataset_mapping dump/englishipa_mapper.json You can set this to whatever output folder you'd like. This is a pre-configured training configuration. Feel free to customize it, but be careful with setting the sample rate and hop size. Once it's finished, you should end up with the following files: lightspeech-en-bookbot/ \u251c\u2500\u2500 checkpoints/ # (1) \u2502 \u251c\u2500\u2500 ckpt-10000.data-00000-of-00001 \u2502 \u251c\u2500\u2500 ckpt-10000.index \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 model-10000.h5 \u2502 \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 model-200000.h5 # (2) \u251c\u2500\u2500 config.yml \u251c\u2500\u2500 events.out.tfevents.1669084428.bookbot-tf-2.10561.0.v2 \u2514\u2500\u2500 predictions/ # (3) \u251c\u2500\u2500 100000steps/ \u2502 \u251c\u2500\u2500 b 'en-US-Madison_11' .png \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 10000steps/ \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 ... This contains all of the training checkpoints. The final model checkpoint (which we want). This contains all mid-training intermediate predictions (mel-spectrograms). It is missing the processor file, and the final model training checkpoint is still in the checkpoints/ subfolder. For the former, we can simply copy the file from dump to the output training folder. And for the latter, we can just copy the file up a directory. cd lightspeech-en-bookbot cp ../dump/englishipa_mapper.json processor.json cp checkpoints/model-200000.h5 model.h5 Training Multi-band MelGAN We can then continue with the training of our Multi-band MelGAN as our Vocoder model. First of all, you have the option to either: 1. Train to generate speech from original mel-spectrogram, or 2. Train to generate speech from LightSpeech-predicted mel-spectrogram. This is also known as training on PostNets. Selecting option 1 would likely give you a more \"universal\" vocoder, one that would likely retain its performance on unseen mel-spectrograms. However, I often find its performance on small-sized datasets quite poor, and hence why I'd usually opt for the second option instead. Training on PostNets would allow the model to also learn the flaws of the LightSpeech-predicted mel-spectrograms and still aim to generate the best audio quality. To do so, we begin by extracting the PostNets of our LightSpeech models. This means running inference on all of our texts and saving the predicted mel-spectrograms. We can do so using this modified LightSpeech PostNet Extraction Script . With that, we can simply run CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/lightspeech/extractls_postnets.py \\ --rootdir ./dump/train \\ --outdir ./dump/train \\ --config ./TensorFlowTTS/examples/lightspeech/conf/lightspeech_englishipa.yaml \\ --checkpoint ./lightspeech-en-bookbot/model.h5 \\ --dataset_mapping ./lightspeech-en-bookbot/processor.json CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/lightspeech/extractls_postnets.py \\ --rootdir ./dump/valid \\ --outdir ./dump/valid \\ --config ./TensorFlowTTS/examples/lightspeech/conf/lightspeech_englishipa.yaml \\ --checkpoint ./lightspeech-en-bookbot/model.h5 \\ --dataset_mapping ./lightspeech-en-bookbot/processor.json That will perform inference on the training and validation subsets. Finally, we can train the Multi-band MelGAN with the HiFi-GAN Discriminator by doing the following CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/multiband_melgan_hf/train_multiband_melgan_hf.py \\ --train-dir ./dump/train/ \\ --dev-dir ./dump/valid/ \\ --outdir ./mb-melgan-hifi-en-bookbot/ \\ --config ./TensorFlowTTS/examples/multiband_melgan_hf/conf/multiband_melgan_hf.en.v1.yml \\ --use-norm 1 \\ --generator_mixed_precision 1 \\ --postnets 1 \\ --resume \"\" CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/multiband_melgan_hf/train_multiband_melgan_hf.py \\ --train-dir ./dump/train/ \\ --dev-dir ./dump/valid/ \\ --outdir ./mb-melgan-hifi-en-bookbot/ \\ --config ./TensorFlowTTS/examples/multiband_melgan_hf/conf/multiband_melgan_hf.en.v1.yml \\ --use-norm 1 \\ --postnets 1 \\ --resume ./mb-melgan-hifi-en-bookbot/checkpoints/ckpt-200000 Note that this first pre-trains only the generator for 200,000 steps, and then continues the remaining steps with the usual GAN training framework. At the end of training, we should end up with the following files mb-melgan-hifi-en-bookbot/ \u251c\u2500\u2500 checkpoints/ # (1) \u2502 \u251c\u2500\u2500 checkpoint \u2502 \u251c\u2500\u2500 ckpt-100000.data-00000-of-00001 \u2502 \u251c\u2500\u2500 ckpt-100000.index \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 discriminator-100000.h5 \u2502 \u251c\u2500\u2500 discriminator-120000.h5 \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 generator-100000.h5 \u2502 \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 generator-1000000.h5 # (2) \u251c\u2500\u2500 config.yml \u251c\u2500\u2500 events.out.tfevents.1669101534.bookbot-tf-2.21897.0.v2 \u251c\u2500\u2500 events.out.tfevents.1669116847.bookbot-tf-2.31894.0.v2 \u2514\u2500\u2500 predictions/ # (3) \u251c\u2500\u2500 100000steps/ \u2502 \u251c\u2500\u2500 b 'en-US-Madison_11' .png \u2502 \u251c\u2500\u2500 b 'en-US-Madison_11' _gen.wav \u2502 \u251c\u2500\u2500 b 'en-US-Madison_11' _ref.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 10000steps/ \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 ... This contains all of the training checkpoints. The final generator checkpoint (which we want). This contains all mid-training intermediate predictions (waveforms). We can simply copy the final generator model into the output model directory. cd mb-melgan-hifi-en-bookbot cp checkpoints/generator-1000000.h5 model.h5 With that, we are done! Extra: Pushing Models to HuggingFace Hub To wrap things up, it will be very convenient to push our models to HuggingFace Hub . This would allow us to not only load pre-trained model weights stored on the Hub, but also has other goodies such as version control, documentation, etc., that comes with using Git. First, create a new repository on the Hub here . Second, install Git LFS in the machine you're working on sudo apt-get install git-lfs git lfs install The rest is pretty much the same as how you would push files to a Git repository. For example git clone https://huggingface.co/bookbot/lightspeech-mfa-en cp -r lightspeech-en-bookbot/* lightspeech-mfa-en cd lightspeech-mfa-en git add . && git commit -m \"added weights\" && git push git clone https://huggingface.co/bookbot/mb-melgan-hifi-postnets-en cp -r mb-melgan-hifi-en-bookbot/* mb-melgan-hifi-postnets-en cd mb-melgan-hifi-postnets-en git add . && git commit -m \"added weights\" && git push This would allow us to load models like so (in Python): model = TFAutoModel . from_pretrained ( \"bookbot/lightspeech-mfa-en\" )","title":"Training"},{"location":"guides/lightspeech-mbmelgan/training/#training","text":"","title":"Training"},{"location":"guides/lightspeech-mbmelgan/training/#folder-structure","text":"Let's revisit the structure of our dataset now that we have new components. We need the audio files, duration files, and metadata file to be located in the same folder. Continuing with the same sample dataset , we should end up with these files by the end of the duration extraction step: en-bookbot/ \u251c\u2500\u2500 durations/ \u2502 \u251c\u2500\u2500 en-AU-Zak_0-durations.npy \u2502 \u251c\u2500\u2500 en-AU-Zal_1-durations.npy \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 en-UK-Thalia_0-durations.npy \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 en-US-Madison_0-durations.npy \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-AU-Zak/ \u2502 \u251c\u2500\u2500 en-AU-Zak_0.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_0.wav \u2502 \u251c\u2500\u2500 en-AU-Zak_1.lab \u2502 \u251c\u2500\u2500 en-AU-Zak_1.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-UK-Thalia/ \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.lab \u2502 \u251c\u2500\u2500 en-UK-Thalia_0.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 en-US-Madison/ \u2502 \u251c\u2500\u2500 en-US-Madison_0.lab \u2502 \u251c\u2500\u2500 en-US-Madison_0.wav \u2502 \u251c\u2500\u2500 ... \u2514\u2500\u2500 train.txt","title":"Folder Structure"},{"location":"guides/lightspeech-mbmelgan/training/#training-lightspeech","text":"We start by first preprocessing and normalizing the audio files. These commands will handle tokenization, feature extraction, etc. tensorflow-tts-preprocess --rootdir ./en-bookbot --outdir ./dump --config TensorFlowTTS/preprocess/englishipa_preprocess.yaml --dataset englishipa --verbose 2 tensorflow-tts-normalize --rootdir ./dump --outdir ./dump --config TensorFlowTTS/preprocess/englishipa_preprocess.yaml --dataset englishipa --verbose 2 It's also recommended to fix mis-matching duration files python TensorFlowTTS/examples/mfa_extraction/fix_mismatch.py \\ --base_path ./dump \\ --trimmed_dur_path ./en-bookbot/trimmed-durations \\ --dur_path ./en-bookbot/durations \\ --use_norm t We can then train the LightSpeech model CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/lightspeech/train_lightspeech.py \\ --train-dir ./dump/train/ \\ --dev-dir ./dump/valid/ \\ --outdir ./lightspeech-en-bookbot \\ # (1) --config ./TensorFlowTTS/examples/lightspeech/conf/lightspeech_englishipa.yaml \\ # (2) --use-norm 1 \\ --f0-stat ./dump/stats_f0.npy \\ --energy-stat ./dump/stats_energy.npy \\ --mixed_precision 1 \\ --dataset_config TensorFlowTTS/preprocess/englishipa_preprocess.yaml \\ --dataset_stats dump/stats.npy \\ --dataset_mapping dump/englishipa_mapper.json You can set this to whatever output folder you'd like. This is a pre-configured training configuration. Feel free to customize it, but be careful with setting the sample rate and hop size. Once it's finished, you should end up with the following files: lightspeech-en-bookbot/ \u251c\u2500\u2500 checkpoints/ # (1) \u2502 \u251c\u2500\u2500 ckpt-10000.data-00000-of-00001 \u2502 \u251c\u2500\u2500 ckpt-10000.index \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 model-10000.h5 \u2502 \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 model-200000.h5 # (2) \u251c\u2500\u2500 config.yml \u251c\u2500\u2500 events.out.tfevents.1669084428.bookbot-tf-2.10561.0.v2 \u2514\u2500\u2500 predictions/ # (3) \u251c\u2500\u2500 100000steps/ \u2502 \u251c\u2500\u2500 b 'en-US-Madison_11' .png \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 10000steps/ \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 ... This contains all of the training checkpoints. The final model checkpoint (which we want). This contains all mid-training intermediate predictions (mel-spectrograms). It is missing the processor file, and the final model training checkpoint is still in the checkpoints/ subfolder. For the former, we can simply copy the file from dump to the output training folder. And for the latter, we can just copy the file up a directory. cd lightspeech-en-bookbot cp ../dump/englishipa_mapper.json processor.json cp checkpoints/model-200000.h5 model.h5","title":"Training LightSpeech"},{"location":"guides/lightspeech-mbmelgan/training/#training-multi-band-melgan","text":"We can then continue with the training of our Multi-band MelGAN as our Vocoder model. First of all, you have the option to either: 1. Train to generate speech from original mel-spectrogram, or 2. Train to generate speech from LightSpeech-predicted mel-spectrogram. This is also known as training on PostNets. Selecting option 1 would likely give you a more \"universal\" vocoder, one that would likely retain its performance on unseen mel-spectrograms. However, I often find its performance on small-sized datasets quite poor, and hence why I'd usually opt for the second option instead. Training on PostNets would allow the model to also learn the flaws of the LightSpeech-predicted mel-spectrograms and still aim to generate the best audio quality. To do so, we begin by extracting the PostNets of our LightSpeech models. This means running inference on all of our texts and saving the predicted mel-spectrograms. We can do so using this modified LightSpeech PostNet Extraction Script . With that, we can simply run CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/lightspeech/extractls_postnets.py \\ --rootdir ./dump/train \\ --outdir ./dump/train \\ --config ./TensorFlowTTS/examples/lightspeech/conf/lightspeech_englishipa.yaml \\ --checkpoint ./lightspeech-en-bookbot/model.h5 \\ --dataset_mapping ./lightspeech-en-bookbot/processor.json CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/lightspeech/extractls_postnets.py \\ --rootdir ./dump/valid \\ --outdir ./dump/valid \\ --config ./TensorFlowTTS/examples/lightspeech/conf/lightspeech_englishipa.yaml \\ --checkpoint ./lightspeech-en-bookbot/model.h5 \\ --dataset_mapping ./lightspeech-en-bookbot/processor.json That will perform inference on the training and validation subsets. Finally, we can train the Multi-band MelGAN with the HiFi-GAN Discriminator by doing the following CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/multiband_melgan_hf/train_multiband_melgan_hf.py \\ --train-dir ./dump/train/ \\ --dev-dir ./dump/valid/ \\ --outdir ./mb-melgan-hifi-en-bookbot/ \\ --config ./TensorFlowTTS/examples/multiband_melgan_hf/conf/multiband_melgan_hf.en.v1.yml \\ --use-norm 1 \\ --generator_mixed_precision 1 \\ --postnets 1 \\ --resume \"\" CUDA_VISIBLE_DEVICES = 0 python TensorFlowTTS/examples/multiband_melgan_hf/train_multiband_melgan_hf.py \\ --train-dir ./dump/train/ \\ --dev-dir ./dump/valid/ \\ --outdir ./mb-melgan-hifi-en-bookbot/ \\ --config ./TensorFlowTTS/examples/multiband_melgan_hf/conf/multiband_melgan_hf.en.v1.yml \\ --use-norm 1 \\ --postnets 1 \\ --resume ./mb-melgan-hifi-en-bookbot/checkpoints/ckpt-200000 Note that this first pre-trains only the generator for 200,000 steps, and then continues the remaining steps with the usual GAN training framework. At the end of training, we should end up with the following files mb-melgan-hifi-en-bookbot/ \u251c\u2500\u2500 checkpoints/ # (1) \u2502 \u251c\u2500\u2500 checkpoint \u2502 \u251c\u2500\u2500 ckpt-100000.data-00000-of-00001 \u2502 \u251c\u2500\u2500 ckpt-100000.index \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 discriminator-100000.h5 \u2502 \u251c\u2500\u2500 discriminator-120000.h5 \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 generator-100000.h5 \u2502 \u251c\u2500\u2500 ... \u2502 \u2514\u2500\u2500 generator-1000000.h5 # (2) \u251c\u2500\u2500 config.yml \u251c\u2500\u2500 events.out.tfevents.1669101534.bookbot-tf-2.21897.0.v2 \u251c\u2500\u2500 events.out.tfevents.1669116847.bookbot-tf-2.31894.0.v2 \u2514\u2500\u2500 predictions/ # (3) \u251c\u2500\u2500 100000steps/ \u2502 \u251c\u2500\u2500 b 'en-US-Madison_11' .png \u2502 \u251c\u2500\u2500 b 'en-US-Madison_11' _gen.wav \u2502 \u251c\u2500\u2500 b 'en-US-Madison_11' _ref.wav \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 10000steps/ \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 ... This contains all of the training checkpoints. The final generator checkpoint (which we want). This contains all mid-training intermediate predictions (waveforms). We can simply copy the final generator model into the output model directory. cd mb-melgan-hifi-en-bookbot cp checkpoints/generator-1000000.h5 model.h5 With that, we are done!","title":"Training Multi-band MelGAN"},{"location":"guides/lightspeech-mbmelgan/training/#extra-pushing-models-to-huggingface-hub","text":"To wrap things up, it will be very convenient to push our models to HuggingFace Hub . This would allow us to not only load pre-trained model weights stored on the Hub, but also has other goodies such as version control, documentation, etc., that comes with using Git. First, create a new repository on the Hub here . Second, install Git LFS in the machine you're working on sudo apt-get install git-lfs git lfs install The rest is pretty much the same as how you would push files to a Git repository. For example git clone https://huggingface.co/bookbot/lightspeech-mfa-en cp -r lightspeech-en-bookbot/* lightspeech-mfa-en cd lightspeech-mfa-en git add . && git commit -m \"added weights\" && git push git clone https://huggingface.co/bookbot/mb-melgan-hifi-postnets-en cp -r mb-melgan-hifi-en-bookbot/* mb-melgan-hifi-postnets-en cd mb-melgan-hifi-postnets-en git add . && git commit -m \"added weights\" && git push This would allow us to load models like so (in Python): model = TFAutoModel . from_pretrained ( \"bookbot/lightspeech-mfa-en\" )","title":"Extra: Pushing Models to HuggingFace Hub"},{"location":"models/english/","text":"English Acoustic Models Model SR (kHz) Tokenizer Dataset FastSpeech2 EN v2 22.05 Character-based Azure FastSpeech2 EN v3 44.10 g2p_en (ARPA) Azure FastSpeech2 MFA EN v2 22.05 g2p_en (ARPA) Azure FastSpeech2 MFA EN v3 44.10 gruut (IPA) Azure FastSpeech2 MFA EN v4 44.10 gruut (IPA) Azure (Mastered) FastSpeech2 MFA EN ESD Angry 44.10 gruut (IPA) Emotional Speech Dataset - Angry LightSpeech MFA EN 44.10 gruut (IPA) Azure (Mastered) LightSpeech MFA EN v2 44.10 gruut (IPA) Azure (Mastered) LightSpeech MFA EN ESD 44.10 gruut (IPA) Emotional Speech Dataset - 0013 Vocoder Models Model SR (kHz) Dataset MB-MelGAN EN 22.05 Azure MB-MelGAN HiFi EN 22.05 Azure MB-MelGAN HiFi PostNets EN 22.05 Azure MB-MelGAN HiFi PostNets EN v2 22.05 Azure MB-MelGAN HiFi PostNets EN v3 44.10 Azure MB-MelGAN HiFi PostNets EN v5 44.10 Azure MB-MelGAN HiFi PostNets EN v6 44.10 Azure (Mastered) MB-MelGAN HiFi PostNets EN v7 44.10 Azure (Mastered) MB-MelGAN HiFi PostNets EN v8 44.10 Azure (Mastered) MB-MelGAN HiFi PostNets EN ESD Angry 44.10 Emotional Speech Dataset - Angry MB-MelGAN HiFi PostNets EN ESD 44.10 Emotional Speech Dataset - 0013","title":"English"},{"location":"models/english/#english","text":"","title":"English"},{"location":"models/english/#acoustic-models","text":"Model SR (kHz) Tokenizer Dataset FastSpeech2 EN v2 22.05 Character-based Azure FastSpeech2 EN v3 44.10 g2p_en (ARPA) Azure FastSpeech2 MFA EN v2 22.05 g2p_en (ARPA) Azure FastSpeech2 MFA EN v3 44.10 gruut (IPA) Azure FastSpeech2 MFA EN v4 44.10 gruut (IPA) Azure (Mastered) FastSpeech2 MFA EN ESD Angry 44.10 gruut (IPA) Emotional Speech Dataset - Angry LightSpeech MFA EN 44.10 gruut (IPA) Azure (Mastered) LightSpeech MFA EN v2 44.10 gruut (IPA) Azure (Mastered) LightSpeech MFA EN ESD 44.10 gruut (IPA) Emotional Speech Dataset - 0013","title":"Acoustic Models"},{"location":"models/english/#vocoder-models","text":"Model SR (kHz) Dataset MB-MelGAN EN 22.05 Azure MB-MelGAN HiFi EN 22.05 Azure MB-MelGAN HiFi PostNets EN 22.05 Azure MB-MelGAN HiFi PostNets EN v2 22.05 Azure MB-MelGAN HiFi PostNets EN v3 44.10 Azure MB-MelGAN HiFi PostNets EN v5 44.10 Azure MB-MelGAN HiFi PostNets EN v6 44.10 Azure (Mastered) MB-MelGAN HiFi PostNets EN v7 44.10 Azure (Mastered) MB-MelGAN HiFi PostNets EN v8 44.10 Azure (Mastered) MB-MelGAN HiFi PostNets EN ESD Angry 44.10 Emotional Speech Dataset - Angry MB-MelGAN HiFi PostNets EN ESD 44.10 Emotional Speech Dataset - 0013","title":"Vocoder Models"},{"location":"models/indonesian/","text":"Indonesian Acoustic Models Model SR (kHz) Tokenizer Dataset FastSpeech2 ID 22.05 Character-based Azure + WaveNet + Weildan FastSpeech2 MFA ID v4 44.10 g2p_id (IPA) Weildan FastSpeech2 MFA ID v5 44.10 g2p_id (IPA) Weildan (Mastered) FastSpeech2 MFA ID v7 44.10 g2p_id (IPA) Azure LightSpeech MFA ID 44.10 g2p_id (IPA) Azure LightSpeech MFA ID v2 22.05 g2p_id (IPA) Azure LightSpeech MFA ID v3 32.00 g2p_id (IPA) Azure Tacotron2 ID 22.05 Character-based Azure Tacotron2 ID v2 44.10 Character-based Azure Vocoder Models Model SR (kHz) Dataset MB-MelGAN HiFi PostNets ID 22.05 Azure + WaveNet + Weildan MB-MelGAN HiFi PostNets ID v4 44.10 Weildan MB-MelGAN HiFi PostNets ID v5 44.10 Weildan (Mastered) MB-MelGAN HiFi PostNets ID v7 44.10 Azure MB-MelGAN HiFi PostNets ID v8 44.10 Azure MB-MelGAN HiFi PostNets ID v9 22.05 Azure MB-MelGAN HiFi PostNets ID v10 32.00 Azure","title":"Indonesian"},{"location":"models/indonesian/#indonesian","text":"","title":"Indonesian"},{"location":"models/indonesian/#acoustic-models","text":"Model SR (kHz) Tokenizer Dataset FastSpeech2 ID 22.05 Character-based Azure + WaveNet + Weildan FastSpeech2 MFA ID v4 44.10 g2p_id (IPA) Weildan FastSpeech2 MFA ID v5 44.10 g2p_id (IPA) Weildan (Mastered) FastSpeech2 MFA ID v7 44.10 g2p_id (IPA) Azure LightSpeech MFA ID 44.10 g2p_id (IPA) Azure LightSpeech MFA ID v2 22.05 g2p_id (IPA) Azure LightSpeech MFA ID v3 32.00 g2p_id (IPA) Azure Tacotron2 ID 22.05 Character-based Azure Tacotron2 ID v2 44.10 Character-based Azure","title":"Acoustic Models"},{"location":"models/indonesian/#vocoder-models","text":"Model SR (kHz) Dataset MB-MelGAN HiFi PostNets ID 22.05 Azure + WaveNet + Weildan MB-MelGAN HiFi PostNets ID v4 44.10 Weildan MB-MelGAN HiFi PostNets ID v5 44.10 Weildan (Mastered) MB-MelGAN HiFi PostNets ID v7 44.10 Azure MB-MelGAN HiFi PostNets ID v8 44.10 Azure MB-MelGAN HiFi PostNets ID v9 22.05 Azure MB-MelGAN HiFi PostNets ID v10 32.00 Azure","title":"Vocoder Models"},{"location":"results/english/","text":"English Azure She says that she has more books as well. Original Audio FastSpeech2 MFA EN v4 & MB-MelGAN HiFi PostNets EN v6 LightSpeech MFA EN v2 & MB-MelGAN HiFi PostNets EN v 8 They are the types of dragons that give you a fright to look at. Original Audio FastSpeech2 MFA EN v4 & MB-MelGAN HiFi PostNets EN v6 LightSpeech MFA EN v2 & MB-MelGAN HiFi PostNets EN v 8 The mouse is asleep in his bed, unaware of the devious spirit stalking him in the night. Original Audio FastSpeech2 MFA EN v4 & MB-MelGAN HiFi PostNets EN v6 LightSpeech MFA EN v2 & MB-MelGAN HiFi PostNets EN v 8 Emotional Speech Dataset - 0013 The nine the eggs, I keep. Emotion Original Audio LightSpeech MFA EN ESD & MB-MelGAN HiFi PostNets EN ESD Angry Happy Neutral Surprise Sad","title":"English"},{"location":"results/english/#english","text":"","title":"English"},{"location":"results/english/#azure","text":"She says that she has more books as well. Original Audio FastSpeech2 MFA EN v4 & MB-MelGAN HiFi PostNets EN v6 LightSpeech MFA EN v2 & MB-MelGAN HiFi PostNets EN v 8 They are the types of dragons that give you a fright to look at. Original Audio FastSpeech2 MFA EN v4 & MB-MelGAN HiFi PostNets EN v6 LightSpeech MFA EN v2 & MB-MelGAN HiFi PostNets EN v 8 The mouse is asleep in his bed, unaware of the devious spirit stalking him in the night. Original Audio FastSpeech2 MFA EN v4 & MB-MelGAN HiFi PostNets EN v6 LightSpeech MFA EN v2 & MB-MelGAN HiFi PostNets EN v 8","title":"Azure"},{"location":"results/english/#emotional-speech-dataset-0013","text":"The nine the eggs, I keep. Emotion Original Audio LightSpeech MFA EN ESD & MB-MelGAN HiFi PostNets EN ESD Angry Happy Neutral Surprise Sad","title":"Emotional Speech Dataset - 0013"},{"location":"results/indonesian/","text":"Indonesian Azure Sepulang sekolah, Fitri sangat lapar. Original Audio LightSpeech MFA ID & MB-MelGAN HiFi PostNets ID v8 Ia harus mengganti baju dahulu. Original Audio LightSpeech MFA ID & MB-MelGAN HiFi PostNets ID v8 Ketika Nina kembali ke gua, ia terlalu besar untuk bisa masuk ke dalamnya! Original Audio LightSpeech MFA ID & MB-MelGAN HiFi PostNets ID v8","title":"Indonesian"},{"location":"results/indonesian/#indonesian","text":"","title":"Indonesian"},{"location":"results/indonesian/#azure","text":"Sepulang sekolah, Fitri sangat lapar. Original Audio LightSpeech MFA ID & MB-MelGAN HiFi PostNets ID v8 Ia harus mengganti baju dahulu. Original Audio LightSpeech MFA ID & MB-MelGAN HiFi PostNets ID v8 Ketika Nina kembali ke gua, ia terlalu besar untuk bisa masuk ke dalamnya! Original Audio LightSpeech MFA ID & MB-MelGAN HiFi PostNets ID v8","title":"Azure"}]}