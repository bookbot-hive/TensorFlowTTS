{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "seawwzouvM3i",
        "outputId": "16afe168-4528-46d9-f357-44e2c9a78c4e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-15 01:50:16.817006: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-15 01:50:16.839517: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-15 01:50:17.202512: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/home/s44504/miniconda3/envs/tftts2/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.12.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n",
            "/home/s44504/miniconda3/envs/tftts2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow_tts.inference import TFAutoModel, AutoConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U5s1iwDrumM9"
      },
      "outputs": [],
      "source": [
        "def convert_text2mel_tflite(\n",
        "    model_path: str, save_name: str, config_path: str = None, use_auth_token: bool = False\n",
        ") -> float:\n",
        "    # load pretrained model\n",
        "    config = AutoConfig.from_pretrained(config_path if config_path else model_path)\n",
        "    kwargs = {\"use_auth_token\": use_auth_token} if use_auth_token else {}\n",
        "    model = TFAutoModel.from_pretrained(\n",
        "        model_path, config=config, enable_tflite_convertible=True, **kwargs\n",
        "    )\n",
        "\n",
        "    # setup model concrete function\n",
        "    concrete_function = model.inference_tflite.get_concrete_function()\n",
        "    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_function])\n",
        "\n",
        "    # specify optimizations\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.target_spec.supported_ops = [\n",
        "        tf.lite.OpsSet.TFLITE_BUILTINS,  # quantize\n",
        "        tf.lite.OpsSet.SELECT_TF_OPS,\n",
        "    ]\n",
        "\n",
        "    # convert and save model to TensorFlowLite\n",
        "    tflite_model = converter.convert()\n",
        "    with open(save_name, \"wb\") as f:\n",
        "        f.write(tflite_model)\n",
        "\n",
        "    size = len(tflite_model) / 1024 / 1024.0\n",
        "    return size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AvXrSdqw00wm"
      },
      "outputs": [],
      "source": [
        "def convert_vocoder_tflite(\n",
        "    model_path: str, save_name: str, config_path: str = None, use_auth_token: bool = False\n",
        ") -> float:\n",
        "    config = AutoConfig.from_pretrained(config_path if config_path else model_path)\n",
        "    kwargs = {\"use_auth_token\": use_auth_token} if use_auth_token else {}\n",
        "    # load pretrained model\n",
        "    model = TFAutoModel.from_pretrained(model_path, config=config, **kwargs)\n",
        "\n",
        "    # setup model concrete function\n",
        "    concrete_function = model.inference_tflite.get_concrete_function()\n",
        "    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_function])\n",
        "\n",
        "    # specify optimizations\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "    converter.target_spec.supported_types = [tf.float16]  # fp16 ops\n",
        "\n",
        "    # convert and save model to TensorFlowLite\n",
        "    tflite_model = converter.convert()\n",
        "    with open(save_name, \"wb\") as f:\n",
        "        f.write(tflite_model)\n",
        "\n",
        "    size = len(tflite_model) / 1024 / 1024.0\n",
        "    return size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jCt7SSbB03Ta"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Please consider providing the trackable_obj argument in the from_concrete_functions. Providing without the trackable_obj argument is deprecated and it will use the deprecated conversion path.\n",
            "2024-07-15 01:51:37.447940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:37.448086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:37.448133: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 2\n",
            "2024-07-15 01:51:37.448187: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
            "2024-07-15 01:51:37.448514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:37.448576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:37.448635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:37.448689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:37.448742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:37.448795: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:37.448920: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:37.448976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:37.449035: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:37.449082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22279 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
            "2024-07-15 01:51:37.449101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:37.449143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22279 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:02:00.0, compute capability: 8.9\n",
            "2024-07-15 01:51:38.079040: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
            "2024-07-15 01:51:38.079060: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
            "WARNING:absl:Please consider providing the trackable_obj argument in the from_concrete_functions. Providing without the trackable_obj argument is deprecated and it will use the deprecated conversion path.\n",
            "2024-07-15 01:51:40.229860: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.229999: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.230046: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 2\n",
            "2024-07-15 01:51:40.230099: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
            "2024-07-15 01:51:40.230394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.230454: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.230512: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.230566: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.230619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.230672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.230771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.230827: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.230882: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.230928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22279 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
            "2024-07-15 01:51:40.230947: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.230994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22279 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:02:00.0, compute capability: 8.9\n",
            "2024-07-15 01:51:40.523261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.523424: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.523471: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 2\n",
            "2024-07-15 01:51:40.523527: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
            "2024-07-15 01:51:40.523776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.523839: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.523898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.523951: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.524009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.524068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.524167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.524223: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.524277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.524323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22279 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
            "2024-07-15 01:51:40.524342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:51:40.524385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22279 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:02:00.0, compute capability: 8.9\n",
            "2024-07-15 01:51:40.681625: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
            "2024-07-15 01:51:40.681648: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
            "2024-07-15 01:51:40.732312: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2051] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
            "Flex ops: FlexAddV2, FlexBatchToSpaceND, FlexBiasAdd, FlexConcatV2, FlexConv2D, FlexConv2DBackpropInput, FlexExpandDims, FlexFloorMod, FlexIdentity, FlexLeakyRelu, FlexMirrorPad, FlexMul, FlexPack, FlexPad, FlexReshape, FlexShape, FlexSpaceToBatchND, FlexSqueeze, FlexStridedSlice, FlexSub, FlexTanh\n",
            "Details:\n",
            "\ttf.AddV2(tensor<1x?x192xf32>, tensor<1x?x192xf32>) -> (tensor<1x?x192xf32>) : {device = \"\"}\n",
            "\ttf.AddV2(tensor<1x?x48xf32>, tensor<1x?x48xf32>) -> (tensor<1x?x48xf32>) : {device = \"\"}\n",
            "\ttf.AddV2(tensor<1x?x96xf32>, tensor<1x?x96xf32>) -> (tensor<1x?x96xf32>) : {device = \"\"}\n",
            "\ttf.BatchToSpaceND(tensor<27x?x192xf32>, tensor<1xi32>, tensor<1x2xi32>) -> (tensor<1x?x192xf32>) : {device = \"\"}\n",
            "\ttf.BatchToSpaceND(tensor<27x?x48xf32>, tensor<1xi32>, tensor<1x2xi32>) -> (tensor<1x?x48xf32>) : {device = \"\"}\n",
            "\ttf.BatchToSpaceND(tensor<27x?x96xf32>, tensor<1xi32>, tensor<1x2xi32>) -> (tensor<1x?x96xf32>) : {device = \"\"}\n",
            "\ttf.BatchToSpaceND(tensor<3x?x192xf32>, tensor<1xi32>, tensor<1x2xi32>) -> (tensor<1x?x192xf32>) : {device = \"\"}\n",
            "\ttf.BatchToSpaceND(tensor<3x?x48xf32>, tensor<1xi32>, tensor<1x2xi32>) -> (tensor<1x?x48xf32>) : {device = \"\"}\n",
            "\ttf.BatchToSpaceND(tensor<3x?x96xf32>, tensor<1xi32>, tensor<1x2xi32>) -> (tensor<1x?x96xf32>) : {device = \"\"}\n",
            "\ttf.BatchToSpaceND(tensor<9x?x192xf32>, tensor<1xi32>, tensor<1x2xi32>) -> (tensor<1x?x192xf32>) : {device = \"\"}\n",
            "\ttf.BatchToSpaceND(tensor<9x?x48xf32>, tensor<1xi32>, tensor<1x2xi32>) -> (tensor<1x?x48xf32>) : {device = \"\"}\n",
            "\ttf.BatchToSpaceND(tensor<9x?x96xf32>, tensor<1xi32>, tensor<1x2xi32>) -> (tensor<1x?x96xf32>) : {device = \"\"}\n",
            "\ttf.BiasAdd(tensor<1x?x192xf32>, tensor<192xf32>) -> (tensor<1x?x192xf32>) : {data_format = \"NHWC\", device = \"\"}\n",
            "\ttf.BiasAdd(tensor<1x?x1x192xf32>, tensor<192xf32>) -> (tensor<1x?x1x192xf32>) : {data_format = \"NHWC\", device = \"\"}\n",
            "\ttf.BiasAdd(tensor<1x?x1x48xf32>, tensor<48xf32>) -> (tensor<1x?x1x48xf32>) : {data_format = \"NHWC\", device = \"\"}\n",
            "\ttf.BiasAdd(tensor<1x?x1x96xf32>, tensor<96xf32>) -> (tensor<1x?x1x96xf32>) : {data_format = \"NHWC\", device = \"\"}\n",
            "\ttf.BiasAdd(tensor<1x?x384xf32>, tensor<384xf32>) -> (tensor<1x?x384xf32>) : {data_format = \"NHWC\", device = \"\"}\n",
            "\ttf.BiasAdd(tensor<1x?x48xf32>, tensor<48xf32>) -> (tensor<1x?x48xf32>) : {data_format = \"NHWC\", device = \"\"}\n",
            "\ttf.BiasAdd(tensor<1x?x4xf32>, tensor<4xf32>) -> (tensor<1x?x4xf32>) : {data_format = \"NHWC\", device = \"\"}\n",
            "\ttf.BiasAdd(tensor<1x?x96xf32>, tensor<96xf32>) -> (tensor<1x?x96xf32>) : {data_format = \"NHWC\", device = \"\"}\n",
            "\ttf.ConcatV2(tensor<1xi32>, tensor<1xi32>, tensor<2xi32>, tensor<i32>) -> (tensor<4xi32>) : {device = \"\"}\n",
            "\ttf.Conv2D(tensor<1x1x?x192xf32>, tensor<1x1x192x192xf32>) -> (tensor<1x1x?x192xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<1x1x?x192xf32>, tensor<1x3x192x192xf32>) -> (tensor<1x1x?x192xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<1x1x?x48xf32>, tensor<1x1x48x48xf32>) -> (tensor<1x1x?x48xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<1x1x?x48xf32>, tensor<1x3x48x48xf32>) -> (tensor<1x1x?x48xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<1x1x?x48xf32>, tensor<1x7x48x4xf32>) -> (tensor<1x1x?x4xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<1x1x?x4xf32>, tensor<1x63x4x1xf32>) -> (tensor<1x1x?x1xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<1x1x?x80xf32>, tensor<1x7x80x384xf32>) -> (tensor<1x1x?x384xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<1x1x?x96xf32>, tensor<1x1x96x96xf32>) -> (tensor<1x1x?x96xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<1x1x?x96xf32>, tensor<1x3x96x96xf32>) -> (tensor<1x1x?x96xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<27x1x?x192xf32>, tensor<1x3x192x192xf32>) -> (tensor<27x1x?x192xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<27x1x?x48xf32>, tensor<1x3x48x48xf32>) -> (tensor<27x1x?x48xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<27x1x?x96xf32>, tensor<1x3x96x96xf32>) -> (tensor<27x1x?x96xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<3x1x?x192xf32>, tensor<1x3x192x192xf32>) -> (tensor<3x1x?x192xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<3x1x?x48xf32>, tensor<1x3x48x48xf32>) -> (tensor<3x1x?x48xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<3x1x?x96xf32>, tensor<1x3x96x96xf32>) -> (tensor<3x1x?x96xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<9x1x?x192xf32>, tensor<1x3x192x192xf32>) -> (tensor<9x1x?x192xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<9x1x?x48xf32>, tensor<1x3x48x48xf32>) -> (tensor<9x1x?x48xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2D(tensor<9x1x?x96xf32>, tensor<1x3x96x96xf32>) -> (tensor<9x1x?x96xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2DBackpropInput(tensor<4xi32>, tensor<16x1x192x384xf32>, tensor<1x?x1x384xf32>) -> (tensor<1x?x1x192xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 8, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2DBackpropInput(tensor<4xi32>, tensor<1x4x4x4xf32>, tensor<1x1x?x4xf32>) -> (tensor<1x1x?x4xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 4, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2DBackpropInput(tensor<4xi32>, tensor<8x1x48x96xf32>, tensor<1x?x1x96xf32>) -> (tensor<1x?x1x48xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 4, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.Conv2DBackpropInput(tensor<4xi32>, tensor<8x1x96x192xf32>, tensor<1x?x1x192xf32>) -> (tensor<1x?x1x96xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 4, 1, 1], use_cudnn_on_gpu = true}\n",
            "\ttf.ExpandDims(tensor<1x192x192xf32>, tensor<i32>) -> (tensor<1x1x192x192xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<1x48x48xf32>, tensor<i32>) -> (tensor<1x1x48x48xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<1x96x96xf32>, tensor<i32>) -> (tensor<1x1x96x96xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<1x?x192xf32>, tensor<i32>) -> (tensor<1x1x?x192xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<1x?x192xf32>, tensor<i32>) -> (tensor<1x?x1x192xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<1x?x384xf32>, tensor<i32>) -> (tensor<1x?x1x384xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<1x?x48xf32>, tensor<i32>) -> (tensor<1x1x?x48xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<1x?x4xf32>, tensor<i32>) -> (tensor<1x1x?x4xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<1x?x80xf32>, tensor<i32>) -> (tensor<1x1x?x80xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<1x?x96xf32>, tensor<i32>) -> (tensor<1x1x?x96xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<1x?x96xf32>, tensor<i32>) -> (tensor<1x?x1x96xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<27x?x192xf32>, tensor<i32>) -> (tensor<27x1x?x192xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<27x?x48xf32>, tensor<i32>) -> (tensor<27x1x?x48xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<27x?x96xf32>, tensor<i32>) -> (tensor<27x1x?x96xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<3x192x192xf32>, tensor<i32>) -> (tensor<1x3x192x192xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<3x48x48xf32>, tensor<i32>) -> (tensor<1x3x48x48xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<3x96x96xf32>, tensor<i32>) -> (tensor<1x3x96x96xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<3x?x192xf32>, tensor<i32>) -> (tensor<3x1x?x192xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<3x?x48xf32>, tensor<i32>) -> (tensor<3x1x?x48xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<3x?x96xf32>, tensor<i32>) -> (tensor<3x1x?x96xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<7x48x4xf32>, tensor<i32>) -> (tensor<1x7x48x4xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<7x80x384xf32>, tensor<i32>) -> (tensor<1x7x80x384xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<9x?x192xf32>, tensor<i32>) -> (tensor<9x1x?x192xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<9x?x48xf32>, tensor<i32>) -> (tensor<9x1x?x48xf32>) : {device = \"\"}\n",
            "\ttf.ExpandDims(tensor<9x?x96xf32>, tensor<i32>) -> (tensor<9x1x?x96xf32>) : {device = \"\"}\n",
            "\ttf.FloorMod(tensor<1xi32>, tensor<1xi32>) -> (tensor<1xi32>) : {device = \"\"}\n",
            "\ttf.Identity(tensor<16x1x192x384xf32>) -> (tensor<16x1x192x384xf32>) : {device = \"\"}\n",
            "\ttf.Identity(tensor<192xf32>) -> (tensor<192xf32>) : {device = \"\"}\n",
            "\ttf.Identity(tensor<1x192x192xf32>) -> (tensor<1x192x192xf32>) : {device = \"\"}\n",
            "\ttf.Identity(tensor<1x2xi32>) -> (tensor<1x2xi32>) : {device = \"\"}\n",
            "\ttf.Identity(tensor<1x48x48xf32>) -> (tensor<1x48x48xf32>) : {device = \"\"}\n",
            "\ttf.Identity(tensor<1x96x96xf32>) -> (tensor<1x96x96xf32>) : {device = \"\"}\n",
            "\ttf.Identity(tensor<1x?x192xf32>) -> (tensor<1x?x192xf32>) : {device = \"\"}\n",
            "\ttf.Identity(tensor<1x?x1xf32>) -> (tensor<1x?x1xf32>) : {device = \"\"}\n",
            "\ttf.Identity(tensor<1x?x48xf32>) -> (tensor<1x?x48xf32>) : {device = \"\"}\n",
            "\ttf.Identity(tensor<1x?x4xf32>) -> (tensor<1x?x4xf32>) : {device = \"\"}\n",
            "\ttf.Identity(tensor<1x?x96xf32>) -> (tensor<1x?x96xf32>) : {device = \"\"}\n",
            "\ttf.Identity(tensor<384xf32>) -> (tensor<384xf32>) : {device = \"\"}\n",
            "\ttf.Identity(tensor<3x192x192xf32>) -> (tensor<3x192x192xf32>) : {device = \"\"}\n",
            "\ttf.Identity(tensor<3x48x48xf32>) -> (tensor<3x48x48xf32>) : {device = \"\"}\n",
            "\ttf.Identity(tensor<3x96x96xf32>) -> (tensor<3x96x96xf32>) : {device = \"\"}\n",
            "\ttf.Identity(tensor<48xf32>) -> (tensor<48xf32>) : {device = \"\"}\n",
            "\ttf.Identity(tensor<4xf32>) -> (tensor<4xf32>) : {device = \"\"}\n",
            "\ttf.Identity(tensor<7x48x4xf32>) -> (tensor<7x48x4xf32>) : {device = \"\"}\n",
            "\ttf.Identity(tensor<7x80x384xf32>) -> (tensor<7x80x384xf32>) : {device = \"\"}\n",
            "\ttf.Identity(tensor<8x1x48x96xf32>) -> (tensor<8x1x48x96xf32>) : {device = \"\"}\n",
            "\ttf.Identity(tensor<8x1x96x192xf32>) -> (tensor<8x1x96x192xf32>) : {device = \"\"}\n",
            "\ttf.Identity(tensor<96xf32>) -> (tensor<96xf32>) : {device = \"\"}\n",
            "\ttf.LeakyRelu(tensor<1x?x192xf32>) -> (tensor<1x?x192xf32>) : {alpha = 2.000000e-01 : f32, device = \"\"}\n",
            "\ttf.LeakyRelu(tensor<1x?x384xf32>) -> (tensor<1x?x384xf32>) : {alpha = 2.000000e-01 : f32, device = \"\"}\n",
            "\ttf.LeakyRelu(tensor<1x?x48xf32>) -> (tensor<1x?x48xf32>) : {alpha = 2.000000e-01 : f32, device = \"\"}\n",
            "\ttf.LeakyRelu(tensor<1x?x96xf32>) -> (tensor<1x?x96xf32>) : {alpha = 2.000000e-01 : f32, device = \"\"}\n",
            "\ttf.MirrorPad(tensor<1x?x192xf32>, tensor<3x2xi32>) -> (tensor<1x?x192xf32>) : {device = \"\", mode = \"REFLECT\"}\n",
            "\ttf.MirrorPad(tensor<1x?x48xf32>, tensor<3x2xi32>) -> (tensor<1x?x48xf32>) : {device = \"\", mode = \"REFLECT\"}\n",
            "\ttf.MirrorPad(tensor<1x?x80xf32>, tensor<3x2xi32>) -> (tensor<1x?x80xf32>) : {device = \"\", mode = \"REFLECT\"}\n",
            "\ttf.MirrorPad(tensor<1x?x96xf32>, tensor<3x2xi32>) -> (tensor<1x?x96xf32>) : {device = \"\", mode = \"REFLECT\"}\n",
            "\ttf.Mul(tensor<i32>, tensor<i32>) -> (tensor<i32>) : {device = \"\"}\n",
            "\ttf.Pack(tensor<i32>, tensor<i32>) -> (tensor<2xi32>) : {axis = 0 : i64, device = \"\"}\n",
            "\ttf.Pack(tensor<i32>, tensor<i32>, tensor<i32>, tensor<i32>) -> (tensor<4xi32>) : {axis = 0 : i64, device = \"\"}\n",
            "\ttf.Pad(tensor<1x?x4xf32>, tensor<3x2xi32>) -> (tensor<1x?x4xf32>) : {device = \"\"}\n",
            "\ttf.Reshape(tensor<2xi32>, tensor<2xi64>) -> (tensor<1x2xi32>) : {device = \"\"}\n",
            "\ttf.Reshape(tensor<i32>, tensor<1xi64>) -> (tensor<1xi32>) : {device = \"\"}\n",
            "\ttf.Shape(tensor<1x?x192xf32>) -> (tensor<3xi32>) : {device = \"\"}\n",
            "\ttf.Shape(tensor<1x?x1x192xf32>) -> (tensor<4xi32>) : {device = \"\"}\n",
            "\ttf.Shape(tensor<1x?x1x384xf32>) -> (tensor<4xi32>) : {device = \"\"}\n",
            "\ttf.Shape(tensor<1x?x1x96xf32>) -> (tensor<4xi32>) : {device = \"\"}\n",
            "\ttf.Shape(tensor<1x?x48xf32>) -> (tensor<3xi32>) : {device = \"\"}\n",
            "\ttf.Shape(tensor<1x?x4xf32>) -> (tensor<3xi32>) : {device = \"\"}\n",
            "\ttf.Shape(tensor<1x?x96xf32>) -> (tensor<3xi32>) : {device = \"\"}\n",
            "\ttf.SpaceToBatchND(tensor<1x?x192xf32>, tensor<1xi32>, tensor<1x2xi32>) -> (tensor<27x?x192xf32>) : {device = \"\"}\n",
            "\ttf.SpaceToBatchND(tensor<1x?x192xf32>, tensor<1xi32>, tensor<1x2xi32>) -> (tensor<3x?x192xf32>) : {device = \"\"}\n",
            "\ttf.SpaceToBatchND(tensor<1x?x192xf32>, tensor<1xi32>, tensor<1x2xi32>) -> (tensor<9x?x192xf32>) : {device = \"\"}\n",
            "\ttf.SpaceToBatchND(tensor<1x?x48xf32>, tensor<1xi32>, tensor<1x2xi32>) -> (tensor<27x?x48xf32>) : {device = \"\"}\n",
            "\ttf.SpaceToBatchND(tensor<1x?x48xf32>, tensor<1xi32>, tensor<1x2xi32>) -> (tensor<3x?x48xf32>) : {device = \"\"}\n",
            "\ttf.SpaceToBatchND(tensor<1x?x48xf32>, tensor<1xi32>, tensor<1x2xi32>) -> (tensor<9x?x48xf32>) : {device = \"\"}\n",
            "\ttf.SpaceToBatchND(tensor<1x?x96xf32>, tensor<1xi32>, tensor<1x2xi32>) -> (tensor<27x?x96xf32>) : {device = \"\"}\n",
            "\ttf.SpaceToBatchND(tensor<1x?x96xf32>, tensor<1xi32>, tensor<1x2xi32>) -> (tensor<3x?x96xf32>) : {device = \"\"}\n",
            "\ttf.SpaceToBatchND(tensor<1x?x96xf32>, tensor<1xi32>, tensor<1x2xi32>) -> (tensor<9x?x96xf32>) : {device = \"\"}\n",
            "\ttf.Squeeze(tensor<1x1x?x192xf32>) -> (tensor<1x?x192xf32>) : {device = \"\", squeeze_dims = [-3]}\n",
            "\ttf.Squeeze(tensor<1x1x?x1xf32>) -> (tensor<1x?x1xf32>) : {device = \"\", squeeze_dims = [-3]}\n",
            "\ttf.Squeeze(tensor<1x1x?x384xf32>) -> (tensor<1x?x384xf32>) : {device = \"\", squeeze_dims = [-3]}\n",
            "\ttf.Squeeze(tensor<1x1x?x48xf32>) -> (tensor<1x?x48xf32>) : {device = \"\", squeeze_dims = [-3]}\n",
            "\ttf.Squeeze(tensor<1x1x?x4xf32>) -> (tensor<1x?x4xf32>) : {device = \"\", squeeze_dims = [-3]}\n",
            "\ttf.Squeeze(tensor<1x1x?x4xf32>) -> (tensor<1x?x4xf32>) : {device = \"\", squeeze_dims = [1]}\n",
            "\ttf.Squeeze(tensor<1x1x?x96xf32>) -> (tensor<1x?x96xf32>) : {device = \"\", squeeze_dims = [-3]}\n",
            "\ttf.Squeeze(tensor<1x?x1x192xf32>) -> (tensor<1x?x192xf32>) : {device = \"\", squeeze_dims = [2]}\n",
            "\ttf.Squeeze(tensor<1x?x1x48xf32>) -> (tensor<1x?x48xf32>) : {device = \"\", squeeze_dims = [2]}\n",
            "\ttf.Squeeze(tensor<1x?x1x96xf32>) -> (tensor<1x?x96xf32>) : {device = \"\", squeeze_dims = [2]}\n",
            "\ttf.Squeeze(tensor<27x1x?x192xf32>) -> (tensor<27x?x192xf32>) : {device = \"\", squeeze_dims = [-3]}\n",
            "\ttf.Squeeze(tensor<27x1x?x48xf32>) -> (tensor<27x?x48xf32>) : {device = \"\", squeeze_dims = [-3]}\n",
            "\ttf.Squeeze(tensor<27x1x?x96xf32>) -> (tensor<27x?x96xf32>) : {device = \"\", squeeze_dims = [-3]}\n",
            "\ttf.Squeeze(tensor<3x1x?x192xf32>) -> (tensor<3x?x192xf32>) : {device = \"\", squeeze_dims = [-3]}\n",
            "\ttf.Squeeze(tensor<3x1x?x48xf32>) -> (tensor<3x?x48xf32>) : {device = \"\", squeeze_dims = [-3]}\n",
            "\ttf.Squeeze(tensor<3x1x?x96xf32>) -> (tensor<3x?x96xf32>) : {device = \"\", squeeze_dims = [-3]}\n",
            "\ttf.Squeeze(tensor<9x1x?x192xf32>) -> (tensor<9x?x192xf32>) : {device = \"\", squeeze_dims = [-3]}\n",
            "\ttf.Squeeze(tensor<9x1x?x48xf32>) -> (tensor<9x?x48xf32>) : {device = \"\", squeeze_dims = [-3]}\n",
            "\ttf.Squeeze(tensor<9x1x?x96xf32>) -> (tensor<9x?x96xf32>) : {device = \"\", squeeze_dims = [-3]}\n",
            "\ttf.StridedSlice(tensor<1x2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> (tensor<1x2xi32>) : {begin_mask = 0 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64}\n",
            "\ttf.StridedSlice(tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> (tensor<i32>) : {begin_mask = 0 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 1 : i64}\n",
            "\ttf.StridedSlice(tensor<3xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> (tensor<i32>) : {begin_mask = 0 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 1 : i64}\n",
            "\ttf.StridedSlice(tensor<4xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> (tensor<i32>) : {begin_mask = 0 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 1 : i64}\n",
            "\ttf.Sub(tensor<1xi32>, tensor<1xi32>) -> (tensor<1xi32>) : {device = \"\"}\n",
            "\ttf.Tanh(tensor<1x?x4xf32>) -> (tensor<1x?x4xf32>) : {device = \"\"}\n",
            "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n",
            "2024-07-15 01:51:40.732373: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2116] Estimated count of arithmetic ops: 0  ops, equivalently 0  MACs\n"
          ]
        }
      ],
      "source": [
        "text2mel = convert_text2mel_tflite(\n",
        "    model_path=\"/home/s44504/lightspeech-sw-tz-victoria-ft-vocab-exp/model.h5\",\n",
        "    config_path=\"/home/s44504/lightspeech-sw-tz-victoria-ft-vocab-exp/config.yml\",\n",
        "    save_name=\"lightspeech_quant.tflite\",\n",
        "    use_auth_token=False,\n",
        ")\n",
        "\n",
        "vocoder = convert_vocoder_tflite(\n",
        "    model_path=\"/home/s44504/mb-melgan-hifi-sw-tz-victoria-ft-vocab-exp/model.h5\",\n",
        "    config_path=\"/home/s44504/mb-melgan-hifi-sw-tz-victoria-ft-vocab-exp/config.yml\",\n",
        "    save_name=\"mbmelgan.tflite\",\n",
        "    use_auth_token=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeuTKMY206Hp",
        "outputId": "9c1d00d4-38c3-4a04-f051-7d5c3f96f4eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text2mel: 4.500709533691406 MBs\n",
            "Vocoder: 5.052890777587891 MBs\n"
          ]
        }
      ],
      "source": [
        "print(f\"Text2mel: {text2mel} MBs\\nVocoder: {vocoder} MBs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a647479dfc894eb490d77d04f7b0b566",
            "189374a11b3c4f5ca3a02ff3c67a00cc",
            "50026ce4ed83400688694bcc5dd68277",
            "47819ac6e45e4bc88636596a2b44a632",
            "c8744a9c7f77408fa822d38519fe8e59",
            "b552beaeca534cda83b3220ac31d4b2a",
            "a1880ddb72574b7783326c681799152f",
            "a3cdb2a901c842a1bbbb63eef4661c46",
            "0c06be711072463c8744a9674e653a27",
            "ea4877bfaaaf4c31bb042aa38aca6055",
            "e81dbeacb45f4b6298d5ce194fdc7159"
          ]
        },
        "id": "wQYX2tA8MMlt",
        "outputId": "d1987cf6-ac45-49fe-cee0-2768770e75fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/s44504/miniconda3/envs/tftts2/lib/python3.8/site-packages/huggingface_hub/file_download.py:671: FutureWarning: 'cached_download' is the legacy way to download files from the HF hub, please consider upgrading to 'hf_hub_download'\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from tensorflow_tts.inference import AutoProcessor\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"bookbot/lightspeech-mfa-sw-v2\", use_auth_token=True)\n",
        "processor.mode = \"eval\" # change processor from train to eval mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eKdcrQT-1B_h"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "def tokenize(text: str, processor: AutoProcessor) -> List[int]:\n",
        "    return processor.text_to_sequence(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OaJ4KXxoNyHg"
      },
      "outputs": [],
      "source": [
        "def prepare_input(\n",
        "    input_ids: List[str], speaker: int\n",
        ") -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]:\n",
        "    input_ids = tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0)\n",
        "    return (\n",
        "        input_ids,\n",
        "        tf.convert_to_tensor([speaker], tf.int32),\n",
        "        tf.convert_to_tensor([1.0], dtype=tf.float32),\n",
        "        tf.convert_to_tensor([1.0], dtype=tf.float32),\n",
        "        tf.convert_to_tensor([1.0], dtype=tf.float32),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_hAfV80oOzOF"
      },
      "outputs": [],
      "source": [
        "def ls_infer(\n",
        "    input_ids: List[str], speaker: int, lightspeech_path: str\n",
        ") -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
        "    # load model to Interpreter\n",
        "    lightspeech = tf.lite.Interpreter(model_path=lightspeech_path)\n",
        "    input_details = lightspeech.get_input_details()\n",
        "    output_details = lightspeech.get_output_details()\n",
        "\n",
        "    # print(input_details)\n",
        "\n",
        "    # resize input tensors according to actual shape\n",
        "    lightspeech.resize_tensor_input(input_details[0][\"index\"], [1, len(input_ids)])\n",
        "    lightspeech.resize_tensor_input(input_details[1][\"index\"], [1])\n",
        "    lightspeech.resize_tensor_input(input_details[2][\"index\"], [1])\n",
        "    lightspeech.resize_tensor_input(input_details[3][\"index\"], [1])\n",
        "    lightspeech.resize_tensor_input(input_details[4][\"index\"], [1])\n",
        "\n",
        "    # allocate tensors\n",
        "    lightspeech.allocate_tensors()\n",
        "\n",
        "    input_data = prepare_input(input_ids, speaker)\n",
        "\n",
        "    # set input tensors\n",
        "    for i, detail in enumerate(input_details):\n",
        "        lightspeech.set_tensor(detail[\"index\"], input_data[i])\n",
        "\n",
        "    # invoke interpreter\n",
        "    lightspeech.invoke()\n",
        "\n",
        "    # print(output_details)\n",
        "\n",
        "    # return outputs\n",
        "    return (\n",
        "        lightspeech.get_tensor(output_details[0][\"index\"]),\n",
        "        lightspeech.get_tensor(output_details[1][\"index\"]),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5ctAMKb-WVrw"
      },
      "outputs": [],
      "source": [
        "def melgan_infer(melspectrogram: tf.Tensor, mb_melgan_path: str) -> tf.Tensor:\n",
        "    # load model to Interpreter\n",
        "    mb_melgan = tf.lite.Interpreter(model_path=mb_melgan_path)\n",
        "    input_details = mb_melgan.get_input_details()\n",
        "    output_details = mb_melgan.get_output_details()\n",
        "\n",
        "    # resize input tensors according to actual shape\n",
        "    mb_melgan.resize_tensor_input(\n",
        "        input_details[0][\"index\"],\n",
        "        [1, melspectrogram.shape[1], melspectrogram.shape[2]],\n",
        "        strict=True,\n",
        "    )\n",
        "\n",
        "    # allocate tensors\n",
        "    mb_melgan.allocate_tensors()\n",
        "\n",
        "    # set input tensors\n",
        "    mb_melgan.set_tensor(input_details[0][\"index\"], melspectrogram)\n",
        "\n",
        "    # invoke interpreter\n",
        "    mb_melgan.invoke()\n",
        "\n",
        "    # return output\n",
        "    return mb_melgan.get_tensor(output_details[0][\"index\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6VR-2dJRNoYX"
      },
      "outputs": [],
      "source": [
        "# text = \"The quick brown fox jumps over the lazy dog, while the phoneme sounds of pheasants, quails and crickets chirp in the background.\"\n",
        "text = \"Hapo mwanzo Mungu aliumba mbingu na dunia.\"\n",
        "# text = \"Giza lilikuwa juu ya uso wa vilindi vya maji, naye Roho wa Mungu alikuwa ametulia juu ya maji.\"\n",
        "# text = \"Nimepata kupungukiwa, pia nimepata kuwa na wingi wa vitu.\"\n",
        "input_ids = tokenize(text, processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ScGkrO9PXaaL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-15 01:54:32.853009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:54:32.853145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:54:32.853211: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:54:32.853265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:54:32.853319: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:54:32.853373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:54:32.853469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:54:32.853525: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:54:32.853580: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:54:32.853627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22279 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
            "2024-07-15 01:54:32.853646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-07-15 01:54:32.853689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22279 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:02:00.0, compute capability: 8.9\n"
          ]
        }
      ],
      "source": [
        "# _, mel_output_tflite = ls_infer(\n",
        "#     input_ids, speaker=0, lightspeech_path=\"fastspeech2_quant.tflite\"\n",
        "# )\n",
        "\n",
        "mel_output_tflite, _ = ls_infer(\n",
        "    input_ids, speaker=0, lightspeech_path=\"lightspeech_quant.tflite\"\n",
        ")\n",
        "\n",
        "audio_tflite = melgan_infer(mel_output_tflite[:, :, :], mb_melgan_path=\"mbmelgan.tflite\")[\n",
        "    0, :, 0\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 238, 80)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mel_output_tflite.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(121856,)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "audio_tflite.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "\n",
        "sf.write(\"./gen2.wav\", audio_tflite, 44100, \"PCM_16\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "tensorflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    },
    "vscode": {
      "interpreter": {
        "hash": "ee7d7838ef53998fd22ad7449b76e48b4013ea11e59d28ee193f2cd757746339"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05afc3ce0ebe45c8935f1f05d7a7aaad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b3fbd0f965e4f739912e657a788ac9c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c06be711072463c8744a9674e653a27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "169999733a2b4608845f1a601a9e08a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "189374a11b3c4f5ca3a02ff3c67a00cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b552beaeca534cda83b3220ac31d4b2a",
            "placeholder": "",
            "style": "IPY_MODEL_a1880ddb72574b7783326c681799152f",
            "value": "Downloading ()/main/processor.json: 100%"
          }
        },
        "2c90c32776b84f0db46c9c78d06b8a45": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47819ac6e45e4bc88636596a2b44a632": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea4877bfaaaf4c31bb042aa38aca6055",
            "placeholder": "",
            "style": "IPY_MODEL_e81dbeacb45f4b6298d5ce194fdc7159",
            "value": " 1.04k/1.04k [00:00&lt;00:00, 50.8kB/s]"
          }
        },
        "4868410054c04e3fa4c74cb3e53965c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "50026ce4ed83400688694bcc5dd68277": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3cdb2a901c842a1bbbb63eef4661c46",
            "max": 1039,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c06be711072463c8744a9674e653a27",
            "value": 1039
          }
        },
        "60f7517340fc4d709bcc92df0a080924": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "630ee1ea2ef24ff5aba202b0ced2f415": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_97625ace8b124deea2d863940a2c7fb1",
            "style": "IPY_MODEL_d3648568741c48548ec391598638ec03",
            "value": true
          }
        },
        "6c6438dd421d47cfbcaceed35717ef7f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dc8e7709ec54f86b8e54a180b5576de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a058dc057785457fb69666993d6b2090",
              "IPY_MODEL_dc007aa393cd4126b2cd353a255b1dfa",
              "IPY_MODEL_630ee1ea2ef24ff5aba202b0ced2f415",
              "IPY_MODEL_d945ed6458cd4a9db5a2b850dcc64b9c",
              "IPY_MODEL_94fa578ab6ab424092b91a6a53de2586"
            ],
            "layout": "IPY_MODEL_4868410054c04e3fa4c74cb3e53965c9"
          }
        },
        "8c569ddf636d458f8c7ed8137cd132fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "94fa578ab6ab424092b91a6a53de2586": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c6438dd421d47cfbcaceed35717ef7f",
            "placeholder": "",
            "style": "IPY_MODEL_169999733a2b4608845f1a601a9e08a6",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "97625ace8b124deea2d863940a2c7fb1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a058dc057785457fb69666993d6b2090": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05afc3ce0ebe45c8935f1f05d7a7aaad",
            "placeholder": "",
            "style": "IPY_MODEL_60f7517340fc4d709bcc92df0a080924",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "a1880ddb72574b7783326c681799152f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3cdb2a901c842a1bbbb63eef4661c46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a647479dfc894eb490d77d04f7b0b566": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_189374a11b3c4f5ca3a02ff3c67a00cc",
              "IPY_MODEL_50026ce4ed83400688694bcc5dd68277",
              "IPY_MODEL_47819ac6e45e4bc88636596a2b44a632"
            ],
            "layout": "IPY_MODEL_c8744a9c7f77408fa822d38519fe8e59"
          }
        },
        "b498f6e495bc44a8beb4b0b2d7221a36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b552beaeca534cda83b3220ac31d4b2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8744a9c7f77408fa822d38519fe8e59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3648568741c48548ec391598638ec03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d945ed6458cd4a9db5a2b850dcc64b9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_0b3fbd0f965e4f739912e657a788ac9c",
            "style": "IPY_MODEL_8c569ddf636d458f8c7ed8137cd132fb",
            "tooltip": ""
          }
        },
        "dc007aa393cd4126b2cd353a255b1dfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_2c90c32776b84f0db46c9c78d06b8a45",
            "placeholder": "",
            "style": "IPY_MODEL_b498f6e495bc44a8beb4b0b2d7221a36",
            "value": ""
          }
        },
        "e81dbeacb45f4b6298d5ce194fdc7159": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea4877bfaaaf4c31bb042aa38aca6055": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
